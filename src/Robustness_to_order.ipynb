{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "monthly-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import sys\n",
    "\n",
    "sys.path.append('../TrainedNarrators/')\n",
    "from data_utils import *\n",
    "from model_utils import setupTokenizer\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "warmup_steps = 0\n",
    "modeltype = 'earlyfusion'\n",
    "modelbase='facebook/bart-base'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pretty-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "tokenizer = tokenizer_ = setupTokenizer(modelbase=modelbase)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "processed = pk.load(open('../dataset/train_dataset_org.dat', 'rb'))\n",
    "dataset = RDFDataSetForTableStructured(tokenizer_,  processed, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38503017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type: earlyfusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DataNarrationBart were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.relative_attention_for_table.query.bias', 'encoder.relative_attention_for_table.key.weight', 'encoder.relative_attention_for_table.query.weight', 'encoder.relative_attention_for_table.Er', 'encoder.relative_attention_for_table.value.bias', 'encoder.relative_attention_for_table.mask', 'encoder.relative_attention_for_table.value.weight', 'encoder.relative_attention_for_table.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if 'bart' in modelbase:\n",
    "    from narrations_models import BartNarrationModel\n",
    "    model_generator = BartNarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)\n",
    "else:\n",
    "    from narrations_models import T5NarrationModel\n",
    "    model_generator = T5NarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prospective-example",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50303, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generator.generator.get_encoder().embed_tokens.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "light-provider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CollapsedMetricsTableEncoderBart(\n",
       "  (token_embedding_layer): Embedding(50265, 768, padding_idx=1)\n",
       "  (embedding_layer): Embedding(50265, 768, padding_idx=1)\n",
       "  (metric_name_sa): SelfAttentionEncoderBart(\n",
       "    (att_block): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (metric_value_sa): SelfAttentionEncoderBart(\n",
       "    (att_block): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (metric_rate_sa): SelfAttentionEncoderBart(\n",
       "    (att_block): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (metrics_relation_module): SelfAttentionEncoderBart(\n",
       "    (att_block): BartEncoderLayer(\n",
       "      (self_attn): BartAttention(\n",
       "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (metric_highlight_layer): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (value_highlight_layer): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (output_layer): Linear(in_features=1536, out_features=768, bias=True)\n",
       "  (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generator.aux_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corresponding-robertson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141286912"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_generator.generator.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_rated/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "given-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_generator.bartconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "included-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh= modelbase.split('/')[1] if 'bart' in modelbase else modelbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loaded-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='outputs_full_rated'\n",
    "output_path = '../TrainedNarrators/P-NarrationsModels/' + \\\n",
    "    modeltype+output_path+'/'+hh+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c439033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_rated/bart-large/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "composite-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.loadModel(model_path=output_path+'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daily-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_everything(43)\n",
    "def generateForSampleTopK(prompt,bs=4,use_top_k=False):\n",
    "    #seed_everything(43)\n",
    "    model_generator.generator.eval()\n",
    "    if model_generator.aux_encoder is not None:\n",
    "        model_generator.aux_encoder.eval()\n",
    "    batch = dataset.processTableInfo(prompt)\n",
    "    # batch=dataset.processTableInfo(test_sample[tidx])\n",
    "    clb, di = batch['class_labels'].unsqueeze(0).to(\n",
    "                device), batch['data_info'].unsqueeze(0).to(device)\n",
    "    met, rate, val = batch['metrics_seq'].unsqueeze(0).to(device), batch['rates'].unsqueeze(\n",
    "                0).to(device), batch['values'].unsqueeze(0).to(device)\n",
    "    preamble_tokens = batch['preamble_tokens'].unsqueeze(\n",
    "                0).to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].unsqueeze(\n",
    "                0).to(device)\n",
    "    met_att = batch['metrics_attention'].unsqueeze(0).to(device)\n",
    "    rate_att = batch['rate_attention'].unsqueeze(0).to(device)\n",
    "    val_att = batch['value_attention'].unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    if model_generator.aux_encoder is not None:\n",
    "        table_rep = model_generator.performAuxEncoding(\n",
    "                    [met, met_att], [val, val_att], [rate, rate_att])\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    do_sample=True, top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=2.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=bs,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        \n",
    "    else:\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    do_sample=True, top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=bs,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "    ss = [tokenizer.decode(s,\n",
    "                                  skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True) for s in sample_outputs]\n",
    "    return ss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pointed-plastic",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_u = json.load(open('../dataset/test set.json'))\n",
    "test_sample_u = []\n",
    "eval_tables_u = []\n",
    "for idx, pc in enumerate(test_data_u):\n",
    "    #print(idx)\n",
    "    test_sample_u.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "operational-interim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "graphic-empire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 91.3% <|> f1score | VALUE_HIGH | 88.89% <|> accuracy | VALUE_HIGH | 90.67% <|> sensitivity | VALUE_HIGH | 87.29% && sensitivity | also_known_as | recall  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['sensitivity', 'accuracy', 'f1score', 'precision'],\n",
       " 'values': ['87.29%', '90.67%', '88.89%', '91.3%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidx=0\n",
    "pc = test_data_u[tidx]\n",
    "prep=processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True)\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sustained-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 91.3% <|> sensitivity | VALUE_HIGH | 87.29% && sensitivity | also_known_as | recall <|> accuracy | VALUE_HIGH | 90.67% <|> f1score | VALUE_HIGH | 88.89%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ', 'classes': ['#CA', '#CB'], 'dataset_attribute': ['<|IMBALANCED|>'], 'metrics': ['f1score', 'accuracy', 'precision', 'sensitivity'], 'values': ['88.89%', '90.67%', '91.3%', '87.29%'], 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'], 'narration': \"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\"}\n"
     ]
    }
   ],
   "source": [
    "tidx=0\n",
    "pc = test_data_u[tidx]\n",
    "prep=processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True)\n",
    "print(prep)\n",
    "outp=generateForSampleTopK(prep,bs=11,use_top_k=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "legitimate-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that the learning algorithm employed to solve this ML task has a moderately high prediction performance and will be able to correctly classify most test samples with only a few instances misclassified.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that the learning algorithm employed to solve this ML task has a moderately high prediction performance and will be able to correctly classify most test samples with only a small margin of error.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that the learning algorithm employed to solve this ML task has a moderately high prediction performance and will be able to correctly classify most test samples with a small margin of error.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of examples drawn randomly from any of the classes.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.28%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of examples drawn randomly from any of the classes.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that the learning algorithm employed to solve this ML task has a moderately high prediction performance and will be able to correctly classify most test samples.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of examples drawn from the positive class #CB as #CA.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of test samples drawn randomly from any of the classes.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of examples drawn from any of the classes.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.28%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a small number of examples drawn from any of the classes.\",\n",
       " \"Trained to tell-apart the examples belonging to the class labels #CA and #CB, the model's classification prowess is characterized by the scores 86.49%, 75.29%, 77.04%, and 81.61%, respectively, across the metrics sensitivity, precision, AUC, and accuracy. From these scores, we can conclude that this model has a moderate classification performance hence will likely misclassify only a few examples drawn randomly from any of the classes.\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-vinyl",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mineral-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt= T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affected-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ttt.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "russian-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('To summarise, this is a good performing model with high accuracy of 92.78%, although a recall of 81.15% means that the model misclassified <rec_diff> of the majority class C1 as C2. The model performs well, with high accuracy (92.78%) and very high auc (96.38%) and very high precision (98.02). A precision score this high means that 98.02% of identifications predicited as class C1 were actually C1.',\n",
       " [3, 2, 6004, 834, 26, 5982, 3155, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[pc['narration'] for pc in full_data]\n",
    "sentences[1],ttt.encode('<acc_diff>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "laden-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ''.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "found-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h i s c l a s s i '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grateful-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-panic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-speech",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pediatric-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics = [[m.lower().strip() for m in test_sample_u[idx]['metrics']] for idx in range(len(test_sample_u))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "furnished-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['specificity', 'accuracy', 'f2score', 'sensitivity', 'precision']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics = [[m.lower().strip() for m in test_sample[idx]['metrics']] for idx in range(len(test_sample))]\n",
    "\n",
    "# replace metrics such as sensitivity with recall\n",
    "cleans_table_refs =[]\n",
    "for r in copy.deepcopy( table_ref_metrics):\n",
    "    if 'sensitivity' in set(r):\n",
    "        idx = r.index('sensitivity')\n",
    "        r[idx] ='recall'\n",
    "    cleans_table_refs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "above-strike",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> f2score | VALUE_HIGH | 84.33% <|> accuracy | VALUE_HIGH | 86.11% <|> precision | VALUE_HIGH | 89.07% <|> auc | VALUE_HIGH | 90.09% <|> sensitivity | VALUE_HIGH | 84.29% && sensitivity | also_known_as | recall  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['sensitivity', 'auc', 'accuracy', 'precision', 'f2score'],\n",
       " 'values': ['84.29%', '90.09%', '86.11%', '89.07%', '84.33%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"Sensitivity, accuracy, AUC and precision scores of 84.29%, 86.11%, and 89.07% respectively show or indicate how good the model's performance is on this binary classification task. From the F2score, precision and sensitivity scores, we can see that the false positive rate is very low. In summary, only a small number of test cases are likely to be misclassified.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidx=4\n",
    "test_sample[tidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    }
   ],
   "source": [
    "outp=generateForSampleTopK(test_sample[tidx],use_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minus-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMetricMentions(narration,replace_identical=True,ref_mentions=None):\n",
    "    metrics_list = ['recall', 'sensitivity', 'f1-score',\n",
    "                    'precision', 'f2-score', 'f1','f1score','f2score','g-mean','gmean',\n",
    "                    'f2', 'accuracy',\n",
    "                    'auc',\n",
    "                    'specificity', ]\n",
    "    nn = normalize_text(narration).split()\n",
    "    # replace identical metrics such as recall and sensitivity based on the mention in the table\n",
    "    \n",
    "    # Take care of when recall and sensitivity is mentioned together\n",
    "    \n",
    "    found_metrics = [s.lower()#.replace('-score', '').replace('score', '')\n",
    "                     for s in set(metrics_list).intersection(nn)]\n",
    "    if replace_identical:\n",
    "        if 'recall' in set(found_metrics) and 'sensitivity' in set(found_metrics):\n",
    "            if 'sensitivity' in ref_mentions:\n",
    "                found_metrics = ' '.join(found_metrics).replace('recall','').split()\n",
    "            elif 'sensitivity' in ref_mentions:\n",
    "                found_metrics = ' '.join(found_metrics).replace('sensitivity','').split()\n",
    "    return found_metrics\n",
    "def metricMentionScore(reference_metrics,sys_output):\n",
    "    ref_counts =  np.sum([len(e) for e in reference_metrics])\n",
    "    sys_mentions = [extractMetricMentions(r,ref_mentions=t) for r,t in zip(sys_output,reference_metrics)]\n",
    "    g_sysCount = np.sum([len(e) for e in sys_mentions])\n",
    "    overlap=np.sum([getOverlap(r,b) for r,b in zip(reference_metrics,sys_mentions)])\n",
    "    recall=overlap/ref_counts\n",
    "    precision=overlap/g_sysCount\n",
    "    f1= 2*(recall*precision)/(recall+precision)\n",
    "    return dict(f1_score=round(f1,5),recall=round(recall,5),precision=round(precision,5))#,sys_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hawaiian-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classLabelsEval(reference_classes,generated,penalty =-1):\n",
    "    # In some cases the model will generate the phrase \"binary classification\" or \"multi-class\"\n",
    "    # \"2 classes\", \"3 classes\",\"two labels\", \"two classes\"\n",
    "    false_mentions = 1\n",
    "    if any(ele in generated for ele in ['binary','two classes','two class labels','two-labels','2 classes',\n",
    "                                        'two-classes','two class labels','2 labels','2 classes']) and len(reference_classes)> 2:\n",
    "        # should have generated multi-class\n",
    "        false_mentions = penalty\n",
    "        \n",
    "    if any(ele in generated for ele in ['multi-class', 'multi class', 'multi classes','multi labels',\n",
    "                                        'three-classes','three classes',\n",
    "                                        'three class labels',\n",
    "                                        '3-labels',\n",
    "                                        '3-classes',\n",
    "                                       'four classes',\n",
    "                                        'four class labels',\n",
    "                                        '4 labels',\n",
    "                                        '4 classes']) and len(reference_classes)< 3:\n",
    "        # should have generated binary\n",
    "        false_mentions = penalty\n",
    "        \n",
    "    \n",
    "    classes_tokens = [f'c{i}' for i in range(1, 6)]\n",
    "    nn= normalize_text(generated).split()\n",
    "    found_classes = [s.lower() for s in set(classes_tokens).intersection(nn)]\n",
    "    n_found = 0\n",
    "    label_mentions ={}\n",
    "    label_was_mentioned=False\n",
    "    predicted_text = normalize_text(generated)\n",
    "    for cl in classes_tokens:\n",
    "        c= predicted_text.count(cl)\n",
    "        if cl in reference_classes:\n",
    "            # The class label exists in the reference class list so the model gets a score of 1\n",
    "            label_mentions[cl] = 1\n",
    "        else:\n",
    "            if c >0:\n",
    "                # Invalid class label was mentioned\n",
    "                # A penalty is assigned t\n",
    "                label_mentions[cl] = penalty\n",
    "            else:\n",
    "                # No mention of the class \n",
    "                label_mentions[cl] = 0\n",
    "        if c > 0:\n",
    "            label_was_mentioned=True\n",
    "    \n",
    "    if not label_was_mentioned:\n",
    "        # if no label was mentioned, we assume  they were mentioned implicitly\n",
    "        return 1\n",
    "    else:\n",
    "        # Check to make sure only the right labels were accounted for\n",
    "        score = np.mean([v for v in list(label_mentions.values()) if v !=0]+[false_mentions])\n",
    "        return np.round(score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fitted-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForDataDistributionMentions(text,dataset_info):\n",
    "    balanced_list = ['balanced', 'balance','identical', 'similar distribution']\n",
    "    imblanced_list = ['disproportionate','imbalance','class imbalance','imbalanced']\n",
    "    if any(ele in text.split() for ele in balanced_list):\n",
    "        return 'balanced'\n",
    "    if any(ele in text for ele in imblanced_list):\n",
    "        return 'imbalanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stainless-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp=[o.replace('G-Mean','GMean') for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pending-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['specificity', 'accuracy', 'f2score', 'sensitivity', 'precision']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "grateful-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1score', 'precision', 'f2score', 'accuracy', 'specificity', 'sensitivity']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractMetricMentions(normalize_text(outp[0]),ref_mentions=table_ref_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "wanted-lounge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imbalanced'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkForDataDistributionMentions(outp[-1],'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "basic-firmware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the following are the scores achieved by the classifier on this binary classification task : ( 1 ) accuracy equal to 86.11 % with the auc , ( 2 ) sensitivity and ( 3 ) precision equal to 90.09 % and ( 4 ) f1score of 84.33 % . in addition , the prediction performance of the model is high despite the class imbalance . judging based on the scores , it would be safe to conclude that this model has a moderate performance as it is likely to misclassify some test examples especially those drawn randomly from the class label .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(outp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "forward-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n"
     ]
    }
   ],
   "source": [
    "for s in outp:\n",
    "    print(metricMentionScore([table_ref_metrics[tidx]],[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
