{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "loving-haven",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import sys\n",
    "\n",
    "sys.path.append('../TrainedNarrators/')\n",
    "from data_utils import *\n",
    "from model_utils import setupTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')\n",
    "# Create a blank Tokenizer with just the English vocab\n",
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acute-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_count= lambda x: len(sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "subjective-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import wordpunct_tokenize,word_tokenize\n",
    "# Read the dataset\n",
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "full_data= json.load(open('../dataset/annotation_data_with_augmentations.json'))['data']+test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bulgarian-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))\n",
    "    # eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabulous-system",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> auc | VALUE_HIGH | 95.87% && precision | VALUE_HIGH | 89.13% && accuracy | VALUE_HIGH | 90.73% && sensitivity | VALUE_HIGH | 90.32% && sensitivity | also_known_as | recall  <|section-sep|> <TaskDec> ml_task | data_dist | is_imbalanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_imbalanced'],\n",
       " 'metrics': ['auc', 'sensitivity', 'accuracy', 'precision'],\n",
       " 'values': ['95.87%', '90.32%', '90.73%', '89.13%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': 'Given the machine learning problem under consideration, the model achieved a high accuracy score of 90.73% with a corresponding high AUC score of 95.87%. Also, the precision score is 89.13% and the recall/sensitivity score is 90.32%. From the dataset distribution provided, we can conclude that only the precision score and Sensitivity score are important to accurately assess the performance of the model on this ML task. The scores achieved across these metrics are very high which imply that prediction decisions for the majority of the test cases will be correct. The recall and precision score motivate a higher trust in output predictions.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standing-nothing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get task names\n",
    "task_names = [d[\"task_name\"] for d in full_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sunrise-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of unique words\n",
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "metrics ={1:[],2:[],3:[],4:[],}\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    #print(idx)\n",
    "    examples = processInputTableAndNarrations(pc, identical_metrics=identicals, augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "divided-oxygen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "australian-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum sentence Length\n",
    "toks=[pc['narration'].lower().split() for pc in processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beautiful-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens=[len(t) for t in toks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "severe-gamma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 160)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lens),max(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "prime-quebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> auc | VALUE_HIGH | 98.59% && recall | VALUE_HIGH | 97.33% && precision | VALUE_HIGH | 94.8% && accuracy | VALUE_HIGH | 96.0%  <|section-sep|> <TaskDec> ml_task | data_dist | is_balanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_balanced'],\n",
       " 'metrics': ['accuracy', 'auc', 'recall', 'precision'],\n",
       " 'values': ['96.0%', '98.59%', '97.33%', '94.8%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': 'An accuracy of 96.0%, precision of 94.8%, recall of 97.33% and AUC of 98.59% was achieved. The model attained a very high performance across all the evaluation metrics. Its predictions can be treated as reliable.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[628]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "developmental-agent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f43992b1070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAFgCAYAAACFYaNMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAATPklEQVR4nO3df7Bc9X3e8fcDtzLGaSIwqor1Y1Bi4tRx65qRXQxtJoa0kRPX0A41dDyxmpKKaRPHiTNOIJ6pp//Zrae2k2kJGnCQWw8WoaQQNyUlMnGmQ40jsGN+maLaAYlfujTG6TgzsVU+/WMPZS0ktFe6u5+9uu/XzM7u+Z6zus+cWR7O/d5zzqaqkCTN3indASRptbKAJamJBSxJTSxgSWpiAUtSk4XuACdi27Ztdccdd3THkKRjyZEGV/QR8LPPPtsdQZKO24ouYElaySxgSWpiAUtSEwtYkppYwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawjtuGTZtJMvFjw6bN3ZGlubKib8iuXk8e2M/l19098fa7r7pgimmklccjYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJLUZGoFnOSTSQ4meWBs7MwkdyZ5dHg+YxhPkl9Lsi/JV5KcN61ckjQvpnkEfCOw7bCxq4E9VXUusGdYBng7cO7w2AFcO8VckjQXplbAVfWHwJ8eNnwJsGt4vQu4dGz8UzXyBWBtkrOnlU2S5sGs54DXV9VTw+ungfXD6w3A/rHtDgxjL5FkR5K9SfYuLi5OL+lJYik3TfeG6dJstd2QvaoqSR3H+3YCOwG2bt265PevNku5abo3TJdma9ZHwM+8MLUwPB8cxp8ANo1tt3EYk6ST1qwL+HZg+/B6O3Db2Ph7hrMhzge+OTZVIUknpalNQSS5CfhR4KwkB4APAR8Gbk5yJfAY8K5h898FfgLYB/w58NPTyiVJ82JqBVxV//goqy4+wrYF/Oy0skjSPPJKOElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNWm7FFlz6JQFknSnkFYNC1gvev7QxPeNAO8dIZ0opyAkqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJqd4WvvJ31s2LS5O7E0VX4tvWbHr72XvotHwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUhMLWJKaWMCS1MQClqQmLQWc5BeTPJjkgSQ3JTktyZYk9yTZl2R3kjUd2SRpVmZewEk2AD8PbK2qNwCnAlcAHwE+VlWvBb4BXDnrbJI0S11TEAvAK5MsAKcDTwEXAbcM63cBl/ZEk6TZmHkBV9UTwEeBxxkV7zeBe4HnqurQsNkBYMOR3p9kR5K9SfYuLi7OIrIkTUXHFMQZwCXAFuA1wKuAbZO+v6p2VtXWqtq6bt26KaWUpOnrmIL4MeDrVbVYVd8BbgUuBNYOUxIAG4EnGrJJ0sx0FPDjwPlJTk8S4GLgIeAu4LJhm+3AbQ3ZJGlmOuaA72H0x7b7gPuHDDuBXwHen2Qf8Grghllnk6RZWjj2Jsuvqj4EfOiw4a8Bb2mII0ktvBJOkppYwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJLUxALW/DplgSQTPzZs2tydWFqShe4A0lE9f4jLr7t74s13X3XBFMNIy88jYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktSkpYCTrE1yS5KvJnk4yVuTnJnkziSPDs9ndGSTpFnpOgL+BHBHVf0Q8EbgYeBqYE9VnQvsGZYl6aQ18wJO8n3AjwA3AFTVt6vqOeASYNew2S7g0llnk6RZ6jgC3gIsAr+Z5EtJrk/yKmB9VT01bPM0sL4hmyTNzEQFnOTCScYmtACcB1xbVW8CvsVh0w1VVUAdJcuOJHuT7F1cXDzOCCvXhk2bSTLxQ9L8Wphwu19nVJrHGpvEAeBAVd0zLN/CqICfSXJ2VT2V5Gzg4JHeXFU7gZ0AW7duPWJJn8yePLCfy6+7e+Ltd191wRTTSDoRL1vASd4KXACsS/L+sVXfC5x6PD+wqp5Osj/J66rqEeBi4KHhsR348PB82/H8+5K0UhzrCHgN8D3Ddn95bPzPgMtO4Oe+F/h0kjXA14CfZjQdcnOSK4HHgHedwL8vSXPvZQu4qj4PfD7JjVX12HL90Kr6MrD1CKsuXq6fIUnzbtI54Fck2QmcM/6eqrpoGqEkaTWYtIB/C/gN4Hrg/04vjiStHpMW8KGqunaqSSRplZn0QozfSfIvkpw93LPhzCRnTjWZJJ3kJj0C3j48f2BsrIDvX944krR6TFTAVbVl2kEkabWZqICTvOdI41X1qeWNI52AUxYmvvz6NRs38cT+x6ccSHp5k05BvHns9WmMzte9D7CANT+ePzTxZdpeoq15MOkUxHvHl5OsBT4zjUCStFoc7+0ov8XotpKSpOM06Rzw7/Di7SFPBf4acPO0QknSajDpHPBHx14fAh6rqgNTyCNJq8ZEUxDDTXm+yuiOaGcA355mKElaDSb9Rox3AV8E/hGj20Tek+REbkcpSavepFMQHwTeXFUHAZKsA36f0bdZSJKOw6RnQZzyQvkO/vcS3itJOoJJj4DvSPJ7wE3D8uXA704nkiStDsf6TrjXMvq6+A8k+YfA3x5W/Q/g09MOJ0kns2MdAX8cuAagqm4FbgVI8teHdX9/itkk6aR2rHnc9VV1/+GDw9g5U0kkSavEsQp47cuse+Uy5pCkVedYBbw3yT87fDDJzwD3TieSJK0Ox5oD/gXgt5O8mxcLdyuwBvgHU8wlSSe9ly3gqnoGuCDJ24A3DMP/pao+N/Vk0jQt4ebt4A3cNR2T3g/4LuCuKWeRZmcJN28Hb+Cu6fBqNklqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawJDWxgJtt2LSZJBM/JJ08FroDrHZPHtjP5dfdPfH2u6+6YIppJM1S2xFwklOTfCnJZ4flLUnuSbIvye4ka7qySdIsdE5BvA94eGz5I8DHquq1wDeAK1tSSdKMtBRwko3ATwLXD8sBLgJuGTbZBVzakU2SZqXrCPjjwC8Dzw/Lrwaeq6pDw/IBYMOR3phkR5K9SfYuLi5OPagkTcvMCzjJO4CDVXXv8by/qnZW1daq2rpu3bplTidJs9NxFsSFwDuT/ARwGvC9wCeAtUkWhqPgjcATDdkkaWZmfgRcVddU1caqOge4AvhcVb0buAu4bNhsO3DbrLNJ0izN04UYvwK8P8k+RnPCNzTnkaSpar0Qo6r+APiD4fXXgLd05pGkWZqnI2BJWlUsYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwNIkTlkgycSPDZs2dyfWCuDX0kuTeP4Ql19398Sb777qgimG0cnCI2BJamIBS1ITC1iSmljAktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUhMLWJKaWMCS1MQClqQmFrAkNbGAJamJBSxNwxJv4L6w5jRv+L4KeUN2aRqO4wbu3vB99fEIWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAS+zDZs2L+mEekmrlxdiLLMnD+z3hHpJE/EIWJKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkprMvICTbEpyV5KHkjyY5H3D+JlJ7kzy6PB8xqyzSdIsdRwBHwJ+qapeD5wP/GyS1wNXA3uq6lxgz7AsSSetmRdwVT1VVfcNr/8P8DCwAbgE2DVstgu4dNbZJGmWWueAk5wDvAm4B1hfVU8Nq54G1h/lPTuS7E2yd3FxcTZBJWkK2go4yfcA/wn4har6s/F1VVVAHel9VbWzqrZW1dZ169bNIKkkTUdLASf5S4zK99NVdesw/EySs4f1ZwMHO7JJ0qx0nAUR4Abg4ar6t2Orbge2D6+3A7fNOpskzVLHDdkvBH4KuD/Jl4exXwU+DNyc5ErgMeBdDdkkaWZmXsBV9d+Bo30Xz8WzzCJJnbwSTpKaWMCS1MQClqQmFrAkNbGAJamJBSxJTSxgSWpiAUtSEwtYkppYwJLUxAKWpCYWsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljA0kluw6bNJJn4sWHT5u7Iq0bHd8JJmqEnD+zn8uvunnj73VddMMU0GucRsCQ1sYAlqYkFLElNLGBJamIBS1ITC1iSmljAktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUhMLWJKaWMCS1MQCPoal3sxamolTFvxMngS8IfsxeDNrzaXnD038ufQzOb88ApakJhawJDWxgCWpiQUsSU0sYEnfbQlnWCRhYc1pfu39cfIsCEnfbQlnWMDoLAvPFDo+HgFLUpNVWcBLubhC0jJb4hTHNKcslnqh1XJnWZVTEEu5uMJfl6RldhxTHNPSfaHVqjwClqR5YAFLUhMLWJKaWMCS1GSuCjjJtiSPJNmX5OruPJLmwBLOmljqRSHd5uYsiCSnAv8O+LvAAeCPktxeVQ/1JpPUaom33pyXMywmMU9HwG8B9lXV16rq28BngEuaM0nS1KSqujMAkOQyYFtV/cyw/FPA36qqnztsux3AjmHxdcAjU4x1FvDsFP/95baS8q6krLCy8q6krLCy8h5v1meratvhg3MzBTGpqtoJ7JzFz0qyt6q2zuJnLYeVlHclZYWVlXclZYWVlXe5s87TFMQTwKax5Y3DmCSdlOapgP8IODfJliRrgCuA25szSdLUzM0URFUdSvJzwO8BpwKfrKoHm2PNZKpjGa2kvCspK6ysvCspK6ysvMuadW7+CCdJq808TUFI0qpiAUtSEwv4MElOTfKlJJ8dlrckuWe4PHr38AfCdknWJrklyVeTPJzkrUnOTHJnkkeH5zO6c74gyS8meTDJA0luSnLavOzbJJ9McjDJA2NjR9yXGfm1IfNXkpw3J3n/zfBZ+EqS306ydmzdNUPeR5L8eHfWsXW/lKSSnDUsz+W+HcbfO+zfB5P867HxE9q3FvBLvQ94eGz5I8DHquq1wDeAK1tSvdQngDuq6oeANzLKfDWwp6rOBfYMy+2SbAB+HthaVW9g9EfWK5iffXsjcPhJ8kfbl28Hzh0eO4BrZ5Rx3I28NO+dwBuq6m8A/xO4BiDJ6xnt6x8e3vPvh8v+Z+VGXpqVJJuAvwc8PjY8l/s2ydsYXZX7xqr6YeCjw/gJ71sLeEySjcBPAtcPywEuAm4ZNtkFXNoSbkyS7wN+BLgBoKq+XVXPMfqQ7Bo2m4usYxaAVyZZAE4HnmJO9m1V/SHwp4cNH21fXgJ8qka+AKxNcvZMgg6OlLeq/ltVHRoWv8DoPHoY5f1MVf1FVX0d2Mfosv+2rIOPAb8MjJ8FMJf7FvjnwIer6i+GbQ4O4ye8by3g7/ZxRh+K54flVwPPjX2wDwAbGnIdbguwCPzmMF1yfZJXAeur6qlhm6eB9W0Jx1TVE4yOGh5nVLzfBO5lPvftC462LzcA+8e2m7fcAP8U+K/D67nLm+QS4Imq+uPDVs1d1sEPAn9nmC77fJI3D+MnnNcCHiR5B3Cwqu7tzjKBBeA84NqqehPwLQ6bbqjR+YVzcY7hMH96CaP/cbwGeBVH+LV0Xs3TvjyWJB8EDgGf7s5yJElOB34V+JfdWZZgATgTOB/4AHBzlulelhbwiy4E3pnkTxjdie0iRvOsa4dfm2F+Lo8+AByoqnuG5VsYFfIzL/zKNjwfPMr7Z+3HgK9X1WJVfQe4ldH+nsd9+4Kj7cu5vWQ+yT8B3gG8u148wX/e8v4Ao/8R//Hw39pG4L4kf5X5y/qCA8Ctw9TIFxn9hnwWy5DXAh5U1TVVtbGqzmE0sf65qno3cBdw2bDZduC2poj/X1U9DexP8rph6GLgIUaXbm8fxuYi6+Bx4Pwkpw9HDi/knbt9O+Zo+/J24D3DX+zPB745NlXRJsk2RtNn76yqPx9bdTtwRZJXJNnC6A9cX+zICFBV91fVX6mqc4b/1g4A5w2f6bnct8B/Bt4GkOQHgTWM7oh24vu2qnwc9gB+FPjs8Pr7h526D/gt4BXd+YZcfxPYC3xl+ICcwWjOeg/wKPD7wJndOcfy/ivgq8ADwH8AXjEv+xa4idHc9HcYFcKVR9uXQBh9ccD/Au5ndGbHPOTdx2g+8svD4zfGtv/gkPcR4O3dWQ9b/yfAWXO+b9cA/3H47N4HXLRc+9ZLkSWpiVMQktTEApakJhawJDWxgCWpiQUsSU0sYElqYgFLUpP/Bwtx4PMEBZB7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "indie-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "empty-field",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of ML tasks 58\n"
     ]
    }
   ],
   "source": [
    "# Number of anotations per task\n",
    "task_dist = Counter(task_names)\n",
    "print(f'The number of ML tasks {len(task_dist)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nearby-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pk.load(open('../dataset/train_dataset_new.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hearing-greece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4529"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cleared-class",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e1243bd22c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elementary-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average number of annotations submitted: 14\n"
     ]
    }
   ],
   "source": [
    "# Average number of narrations per task\n",
    "mean_annotations = np.average([v for b,v in dict(task_dist).items()])\n",
    "print(f'The average number of annotations submitted: {int(mean_annotations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "productive-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "narrations = ' '.join([pc['narration'].lower() for pc in processed]).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "unauthorized-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter(narrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "worthy-header",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique words is: 3548\n"
     ]
    }
   ],
   "source": [
    "print(f'The number of unique words is: {len(words)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "local-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average number of sentences per annotation \n",
    "narr_count = [sent_count(pc['narration']) for pc in processed+test_sample]\n",
    "nn= [idx for idx, pc in enumerate(processed) if sent_count(pc['narration']) ==9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "pharmaceutical-korean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6487362281270252"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(narr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "formed-holmes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3: 1844, 4: 1403, 5: 446, 6: 194, 2: 579, 7: 110, 8: 53}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(Counter(narr_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-glasgow",
   "metadata": {},
   "source": [
    "### Most of the narrations have about 3 sentences summarizing the performance of the corresponding classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "demanding-passport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f43df0da1f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(narr_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "major-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processMetricsDataChecks(metric_scores, metric_rates, augment=False, identicals={}, nb_metrics=6):\n",
    "    pp = pd.read_json(json.loads(metric_scores))\n",
    "    metric_rates = json.loads(metric_rates)\n",
    "    if augment:\n",
    "        temp_cols = pp.columns.to_list()\n",
    "        random.shuffle(temp_cols)\n",
    "        pp = pp[temp_cols]\n",
    "    metrics = [s.strip() for s in pp.columns.to_list()]\n",
    "    metric_rates = {s.strip(): v for s, v in metric_rates.items()}\n",
    "    values = pp.values.tolist()[0]\n",
    "\n",
    "    metrics_score_string = '<TM> '\n",
    "\n",
    "    # Make sure the ratings of all metrics are provided\n",
    "    #print(metrics, metric_rates.keys())\n",
    "    assert set(metrics) == set(list(metric_rates.keys()))\n",
    "\n",
    "    metrics_info = []\n",
    "    metrics_list = []\n",
    "    values_list = []\n",
    "    rates_list = []\n",
    "    for idx, (m, v) in enumerate(zip(metrics, values)):\n",
    "        mx = m.lower().replace('-score', '').strip()\n",
    "        mx = m.lower().replace(' score', '').strip()\n",
    "        mx = m.lower().replace('score', '').strip()\n",
    "        score_rate = ''\n",
    "        if int(metric_rates.get(m, 0)) in [4, 5]:\n",
    "            score_rate = 'HIGH'\n",
    "        elif int(metric_rates.get(m, 0)) in [3]:\n",
    "            score_rate = 'MODERATE'\n",
    "        else:\n",
    "            score_rate = 'LOW'\n",
    "\n",
    "        metrics_list.append(m.replace('-', '').lower())\n",
    "        values_list.append(f'{roundN(v,2)}%')\n",
    "        rates_list.append(f'{score_rate}')\n",
    "    return [(a,b,c) for a,b,c in zip(metrics_list,rates_list,values_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "every-atlas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a rough estimate of number of models per each task.\n",
    "# Compute the number of unique words\n",
    "mask_met=False\n",
    "taskNames_samples= {}\n",
    "preambles = {}\n",
    "pream = {}\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    #print(idx)\n",
    "    tn = pc['task_name']\n",
    "    examples = processInputTableAndNarrations(pc, identical_metrics=identicals, augnment_metrics=False,)\n",
    "    pp = processMetricsDataChecks(pc[\"metrics_values\"],pc[\"imetric_score_rate\"],identicals=identicals)\n",
    "    if tn not in preambles.keys():\n",
    "        preambles[tn]= []\n",
    "        pream[tn] = []\n",
    "        taskNames_samples[tn]= []\n",
    "    preambles[tn].append(examples['preamble'])\n",
    "    taskNames_samples[tn].append(examples)\n",
    "    pream[tn].append(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "continued-spanish",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pream['UPS customer service Ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "broadband-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "tns={}\n",
    "for tn,elems in pream.items():\n",
    "    po=[]\n",
    "    for p in elems:\n",
    "        po = po+p\n",
    "    tns[tn] = Counter([p[0] for p in list(set(po))]).most_common(1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "automatic-shower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns['UPS customer service Ratings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "automotive-bedroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.551724137931035"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([v for v in tns.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-freeze",
   "metadata": {},
   "source": [
    "For this NLG task, the goal is generating analytical sentences based on the evaluation metrics' scores achieved by any given classification model. \n",
    "To generate the dataset for this study, different ML models were trained on a number of classification tasks across different application domains. \n",
    "For each classifier, experts were asked to rate the scores achieved for a given set of metrics. \n",
    "Based on the ratings assigned, metrics' scores, and information on the dataset distribution across the class labels, they were also asked to write analytical statements summarising the classification performance of the corresponding model. \n",
    "A carefully designed pre-processing routine is applied to remove invalid submissions resulting in 612 high-quality data-sentence pairs. The training, validation and test set consists of 582 data-sentence pairs, 30 data-sentence pairs, and 30 data-sentence pairs, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-father",
   "metadata": {},
   "source": [
    "### The dataset contains narrations of models or classifiers trained on 58 different machine learning problems. However, only classification tasks were considered. We leave regression tasks for future work.\n",
    "\n",
    "### Across each classification task, about 6 different model were trained. These models include random forest, support vector machines, gradient boosting, logistic regression, and KNN. \n",
    "\n",
    "### The annotators were asked to summarized the performance of the model based on different combination of metric scores. For simplicity only the most common classification metrics were considered. These are accuracy, precision, AUC, recall, specificity, F1-score and F2-score. This implies that the NLG models trained on this dataset will struggle to produce a valid summary of input with unknown metrics.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
