{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monthly-cleveland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "import sys\n",
    "\n",
    "sys.path.append('../TrainedNarrators/')\n",
    "from data_utils import *\n",
    "from model_utils import setupTokenizer\n",
    "learning_rate = 3e-4\n",
    "batch_size = 16\n",
    "epochs = 20\n",
    "warmup_steps = 0\n",
    "modeltype = 'baseline'\n",
    "modelbase='facebook/bart-large'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pretty-burlington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "tokenizer = tokenizer_ = setupTokenizer(modelbase=modelbase)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "processed = pk.load(open('../dataset/train_dataset.dat', 'rb'))\n",
    "dataset = RDFDataSetForTableStructured(tokenizer_,  processed, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38503017",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bart' in modelbase:\n",
    "    from narrations_models import BartNarrationModel\n",
    "    model_generator = BartNarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)\n",
    "else:\n",
    "    from narrations_models import T5NarrationModel\n",
    "    model_generator = T5NarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-excuse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "given-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_generator.bartconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "included-hostel",
   "metadata": {},
   "outputs": [],
   "source": [
    "hh= modelbase.split('/')[1] if 'bart' in modelbase else modelbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loaded-hammer",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='outputs'\n",
    "output_path = '../TrainedNarrators/P-NarrationsModels/' + \\\n",
    "    modeltype+output_path+'/'+hh+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c439033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../TrainedNarrators/P-NarrationsModels/baselineoutputs/bart-large/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "composite-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_generator.loadModel(model_path=output_path+'trained_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "daily-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_everything(43)\n",
    "def generateForSampleTopK(prompt,bs=4,use_top_k=False):\n",
    "    #seed_everything(43)\n",
    "    model_generator.generator.eval()\n",
    "    if model_generator.aux_encoder is not None:\n",
    "        model_generator.aux_encoder.eval()\n",
    "    batch = dataset.processTableInfo(prompt)\n",
    "    # batch=dataset.processTableInfo(test_sample[tidx])\n",
    "    clb, di = batch['class_labels'].unsqueeze(0).to(\n",
    "                device), batch['data_info'].unsqueeze(0).to(device)\n",
    "    met, rate, val = batch['metrics_seq'].unsqueeze(0).to(device), batch['rates'].unsqueeze(\n",
    "                0).to(device), batch['values'].unsqueeze(0).to(device)\n",
    "    preamble_tokens = batch['preamble_tokens'].unsqueeze(\n",
    "                0).to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].unsqueeze(\n",
    "                0).to(device)\n",
    "    met_att = batch['metrics_attention'].unsqueeze(0).to(device)\n",
    "    rate_att = batch['rate_attention'].unsqueeze(0).to(device)\n",
    "    val_att = batch['value_attention'].unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    if model_generator.aux_encoder is not None:\n",
    "        table_rep = model_generator.performAuxEncoding(\n",
    "                    [met, met_att], [val, val_att], [rate, rate_att])\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    do_sample=True, top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=2.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=bs,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        \n",
    "    else:\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    do_sample=True, top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=bs,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "    ss = [tokenizer.decode(s,\n",
    "                                  skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True) for s in sample_outputs]\n",
    "    return ss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "pointed-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "test_data_u = json.load(open('../dataset/newSet.json'))\n",
    "test_sample_u = []\n",
    "eval_tables_u = []\n",
    "for idx, pc in enumerate(test_data_u):\n",
    "    print(idx)\n",
    "    test_sample_u.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "operational-interim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "graphic-empire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> f1score | VALUE_HIGH | 81.28% <|> specificity | VALUE_HIGH | 88.76% <|> sensitivity | VALUE_HIGH | 75.88% && sensitivity | also_known_as | recall <|> precision | VALUE_HIGH | 87.51% <|> accuracy | VALUE_HIGH | 82.21%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['accuracy', 'f1score', 'sensitivity', 'specificity', 'precision'],\n",
       " 'values': ['82.21%', '81.28%', '75.88%', '88.76%', '87.51%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': ''}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidx=0\n",
    "pc = test_data_u[tidx]\n",
    "prep=processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True)\n",
    "prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sustained-exclusive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 87.51% <|> accuracy | VALUE_HIGH | 82.21% <|> sensitivity | VALUE_HIGH | 75.88% && sensitivity | also_known_as | recall <|> specificity | VALUE_HIGH | 88.76% <|> f1score | VALUE_HIGH | 81.28%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ', 'classes': ['#CA', '#CB'], 'dataset_attribute': ['<|BALANCED|>'], 'metrics': ['precision', 'accuracy', 'sensitivity', 'f1score', 'specificity'], 'values': ['87.51%', '82.21%', '75.88%', '81.28%', '88.76%'], 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'], 'narration': ''}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/essel/anaconda3/envs/annotation/lib/python3.8/site-packages/transformers/generation_utils.py:2142: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  next_indices = next_tokens // vocab_size\n"
     ]
    }
   ],
   "source": [
    "prep=processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=True)\n",
    "print(prep)\n",
    "outp=generateForSampleTopK(prep,bs=11,use_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "legitimate-mediterranean",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (82.21%), precision (87.51%), sensitivity (75.88%), specificity (88.76%), and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true labels for several test cases with a small margin of error (actually, the likelihood for mislabeling test cases is <acc_diff> %).',\n",
       " \"The classifier was trained on this balanced dataset to correctly separate the examples into two different class labels (i.e. #CA and #CB ). The model's performance assessment can be summarized as moderately high given the scores attained for the precision, sensitivity/recall, accuracy, specificity, F1score, and precision. For the accuracy, it scored 82.21%, has a sensitivity score of 75.88%, precision score of 87.51%, specificity score of 88.76%, and finally, an F1score of 81.28%. In general, this model tends to frequently label cases as #CB than #CA, given the difference between the recall and precision scores but when it does, it is usually correct. This implies that the confidence level with respect to the #CB predictions is high.\",\n",
       " 'The performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (82.21%), precision (87.51%), sensitivity (75.88%), specificity (88.76%), and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true labels for several test cases with a small margin of error (actually, the likelihood for mislabeling test cases is <acc_diff> %).',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores are high implying that this model will be moderately effective in terms of its predictive power for the majority of test cases/samples. Furthermore, the precision and sensitivity scores show that the likelihood of misclassifying test samples is lower.',\n",
       " 'The performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (82.21%), sensitivity (75.88%), precision (87.51%), specificity (88.76%), and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true labels for several test cases with a small margin of error (actually, the likelihood for mislabeling test cases is <acc_diff> %).',\n",
       " 'The performance of the classifier regarding this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (82.21%), precision (87.51%), sensitivity (75.88%), specificity (88.76%), and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true labels for several test cases with a small margin of error (actually, the likelihood for mislabeling test cases is <acc_diff> %).',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores are high implying that this model will be moderately effective in terms of its predictive power for the majority of test cases/samples. Furthermore, the precision and sensitivity scores show that the likelihood of misclassifying test samples is lower.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores are high implying that this model will be moderately effective in terms of its predictive power for the majority of test cases/samples. Furthermore, the precision and sensitivity scores show that the likelihood of misclassifying test samples is lower.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, precision, sensitivity, specificity, and precision scored 81.28%, 82.21%, 87.51%, 88.76%, 75.88%, and 87.39%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different class labels, #CA and #CB, under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-desperate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-vinyl",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "mineral-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttt= T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affected-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ttt.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "russian-selling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('To summarise, this is a good performing model with high accuracy of 92.78%, although a recall of 81.15% means that the model misclassified <rec_diff> of the majority class C1 as C2. The model performs well, with high accuracy (92.78%) and very high auc (96.38%) and very high precision (98.02). A precision score this high means that 98.02% of identifications predicited as class C1 were actually C1.',\n",
       " [3, 2, 6004, 834, 26, 5982, 3155, 1])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences=[pc['narration'] for pc in full_data]\n",
    "sentences[1],ttt.encode('<acc_diff>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "laden-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ''.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "found-attribute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T h i s c l a s s i '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "grateful-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-panic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-speech",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "pediatric-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics = [[m.lower().strip() for m in test_sample_u[idx]['metrics']] for idx in range(len(test_sample_u))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "furnished-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['specificity', 'accuracy', 'f2score', 'sensitivity', 'precision']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref_metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics = [[m.lower().strip() for m in test_sample[idx]['metrics']] for idx in range(len(test_sample))]\n",
    "\n",
    "# replace metrics such as sensitivity with recall\n",
    "cleans_table_refs =[]\n",
    "for r in copy.deepcopy( table_ref_metrics):\n",
    "    if 'sensitivity' in set(r):\n",
    "        idx = r.index('sensitivity')\n",
    "        r[idx] ='recall'\n",
    "    cleans_table_refs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "above-strike",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> f2score | VALUE_HIGH | 84.33% <|> accuracy | VALUE_HIGH | 86.11% <|> precision | VALUE_HIGH | 89.07% <|> auc | VALUE_HIGH | 90.09% <|> sensitivity | VALUE_HIGH | 84.29% && sensitivity | also_known_as | recall  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['sensitivity', 'auc', 'accuracy', 'precision', 'f2score'],\n",
       " 'values': ['84.29%', '90.09%', '86.11%', '89.07%', '84.33%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"Sensitivity, accuracy, AUC and precision scores of 84.29%, 86.11%, and 89.07% respectively show or indicate how good the model's performance is on this binary classification task. From the F2score, precision and sensitivity scores, we can see that the false positive rate is very low. In summary, only a small number of test cases are likely to be misclassified.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidx=4\n",
    "test_sample[tidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "theoretical-humidity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    }
   ],
   "source": [
    "outp=generateForSampleTopK(test_sample[tidx],use_top_k=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minus-heating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMetricMentions(narration,replace_identical=True,ref_mentions=None):\n",
    "    metrics_list = ['recall', 'sensitivity', 'f1-score',\n",
    "                    'precision', 'f2-score', 'f1','f1score','f2score','g-mean','gmean',\n",
    "                    'f2', 'accuracy',\n",
    "                    'auc',\n",
    "                    'specificity', ]\n",
    "    nn = normalize_text(narration).split()\n",
    "    # replace identical metrics such as recall and sensitivity based on the mention in the table\n",
    "    \n",
    "    # Take care of when recall and sensitivity is mentioned together\n",
    "    \n",
    "    found_metrics = [s.lower()#.replace('-score', '').replace('score', '')\n",
    "                     for s in set(metrics_list).intersection(nn)]\n",
    "    if replace_identical:\n",
    "        if 'recall' in set(found_metrics) and 'sensitivity' in set(found_metrics):\n",
    "            if 'sensitivity' in ref_mentions:\n",
    "                found_metrics = ' '.join(found_metrics).replace('recall','').split()\n",
    "            elif 'sensitivity' in ref_mentions:\n",
    "                found_metrics = ' '.join(found_metrics).replace('sensitivity','').split()\n",
    "    return found_metrics\n",
    "def metricMentionScore(reference_metrics,sys_output):\n",
    "    ref_counts =  np.sum([len(e) for e in reference_metrics])\n",
    "    sys_mentions = [extractMetricMentions(r,ref_mentions=t) for r,t in zip(sys_output,reference_metrics)]\n",
    "    g_sysCount = np.sum([len(e) for e in sys_mentions])\n",
    "    overlap=np.sum([getOverlap(r,b) for r,b in zip(reference_metrics,sys_mentions)])\n",
    "    recall=overlap/ref_counts\n",
    "    precision=overlap/g_sysCount\n",
    "    f1= 2*(recall*precision)/(recall+precision)\n",
    "    return dict(f1_score=round(f1,5),recall=round(recall,5),precision=round(precision,5))#,sys_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hawaiian-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classLabelsEval(reference_classes,generated,penalty =-1):\n",
    "    # In some cases the model will generate the phrase \"binary classification\" or \"multi-class\"\n",
    "    # \"2 classes\", \"3 classes\",\"two labels\", \"two classes\"\n",
    "    false_mentions = 1\n",
    "    if any(ele in generated for ele in ['binary','two classes','two class labels','two-labels','2 classes',\n",
    "                                        'two-classes','two class labels','2 labels','2 classes']) and len(reference_classes)> 2:\n",
    "        # should have generated multi-class\n",
    "        false_mentions = penalty\n",
    "        \n",
    "    if any(ele in generated for ele in ['multi-class', 'multi class', 'multi classes','multi labels',\n",
    "                                        'three-classes','three classes',\n",
    "                                        'three class labels',\n",
    "                                        '3-labels',\n",
    "                                        '3-classes',\n",
    "                                       'four classes',\n",
    "                                        'four class labels',\n",
    "                                        '4 labels',\n",
    "                                        '4 classes']) and len(reference_classes)< 3:\n",
    "        # should have generated binary\n",
    "        false_mentions = penalty\n",
    "        \n",
    "    \n",
    "    classes_tokens = [f'c{i}' for i in range(1, 6)]\n",
    "    nn= normalize_text(generated).split()\n",
    "    found_classes = [s.lower() for s in set(classes_tokens).intersection(nn)]\n",
    "    n_found = 0\n",
    "    label_mentions ={}\n",
    "    label_was_mentioned=False\n",
    "    predicted_text = normalize_text(generated)\n",
    "    for cl in classes_tokens:\n",
    "        c= predicted_text.count(cl)\n",
    "        if cl in reference_classes:\n",
    "            # The class label exists in the reference class list so the model gets a score of 1\n",
    "            label_mentions[cl] = 1\n",
    "        else:\n",
    "            if c >0:\n",
    "                # Invalid class label was mentioned\n",
    "                # A penalty is assigned t\n",
    "                label_mentions[cl] = penalty\n",
    "            else:\n",
    "                # No mention of the class \n",
    "                label_mentions[cl] = 0\n",
    "        if c > 0:\n",
    "            label_was_mentioned=True\n",
    "    \n",
    "    if not label_was_mentioned:\n",
    "        # if no label was mentioned, we assume  they were mentioned implicitly\n",
    "        return 1\n",
    "    else:\n",
    "        # Check to make sure only the right labels were accounted for\n",
    "        score = np.mean([v for v in list(label_mentions.values()) if v !=0]+[false_mentions])\n",
    "        return np.round(score,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fitted-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkForDataDistributionMentions(text,dataset_info):\n",
    "    balanced_list = ['balanced', 'balance','identical', 'similar distribution']\n",
    "    imblanced_list = ['disproportionate','imbalance','class imbalance','imbalanced']\n",
    "    if any(ele in text.split() for ele in balanced_list):\n",
    "        return 'balanced'\n",
    "    if any(ele in text for ele in imblanced_list):\n",
    "        return 'imbalanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "stainless-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "outp=[o.replace('G-Mean','GMean') for o in outp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pending-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['specificity', 'accuracy', 'f2score', 'sensitivity', 'precision']]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "grateful-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f1score', 'precision', 'f2score', 'accuracy', 'specificity', 'sensitivity']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractMetricMentions(normalize_text(outp[0]),ref_mentions=table_ref_metrics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "wanted-lounge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imbalanced'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkForDataDistributionMentions(outp[-1],'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "basic-firmware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the following are the scores achieved by the classifier on this binary classification task : ( 1 ) accuracy equal to 86.11 % with the auc , ( 2 ) sensitivity and ( 3 ) precision equal to 90.09 % and ( 4 ) f1score of 84.33 % . in addition , the prediction performance of the model is high despite the class imbalance . judging based on the scores , it would be safe to conclude that this model has a moderate performance as it is likely to misclassify some test examples especially those drawn randomly from the class label .'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_text(outp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "forward-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.90909, 'recall': 1.0, 'precision': 0.83333}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n",
      "{'f1_score': 0.66667, 'recall': 0.6, 'precision': 0.75}\n"
     ]
    }
   ],
   "source": [
    "for s in outp:\n",
    "    print(metricMentionScore([table_ref_metrics[tidx]],[s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-advice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
