{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "magnetic-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/essel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/essel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 8000\n",
    "import subprocess as sp\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "def mask_unused_gpus(leave_unmasked=1,random=True):\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    try:\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "        available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "        import numpy as np\n",
    "        #print(available_gpus)\n",
    "        if random:\n",
    "           available_gpus = np.asarray(available_gpus)\n",
    "           np.random.shuffle(available_gpus)\n",
    "        if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' %\\\n",
    "                                                                  len(available_gpus))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, available_gpus[:leave_unmasked]))\n",
    "        #print(','.join(map(str, available_gpus[:leave_unmasked])))\n",
    "    except Exception as e:\n",
    "        print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "\n",
    "#mask_unused_gpus(2)\n",
    "mask_unused_gpus(1)\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "#sys.path.append('/content/drive/MyDrive/PerformanceNarrativesRuns/')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "import functools\n",
    "\n",
    "from nltk import wordpunct_tokenize,word_tokenize\n",
    "# Read the dataset\n",
    "full_data= json.load(open('../dataset/annotation_data_with_augmentations.json'))['data']\n",
    "#jdata = json.load(open('james_submission.json'))\n",
    "first_batch= json.load(open('../dataset/annotation_download.json'))\n",
    "full_data = train_data = full_data#+first_batch\n",
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "classes_tokens =[f'C{i}' for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atmospheric-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def calc_stuff(parameter=None):\n",
    "    # Do something.\n",
    "    return 1, 2, 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acknowledged-ability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.33 ms ± 23.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "futures = [calc_stuff.remote(i) for i in range(4)]\n",
    "_=ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sorted-communist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 ns ± 2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for j in range(4):\n",
    "    res = j\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-mortgage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 73.71% && accuracy | VALUE_MODERATE | 74.27% && recall | VALUE_HIGH | 76.21%  <|section-sep|> <TaskDec> ml_task | data_dist | is_balanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_balanced'],\n",
       " 'metrics': ['recall', 'accuracy', 'precision'],\n",
       " 'values': ['76.21%', '74.27%', '73.71%'],\n",
       " 'rates': ['HIGH', 'MODERATE', 'HIGH'],\n",
       " 'narration': 'The classification model under evaluation boasts an accuracy of 74.27%, a recall (sensitivity) and precision of 76.21% and 73.71%, respectively. The model has a fairly moderate prediction performance as shown by the precision and recall scores. The model is fairly confident when you consider the prediction decisions made for the test samples from the class #CA and the class #CB.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc= full_data[123]\n",
    "examples = composePreambleAndInputs(pc,identical_metrics=identicals,augnment_metrics=True,augment_output_order=True)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acquired-bloom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ju op', 'ju op')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_whitespace('ju  op'),'ju op'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processInputTableAndNarrations = composePreambleAndInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "appointed-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #reval_tables.append(parseTableStructureForEval(pc,identicals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pleasant-monitoring",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> accuracy | VALUE_HIGH | 90.67% && sensitivity | VALUE_HIGH | 87.29% && sensitivity | also_known_as | recall && precision | VALUE_HIGH | 91.3% && f1score | VALUE_HIGH | 88.89%  <|section-sep|> <TaskDec> ml_task | data_dist | is_imbalanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_imbalanced'],\n",
       " 'metrics': ['accuracy', 'sensitivity', 'precision', 'f1score'],\n",
       " 'values': ['90.67%', '87.29%', '91.3%', '88.89%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processInputTableAndNarrations(test_data[0],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "increased-psychiatry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> f1score | VALUE_HIGH | 88.89% <|> precision | VALUE_HIGH | 91.3% <|> sensitivity | VALUE_HIGH | 87.29% && sensitivity | also_known_as | recall <|> accuracy | VALUE_HIGH | 90.67%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['f1score', 'precision', 'sensitivity', 'accuracy'],\n",
       " 'values': ['88.89%', '91.3%', '87.29%', '90.67%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processInputTableAndNarrations(test_data[0],reverse_only=True,augnment_metrics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "light-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 95.41% && accuracy | VALUE_HIGH | 95.77% && auc | VALUE_HIGH | 98.62% && recall | VALUE_HIGH | 95.31%  <|section-sep|> <TaskDec> ml_task | data_dist | is_imbalanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_imbalanced'],\n",
       " 'metrics': ['auc', 'recall', 'precision', 'accuracy'],\n",
       " 'values': ['98.62%', '95.31%', '95.41%', '95.77%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': 'All the four reported metrics are very high, with a precision of 95.41 and recall of 95.31, alongside an AUC of 98.62 and accuracy of 95.77. The machine learning model on this classification problem has a very high recall of 95.31, showing that it correctly classifies the majority of the positive class (i.e. has a low number of false negatives). This, alongside the equally high accuracy of 95.77% allows us to conclude that the model performs well on this ML task.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc, identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "superb-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-Augmented Dataset\n",
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc, identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unusual-destruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "pk.dump(processed,open('../dataset/train_dataset_org.dat','wb'))\n",
    "processed = pk.load(open('../dataset/train_dataset_org.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-heather",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "linear-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc, identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)\n",
    "\n",
    "# Reverse\n",
    "repeats=[]\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,\n",
    "                                              reverse_only=True,\n",
    "                                              augment_output_order=False)\n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if len(examples['classes'])> 2:\n",
    "        if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    flg= random.choice([0,1])\n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if len(examples['classes'])> 3 and flg==1:\n",
    "        if tt not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "            \n",
    "\n",
    "\n",
    "## First round of augmentation\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "\n",
    "## Second round of augmentation\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "\n",
    "## third round of augmentation\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "\n",
    "## fourth round of augmentation\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)\n",
    "\n",
    "## fifth round of augmentation\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-facing",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef1cb62a174a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "digital-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sixth round of augmentation\n",
    "for idx,pc in enumerate(random.sample(full_data,300)):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    \n",
    "    tt= examples['preamble']+examples['narration']\n",
    "    if tt not in set(preambles):\n",
    "            preambles.append(tt)\n",
    "            processed.append(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collected-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "pk.dump(processed,open('../dataset/train_dataset_new.dat','wb'))\n",
    "processed = pk.load(open('../dataset/train_dataset_new.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "different-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> auc | VALUE_HIGH | 96.08% && precision | VALUE_HIGH | 82.64% && recall | VALUE_HIGH | 94.72% && accuracy | VALUE_HIGH | 89.12%  <|section-sep|> <TaskDec> ml_task | data_dist | is_balanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['is_balanced'],\n",
       " 'metrics': ['recall', 'accuracy', 'auc', 'precision'],\n",
       " 'values': ['94.72%', '89.12%', '96.08%', '82.64%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': 'This algorithm employed to solve this binary classification problem is shown to be very effective with accuracy, precision and AUC scores of 89.12%, 94.72% and 96.08%. It has a slightly lower precision score of 82.64%. Overall, 89.12% of predictions are correct and an almost perfect AUC score of 96.08% means the model is highly effective in terms of separating the test observations under the different classes.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-scottish",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-reputation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-looking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collected-mumbai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "specific-diploma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preambles.index('<MetricsInfo> precision | VALUE_HIGH | 82.64% <|> auc | VALUE_HIGH | 96.08% <|> accuracy | VALUE_HIGH | 89.12% <|> recall | VALUE_HIGH | 94.72%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "arbitrary-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> recall | VALUE_HIGH | 94.72% <|> precision | VALUE_HIGH | 82.64% <|> auc | VALUE_HIGH | 96.08% <|> accuracy | VALUE_HIGH | 89.12%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['recall', 'accuracy', 'auc', 'precision'],\n",
       " 'values': ['94.72%', '89.12%', '96.08%', '82.64%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': 'This algorithm employed to solve this binary classification problem is shown to be very effective with accuracy, precision and AUC scores of 89.12%, 94.72% and 96.08%. It has a slightly lower precision score of 82.64%. Overall, 89.12% of predictions are correct and an almost perfect AUC score of 96.08% means the model is highly effective in terms of separating the test observations under the different classes.'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mobile-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if len(examples['classes'])> 2:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    flg= random.choice([0,1])\n",
    "    \n",
    "    if len(examples['classes'])> 3 and flg==1:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)        \n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "'''\n",
    "sample_rounds = 10\n",
    "sample_size = 500\n",
    "for i in range(sample_rounds):\n",
    "    for idx,pc in enumerate(random.sample(full_data,sample_size)):\n",
    "        examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples) \n",
    "\n",
    "'''\n",
    "       \n",
    "        \n",
    "random.shuffle(processed)\n",
    "random.shuffle(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "intensive-douglas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3203"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intended-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "given-lightning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4977"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "directed-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanRatingPreamble(preamble):\n",
    "    # This function will replace the VALUE_LOW, VALUE_HIGH and VALUE_MODERATE mentions in the preamble a common token VALUE\n",
    "    preamble_dict = {'VALUE_LOW':'VALUE','VALUE_HIGH':'VALUE','VALUE_MODERATE':'VALUE'}\n",
    "    preamble = [functools.reduce(lambda a, kv: a.replace(*kv),\n",
    "                preamble_dict.items(),\n",
    "                                 re.sub('\\s+', ' ', ss.strip().replace('\\n', ' '))) for ss in [preamble]][0]\n",
    "    return preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "inappropriate-defendant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<MetricsInfo> accuracy | VALUE | 89.12% <|> auc | VALUE | 96.08% <|> recall | VALUE | 94.72% <|> precision | VALUE | 82.64% <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB <|section-sep|> <|table2text|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanRatingPreamble(preambles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "successful-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> sensitivity | VALUE_HIGH | 88.89% && sensitivity | also_known_as | recall <|> accuracy | VALUE_HIGH | 85.53% <|> f2score | VALUE_HIGH | 88.15% <|> precision | VALUE_HIGH | 85.33% <|> specificity | VALUE_HIGH | 82.59%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['specificity', 'accuracy', 'sensitivity', 'precision', 'f2score'],\n",
       " 'values': ['82.59%', '85.53%', '88.89%', '85.33%', '88.15%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"Evaluating the classifier's prowess on the classification task produced the scores 85.33%, 88.89%, 82.59%, 85.53%, and 88.15%, respectively, across the metrics precision, sensitivity, specificity, accuracy, and F2score. The difference between the precision, and sensitivity scores indicates that the classifier is very confident about its #CB predictions. Similarly, the specificity score also suggests the confidence with respect to #CA predictions is also high. From the above statements, we can conclude that the classifier has a good classification ability, only misclassifying a small percentage of all possible test cases.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instructional-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if len(examples['classes'])> 2:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "\n",
    "random.shuffle(processed)\n",
    "random.shuffle(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "automotive-fruit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "premier-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mobile-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "difficult-assumption",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-21993b65c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Clean all the preamble to strip them of the ratings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw_preamble'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanRatingPreamble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preamble'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processed' is not defined"
     ]
    }
   ],
   "source": [
    "# Clean all the preamble to strip them of the ratings\n",
    "for pc in processed:\n",
    "    pc['raw_preamble'] = cleanRatingPreamble(pc['preamble'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-vision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intermediate-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "pk.dump(processed,open('../dataset/train_dataset_full.dat','wb'))\n",
    "processed = pk.load(open('../dataset/train_dataset_full.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6874a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4977"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "future-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 90.98% <|> accuracy | VALUE_HIGH | 93.42% <|> recall | VALUE_HIGH | 88.13% <|> f1score | VALUE_HIGH | 89.42%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA, #CB and #CC  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB', '#CC'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['precision', 'recall', 'f1score', 'accuracy'],\n",
       " 'values': ['90.98%', '88.13%', '89.42%', '93.42%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The algorithm's prediction performance on the given multi-class classification problem where the test instances are classified as either #CA or #CB or #CC is: (a) Accuracy = 93.42%. (b) Precision = 90.98%. (c) F1score = 89.42%. (d) Recall = 88.13%. On this multi-class problem, the algorithm is shown to perform very well across all the evaluation metrics under consideration. The scores across the different metrics indicate that it is very effective and precise at correctly labeling most of the test observations.\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[593]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "restricted-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from model_utils import setupTokenizer\n",
    "modeltype = 'baseline'\n",
    "modelbase='facebook/bart-large'\n",
    "tokenizer = tokenizer_ = setupTokenizer(modelbase=modelbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "medieval-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RDFDataSetForTableStructured(tokenizer_,  processed, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True,use_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "public-overview",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble_tokens': tensor([    0, 50267,  5234, 37938,  1721, 50270, 15483,  1814,     4,  5208,\n",
       "           207, 50265,  7904, 45386,  1721, 50270, 15483,  8060,     4,  3714,\n",
       "           207, 50265, 13139,  1250,  1721, 50270, 15483,  7953,     4,  1558,\n",
       "           207, 50265,   506,   134, 31673,  1721, 50270, 15483,  8572,     4,\n",
       "          3714,   207, 50269, 50266, 50281, 15483, 50287, 15483, 50283, 49145,\n",
       "         50281, 15483, 50284, 15483, 50273,     6, 50274,   463, 50275, 50269,\n",
       "         50268,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'preamble_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'class_labels': tensor([50273, 50274, 50275,     0,     0,     0,     0,     0],\n",
       "        dtype=torch.int32),\n",
       " 'data_info': tensor([50283]),\n",
       " 'metrics_seq': tensor([    0,  5234, 37938,     2,     1,     1,     1,     1,     0, 13139,\n",
       "          1250,     2,     1,     1,     1,     1,     0,   506,   134, 31673,\n",
       "             2,     1,     1,     1,     0,  7904, 45386,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        dtype=torch.int32),\n",
       " 'metrics_attention': tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32),\n",
       " 'values': tensor([   0, 3248,    4, 5208,  207,    2,    1,    1,    0, 4652,    4, 1558,\n",
       "          207,    2,    1,    1,    0, 5046,    4, 3714,  207,    2,    1,    1,\n",
       "            0, 6478,    4, 3714,  207,    2,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        dtype=torch.int32),\n",
       " 'rates': tensor([    0,   725, 29878,     2,     1,     1,     1,     1,     0,   725,\n",
       "         29878,     2,     1,     1,     1,     1,     0,   725, 29878,     2,\n",
       "             1,     1,     1,     1,     0,   725, 29878,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        dtype=torch.int32),\n",
       " 'labels': tensor([    0,   133, 17194,    18, 16782,   819,    15,     5,   576,  3228,\n",
       "            12,  4684, 20257,   936,   147,     5,  1296, 10960,    32,  8967,\n",
       "            25,  1169, 50273,   368, 50274,   368, 50275,   354,    35,    36,\n",
       "           102,    43, 42688,  5457,  8060,     4,  3714,  2153,    36,   428,\n",
       "            43, 29484,  5457,  1814,     4,  5208,  2153,    36,   438,    43,\n",
       "         50297,  5214,  8572,     4,  3714,  2153,    36,   417,    43, 35109,\n",
       "          5457,  7953,     4,  1558,  2153,   374,    42,  3228,    12,  4684,\n",
       "           936,     6,     5, 17194,    16,  2343,     7,  3008,   182,   157,\n",
       "           420,    70,     5, 10437, 12758,   223,  6077,     4,    20,  4391,\n",
       "           420,     5,   430, 12758,  6364,    14,    24,    16,   182,  2375,\n",
       "             8, 12548,    23, 12461, 27963,   144,     9,     5,  1296, 15864,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'labels_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rate_attention': tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32),\n",
       " 'value_attention': tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.processTableInfo(processed[593])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incredible-border",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble_tokens': tensor([    0, 50267,  5234, 37938,  1721, 50271, 15483,  5545,     4,  6668,\n",
       "           207, 50265, 13139,  1250,  1721, 50271, 15483,  3620,     4,  4563,\n",
       "           207, 50265,   506,   176, 31673,  1721, 50271, 15483,  4268,     4,\n",
       "          2546,   207, 50265,  7904, 45386,  1721, 50272, 15483,  2491,     4,\n",
       "          3272,   207, 50269, 50266, 50281, 15483, 50287, 15483, 50283, 49145,\n",
       "         50281, 15483, 50284, 15483, 50273,     6, 50274,   463, 50275, 50269,\n",
       "         50268,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'preamble_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'class_labels': tensor([50273, 50274, 50275,     0,     0,     0,     0,     0],\n",
       "        dtype=torch.int32),\n",
       " 'data_info': tensor([50283]),\n",
       " 'metrics_seq': tensor([    0,  5234, 37938,     2,     1,     1,     1,     1,     0, 13139,\n",
       "          1250,     2,     1,     1,     1,     1,     0,   506,   176, 31673,\n",
       "             2,     1,     1,     1,     0,  7904, 45386,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        dtype=torch.int32),\n",
       " 'metrics_attention': tensor([1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32),\n",
       " 'values': tensor([   0, 4111,    4, 6668,  207,    2,    1,    1,    0, 3506,    4, 4563,\n",
       "          207,    2,    1,    1,    0, 4540,    4, 2546,  207,    2,    1,    1,\n",
       "            0, 3367,    4, 3272,  207,    2,    1,    1,    1,    1,    1,    1,\n",
       "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
       "        dtype=torch.int32),\n",
       " 'rates': tensor([    0, 45997,  2076,  8625,     2,     1,     1,     1,     0, 45997,\n",
       "          2076,  8625,     2,     1,     1,     1,     0, 45997,  2076,  8625,\n",
       "             2,     1,     1,     1,     0,   574,  4581,     2,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1],\n",
       "        dtype=torch.int32),\n",
       " 'labels': tensor([    0,   133, 20257,   819,    50, 23218, 32069,    30,     5,  1421,\n",
       "            15,    42,  3228,    12,  4684, 20257,   936,   147,     5,  1296,\n",
       "         10960,    32,  8967,    25,  1169, 50273,   368, 50274,   368, 50275,\n",
       "           354, 38152,    25,  3905,    35,    10,     4, 42688,    36,  3367,\n",
       "             4,  3272, 20186,   741,     4, 35109,    36,  3506,     4,  4563,\n",
       "         20186,   740,     4,    10, 29484,  1471,     9,  5545,     4,  6668,\n",
       "          4234,   385,     4, 50298, 30349,     7,  4268,     4,  2546,  2153,\n",
       "          1216,  4391,   420,     5,   430, 12758,  3608,    14,    42,  1421,\n",
       "            16,   540,  2375,     8,   540, 12548,    36,  5652,   421,    43,\n",
       "            11,  1110,     9, 12775, 15924,     5,  1528, 14105,     9,   484,\n",
       "          1296,  7721,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1]),\n",
       " 'labels_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'rate_attention': tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32),\n",
       " 'value_attention': tensor([1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-great",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
