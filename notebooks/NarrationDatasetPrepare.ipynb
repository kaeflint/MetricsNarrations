{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "magnetic-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/essel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/essel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 8000\n",
    "import subprocess as sp\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "def mask_unused_gpus(leave_unmasked=1,random=True):\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    try:\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "        available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "        import numpy as np\n",
    "        #print(available_gpus)\n",
    "        if random:\n",
    "           available_gpus = np.asarray(available_gpus)\n",
    "           np.random.shuffle(available_gpus)\n",
    "        if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' %\\\n",
    "                                                                  len(available_gpus))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, available_gpus[:leave_unmasked]))\n",
    "        #print(','.join(map(str, available_gpus[:leave_unmasked])))\n",
    "    except Exception as e:\n",
    "        print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "\n",
    "#mask_unused_gpus(2)\n",
    "mask_unused_gpus(1)\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "#sys.path.append('/content/drive/MyDrive/PerformanceNarrativesRuns/')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "import functools\n",
    "\n",
    "from nltk import wordpunct_tokenize,word_tokenize\n",
    "# Read the dataset\n",
    "full_data= json.load(open('../dataset/annotation_data_with_augmentations.json'))['data']\n",
    "#jdata = json.load(open('james_submission.json'))\n",
    "first_batch= json.load(open('../dataset/annotation_download.json'))\n",
    "full_data = train_data = full_data+first_batch\n",
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "classes_tokens =[f'C{i}' for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-mortgage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> f1score | VALUE_LOW | 48.54% <|> recall | VALUE_MODERATE | 70.12% <|> accuracy | VALUE_MODERATE | 81.13% <|> precision | VALUE_LOW | 37.12%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['recall', 'f1score', 'precision', 'accuracy'],\n",
       " 'values': ['70.12%', '48.54%', '37.12%', '81.13%'],\n",
       " 'rates': ['MODERATE', 'LOW', 'LOW', 'MODERATE'],\n",
       " 'narration': \"The machine learning algorithm trained on this classification task was evaluated and it achieved a low F1score of 48.54% with a very low precision of 37.12% and a moderate recall (i.e. the prediction sensitivity) score of 70.12%. The accuracy score of 81.13% is not that impressive as the dummy model assigning the majority class #CA to any given input can achieve close to this performance. The model's overall classification performance is very poor since it achieved lower values/scores for both the precision and F1score. In summary, confidence in the model's prediction decision related to the minority label #CB is low and should be taken with caution.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc= full_data[123]\n",
    "examples = composePreambleAndInputs(pc,identical_metrics=identicals,augnment_metrics=True,augment_output_order=True)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processInputTableAndNarrations = composePreambleAndInputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #reval_tables.append(parseTableStructureForEval(pc,identicals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    #print(idx)\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)\n",
    "    \n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "        \n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if len(examples['classes'])> 2:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "\n",
    "random.shuffle(processed)\n",
    "random.shuffle(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premier-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mobile-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "intermediate-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "#pk.dump(processed,open('../dataset/train_dataset.dat','wb'))\n",
    "processed = pk.load(open('../dataset/train_dataset.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6874a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2301"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "future-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_MODERATE | 60.32% <|> accuracy | VALUE_HIGH | 63.97% <|> f1score | VALUE_MODERATE | 60.8%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['f1score', 'precision', 'accuracy'],\n",
       " 'values': ['60.8%', '60.32%', '63.97%'],\n",
       " 'rates': ['MODERATE', 'MODERATE', 'HIGH'],\n",
       " 'narration': 'The F1score, accuracy and precision are 60.8%, 63.97% and 60.32%, respectively. The given F1score and accuracy scoring is indicative of a model with fairly good signs of being accurate and precises in determining #CA and #CB. However, the models only perform decently well, with still room for improvement, and with similar precision and accuracy scores suggesting a combined issue with the model.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
