{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "invalid-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import functools\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../TrainedNarrators/')\n",
    "from data_utils import *\n",
    "import subprocess as sp\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup,get_cosine_schedule_with_warmup\n",
    "from losses import *\n",
    "from torch.utils.data import DataLoader,RandomSampler\n",
    "import pickle as pk\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "processed = pk.load(open('../dataset/train_dataset_new.dat', 'rb'))\n",
    "#'../dataset/train_dataset_org.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "signal-pierre",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *\n",
    "from model_utils import setupTokenizer\n",
    "learning_rate = 3e-4\n",
    "batch_size = 8\n",
    "epochs = 10\n",
    "warmup_steps = 0\n",
    "modeltype = 'earlyfusion'\n",
    "modelbase='facebook/bart-large'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "damaged-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "tokenizer = tokenizer_ = setupTokenizer(modelbase=modelbase)\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#processed = pk.load(open('../dataset/train_dataset_org.dat', 'rb'))\n",
    "dataset = RDFDataSetForTableStructured(tokenizer_,  processed, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weekly-moisture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Type: earlyfusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DataNarrationBart were not initialized from the model checkpoint at facebook/bart-large and are newly initialized: ['encoder.relative_attention_for_table.query.bias', 'encoder.relative_attention_for_table.key.weight', 'encoder.relative_attention_for_table.query.weight', 'encoder.relative_attention_for_table.value.weight', 'encoder.relative_attention_for_table.key.bias', 'encoder.relative_attention_for_table.mask', 'encoder.relative_attention_for_table.value.bias', 'encoder.relative_attention_for_table.Er']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "if 'bart' in modelbase:\n",
    "    from narrations_models import BartNarrationModel\n",
    "    model_generator = BartNarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)\n",
    "else:\n",
    "    from narrations_models import T5NarrationModel\n",
    "    model_generator = T5NarrationModel(\n",
    "    vocab_size=len(tokenizer), model_type=modeltype,modelbase=modelbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "listed-implement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4529\n",
      "Using the Rating Information\n",
      "4,529 training samples\n",
      "  100 validation samples\n"
     ]
    }
   ],
   "source": [
    "use_raw=False\n",
    "#processed = pk.load(open('../dataset/train_dataset_new.dat', 'rb'))\n",
    "print(len(processed))\n",
    "\n",
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))\n",
    "    # eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))\n",
    "\n",
    "\n",
    "if use_raw:\n",
    "    print('Using Raw data without ratings')\n",
    "else:\n",
    "    print('Using the Rating Information')\n",
    "\n",
    "dataset = RDFDataSetForTableStructured(tokenizer_,  processed, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True,use_raw= use_raw)\n",
    "test_dataset = RDFDataSetForTableStructured(tokenizer_, test_sample, modelbase,max_preamble_len=160,\n",
    "                                       max_len_trg=180, max_rate_toks=8,\n",
    "                                       lower_narrations=True, process_target=True,use_raw= use_raw)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(len(dataset))\n",
    "\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = dataset, test_dataset\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # The training samples.\n",
    "    sampler=RandomSampler(train_dataset)  # Select batches randomly\n",
    "    , batch_size=batch_size  # Trains with this batch size.\n",
    ")\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,  # The validation samples.\n",
    "    sampler=SequentialSampler(val_dataset),  # Pull out batches sequentially.\n",
    "    batch_size=1  # Evaluate with this batch size.\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,  # The validation samples.\n",
    "    sampler=SequentialSampler(test_dataset),  # Pull out batches sequentially.\n",
    "    batch_size=4  # Evaluate with this batch size.\n",
    ")\n",
    "val_size = int(len(test_dataset))\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "legal-halifax",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    }
   ],
   "source": [
    "seed_everything(43)\n",
    "epsilon = 1e-8\n",
    "lr = learning_rate = 1e-4\n",
    "epochs = 10\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "warmup_ratio =0.15\n",
    "warmup_steps = int(total_steps*warmup_ratio)\n",
    "accumulation_steps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-surfing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6230*warmup_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "opened-white",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(850, 5670)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps,total_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "standard-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation Completed\n"
     ]
    }
   ],
   "source": [
    "# compile the model setting up the optimizer and the learning rate schedule\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "model_generator.compile(\n",
    "    lr=learning_rate, warmup_steps=warmup_steps, total_steps=total_steps, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "endangered-indonesia",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_only = True\n",
    "def baselineTraining(step,batch):\n",
    "    preamble_tokens = batch['preamble_tokens'].to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    #labels[labels == 0] = -100\n",
    "    #labels[labels == -100] = 0\n",
    "\n",
    "    decoder_attention_mask = batch['labels_attention_mask'].to(device)\n",
    "    outputs = model_generator.generator(input_ids=preamble_tokens,\n",
    "                                        attention_mask=preamble_attention_mask,\n",
    "                                        labels=labels,\n",
    "                                        decoder_attention_mask=decoder_attention_mask,\n",
    "\n",
    "                                        )\n",
    "    # Total loss is the info_loss and the LM loss\n",
    "    loss = computeLoss(outputs[1],labels,rank_alpha=0.7,mle_only=mle_only,ignore_index=1,padding_idx=1)/ accumulation_steps#.item()\n",
    "    # print(loss,info_loss)\n",
    "    # last_hidden_states = outputs.hidden_states[-1]\n",
    "    # print(last_hidden_states.shape)\n",
    "\n",
    "    batch_loss = loss.item()  # + info_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        model_generator.optimizer.step()                           # Now we can do an optimizer step\n",
    "        model_generator.scheduler.step()\n",
    "        model_generator.generator.zero_grad()\n",
    "        \n",
    "    # info_loss.backward()\n",
    "    # optimizer2.step()\n",
    "\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "outer-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_only = True\n",
    "def baselineTraining(step,batch):\n",
    "    preamble_tokens = batch['preamble_tokens'].to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    #labels[labels == 0] = -100\n",
    "    #labels[labels == -100] = 0\n",
    "\n",
    "    decoder_attention_mask = batch['labels_attention_mask'].to(device)\n",
    "    outputs = model_generator.generator(input_ids=preamble_tokens,\n",
    "                                        attention_mask=preamble_attention_mask,\n",
    "                                        labels=labels,\n",
    "                                        decoder_attention_mask=decoder_attention_mask,\n",
    "\n",
    "                                        )\n",
    "    # Total loss is the info_loss and the LM loss\n",
    "    loss = computeLoss(outputs[1],labels,rank_alpha=0.7,mle_only=mle_only,ignore_index=1,padding_idx=1)/ accumulation_steps#.item()\n",
    "    # print(loss,info_loss)\n",
    "    # last_hidden_states = outputs.hidden_states[-1]\n",
    "    # print(last_hidden_states.shape)\n",
    "\n",
    "    batch_loss = loss.item()  # + info_loss\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        model_generator.optimizer.step()                           # Now we can do an optimizer step\n",
    "        model_generator.scheduler.step()\n",
    "        model_generator.generator.zero_grad()\n",
    "        \n",
    "    # info_loss.backward()\n",
    "    # optimizer2.step()\n",
    "\n",
    "    return batch_loss\n",
    "# Training step for the fusion model\n",
    "def FusionModelsTraining(step,batch):\n",
    "    met, rate, val = batch['metrics_seq'].to(\n",
    "        device), batch['rates'].to(device), batch['values'].to(device)\n",
    "    clb, di = batch['class_labels'].to(\n",
    "        device), batch['data_info'].to(device)\n",
    "    met_att = batch['metrics_attention'].to(device)\n",
    "    rate_att = batch['rate_attention'].to(device)\n",
    "    val_att = batch['value_attention'].to(device)\n",
    "\n",
    "    preamble_tokens = batch['preamble_tokens'].to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    #labels[labels == 0] = -100\n",
    "    #labels[labels == -100] = 0\n",
    "    table_rep = model_generator.performAuxEncoding([met.detach().clone(), met_att.detach().clone()],\n",
    "                                                   [val.detach().clone(),\n",
    "                                                    val_att.detach().clone()],\n",
    "                                                   [rate.detach().clone(), rate_att.detach().clone()])\n",
    "\n",
    "    decoder_attention_mask = batch['labels_attention_mask'].to(device)\n",
    "\n",
    "    outputs = model_generator.generator(input_ids=preamble_tokens,\n",
    "                                        attention_mask=preamble_attention_mask,\n",
    "                                        table_inputs=table_rep,\n",
    "                                        table_attention_mask=None,\n",
    "                                        labels=labels,\n",
    "                                        decoder_attention_mask=decoder_attention_mask\n",
    "                                        )\n",
    "    # Total loss is the info_loss and the LM loss\n",
    "    #loss = outputs[0].mean()\n",
    "    loss = computeLoss(outputs[1],labels,rank_alpha=0.7,mle_only=mle_only,ignore_index=1,padding_idx=1)/ accumulation_steps#.item()\n",
    "    batch_loss = loss.item()  # + info_loss\n",
    "    loss.backward()\n",
    "    if (step+1) % accumulation_steps == 0:             # Wait for several backward steps\n",
    "        model_generator.optimizer.step()                           # Now we can do an optimizer step\n",
    "        model_generator.scheduler.step()\n",
    "        model_generator.generator.zero_grad()\n",
    "        model_generator.aux_encoder.zero_grad()\n",
    "        \n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "# Set up the function for training the model\n",
    "def trainNarrator(train_dataset_loader, epochs):\n",
    "    print('======== Beginning Model Training ======')\n",
    "\n",
    "    # initialize the time keeper\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    training_stats = []\n",
    "    gama = 0\n",
    "    for epoch_i in tqdm(range(0, epochs)):\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        print(\"\")\n",
    "        print(\n",
    "            '======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        total_train_loss = 0\n",
    "\n",
    "        model_generator.generator.train()\n",
    "        model_generator.generator.zero_grad()\n",
    "        \n",
    "        if model_generator.aux_encoder is not None:\n",
    "            model_generator.aux_encoder.train()\n",
    "            model_generator.aux_encoder.zero_grad()\n",
    "        for step, batch in enumerate(train_dataset_loader):\n",
    "            if model_generator.model_type == 'baseline':\n",
    "                batch_loss = baselineTraining(step,batch)\n",
    "\n",
    "            else:\n",
    "                batch_loss = FusionModelsTraining(step,batch)\n",
    "            total_train_loss += batch_loss\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataset_loader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = format_time(time.time() - t0)\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(\n",
    "        format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "nasty-composition",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Beginning Model Training ======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [02:57<26:34, 177.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 474.00\n",
      "  Training epoch took: 0:02:57\n",
      "\n",
      "======== Epoch 2 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [05:54<23:38, 177.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 216.46\n",
      "  Training epoch took: 0:02:57\n",
      "\n",
      "======== Epoch 3 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [08:52<20:42, 177.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 170.54\n",
      "  Training epoch took: 0:02:58\n",
      "\n",
      "======== Epoch 4 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [11:50<17:47, 177.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 147.15\n",
      "  Training epoch took: 0:02:58\n",
      "\n",
      "======== Epoch 5 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [14:49<14:51, 178.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 131.69\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "======== Epoch 6 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [17:48<11:53, 178.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 119.34\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "======== Epoch 7 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [20:47<08:55, 178.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 108.47\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "======== Epoch 8 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [23:46<05:57, 178.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 97.95\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "======== Epoch 9 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [26:44<02:58, 178.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 91.09\n",
      "  Training epoch took: 0:02:58\n",
      "\n",
      "======== Epoch 10 / 10 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [29:43<00:00, 178.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Average training loss: 85.35\n",
      "  Training epoch took: 0:02:59\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:29:44 (h:mm:ss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainNarrator(train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "italic-broad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "allied-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAndEvaluate(data_loader,sample_too=False,seed=43):\n",
    "    model_generator.generator.eval()\n",
    "    if model_generator.aux_encoder is not None:\n",
    "        model_generator.aux_encoder.eval()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    generated_output_dict = {}\n",
    "\n",
    "    for bs in range(10):\n",
    "        bs= bs+1\n",
    "        generated_outputs = []\n",
    "        seed_everything(seed)\n",
    "        \n",
    "        for idx, batch in enumerate(data_loader):\n",
    "            \n",
    "           \n",
    "            met, rate, val = batch['metrics_seq'].to(\n",
    "        device), batch['rates'].to(device), batch['values'].to(device)\n",
    "            clb, di = batch['class_labels'].to(\n",
    "                device), batch['data_info'].to(device)\n",
    "            met_att = batch['metrics_attention'].to(device)\n",
    "            rate_att = batch['rate_attention'].to(device)\n",
    "            val_att = batch['value_attention'].to(device)\n",
    "\n",
    "            preamble_tokens = batch['preamble_tokens'].to(device)\n",
    "            preamble_attention_mask = batch['preamble_attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            if model_generator.aux_encoder is not None:\n",
    "                table_rep = model_generator.performAuxEncoding(\n",
    "                    [met, met_att], [val, val_att], [rate, rate_att])\n",
    "                #table_rep= torch.ones_like(table_rep)\n",
    "                #return table_rep\n",
    "                sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=1,\n",
    "                                                                    do_sample=sample_too\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "            else:\n",
    "                sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=1,\n",
    "                                                                    do_sample=sample_too\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "            ss = [tokenizer.decode(s,\n",
    "                                  skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True) for s in sample_outputs]\n",
    "            \n",
    "            #break\n",
    "            generated_outputs+=ss\n",
    "        \n",
    "        print(f'Generation based on beam size {bs} is complete')\n",
    "        #computeParentScore(refs=refs, predicted=generated_outputs, tables=eval_tables)\n",
    "        print('\\n')\n",
    "        generated_output_dict[bs] = generated_outputs\n",
    "    return generated_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "arranged-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed_everything(43)\n",
    "def generateForSampleTopK(prompt,bs=4,use_top_k=False,seed=43):\n",
    "    seed_everything(seed)\n",
    "    model_generator.generator.eval()\n",
    "    if model_generator.aux_encoder is not None:\n",
    "        model_generator.aux_encoder.eval()\n",
    "    batch = dataset.processTableInfo(prompt)\n",
    "    # batch=dataset.processTableInfo(test_sample[tidx])\n",
    "    clb, di = batch['class_labels'].unsqueeze(0).to(\n",
    "                device), batch['data_info'].unsqueeze(0).to(device)\n",
    "    met, rate, val = batch['metrics_seq'].unsqueeze(0).to(device), batch['rates'].unsqueeze(\n",
    "                0).to(device), batch['values'].unsqueeze(0).to(device)\n",
    "    preamble_tokens = batch['preamble_tokens'].unsqueeze(\n",
    "                0).to(device)\n",
    "    preamble_attention_mask = batch['preamble_attention_mask'].unsqueeze(\n",
    "                0).to(device)\n",
    "    met_att = batch['metrics_attention'].unsqueeze(0).to(device)\n",
    "    rate_att = batch['rate_attention'].unsqueeze(0).to(device)\n",
    "    val_att = batch['value_attention'].unsqueeze(0).to(device)\n",
    "    \n",
    "    \n",
    "    if model_generator.aux_encoder is not None:\n",
    "        table_rep = model_generator.performAuxEncoding(\n",
    "                    [met, met_att], [val, val_att], [rate, rate_att])\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    do_sample=True, top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=2.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=bs,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        \n",
    "    else:\n",
    "        if use_top_k:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    do_sample=True,\n",
    "                                                                top_k=50, \n",
    "    top_p=0.95,\n",
    "                                                                    max_length = 190,\n",
    "                                                                early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                     \n",
    "                                                                    num_return_sequences=15,\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "        else:\n",
    "            sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=2.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    #num_return_sequences=bs,\n",
    "                                                                    do_sample=True\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "    ss = [tokenizer.decode(s,\n",
    "                                  skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True) for s in sample_outputs]\n",
    "    return ss, sample_outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "catholic-sally",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'preamble': '<MetricsInfo> f1score | VALUE_HIGH | 80.47% && specificity | VALUE_HIGH | 78.74% && sensitivity | VALUE_HIGH | 82.11% && sensitivity | also_known_as | recall && accuracy | VALUE_HIGH | 80.4% && precision | VALUE_HIGH | 78.91%  <|section-sep|> <TaskDec> ml_task | data_dist | is_balanced && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ', 'classes': ['#CA', '#CB'], 'dataset_attribute': ['is_balanced'], 'metrics': ['f1score', 'specificity', 'precision', 'sensitivity', 'accuracy'], 'values': ['80.47%', '78.74%', '78.91%', '82.11%', '80.4%'], 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'], 'narration': \"The score achieved by the model on this classification task are 80.47%, 78.74%, 78.91%, 82.11%, and 80.4%, respectively across the evaluation metrics F1score, specificity, precision, sensitivity, and accuracy. These scores are moderately high suggesting it can correctly predict the labels of test cases with a small margin of error. Finally, the model's confidence is fairly high according to the F1score and accuracy score.\"}\n"
     ]
    }
   ],
   "source": [
    "tidx=28\n",
    "pc = test_data[tidx]\n",
    "prep=processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals,augnment_metrics=False,reverse_only=False)\n",
    "print(prep)\n",
    "outp,vb=generateForSampleTopK(prep,bs=6,use_top_k=False,seed=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "empty-verification",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 80.4%; (b) Specificity score of 78.74%; and (c) Sensitivity score is 82.11%. These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true label for most test cases.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "physical-contractor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (98.45%), AUC (99.04%), Sensitivity (90.2%), and finally, an F1score of 93.95%. With such high scores across the different metrics under consideration, we can be certain that this model will be highly effective at correctly predicting the true label for the majority of test cases. Furthermore, the likelihood of misclassification is very marginal.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-qatar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "continent-wheel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On this multi-class classification problem, where the test instances are classified as either #CA or #CB or #CC or #CD, the classification performance of the learning algorithm is summarized by the following evaluation scores: (a) Accuracy equal to 73.78%. (b) Precision score equals 79.09%; (c) Recall (or Sensitivity) score is 74.77%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the labels #CA, #CB, and #CC. Furthermore, from the accuracy and recall scores, we can conclude that it will likely misclassify a small number of samples drawn randomly from each class.']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-transition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "following-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "informal-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "SequentialSampler??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "illegal-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 160])\n",
      "<MetricsInfo> recall | VALUE_HIGH | 63.49% <|> f1score | VALUE_MODERATE | 62.07% <|> accuracy | VALUE_MODERATE | 62.5% <|> precision | VALUE_MODERATE | 66.95%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA, #CB and #CC  <|section-sep|> <|table2text|> \n",
      "<s> <MetricsInfo> recall | VALUE_HIGH | 63.49% <|> f1score | VALUE_MODERATE | 62.07% <|> accuracy | VALUE_MODERATE | 62.5% <|> precision | VALUE_MODERATE | 66.95% <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA, #CB and #CC <|section-sep|> <|table2text|> </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(test_dataloader):\n",
    "    print(idx, batch['preamble_tokens'].shape)\n",
    "    print(test_sample[3]['preamble'])\n",
    "    print(tokenizer.decode(batch['preamble_tokens'][3]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "tested-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "raised-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uuugenerateAndEvaluate(seed=43):\n",
    "    model_generator.generator.eval()\n",
    "    if model_generator.aux_encoder is not None:\n",
    "        model_generator.aux_encoder.eval()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    generated_output_dict = {}\n",
    "\n",
    "    for bs in range(10):\n",
    "        bs= bs+1\n",
    "        generated_outputs = []\n",
    "        seed_everything(seed)\n",
    "        for prompt in test_sample:\n",
    "            tt = prompt\n",
    "            batch = dataset.processTableInfo(tt)\n",
    "            # batch=dataset.processTableInfo(test_sample[tidx])\n",
    "            clb, di = batch['class_labels'].unsqueeze(0).to(\n",
    "                device), batch['data_info'].unsqueeze(0).to(device)\n",
    "            met, rate, val = batch['metrics_seq'].unsqueeze(0).to(device), batch['rates'].unsqueeze(\n",
    "                0).to(device), batch['values'].unsqueeze(0).to(device)\n",
    "            preamble_tokens = batch['preamble_tokens'].unsqueeze(\n",
    "                0).to(device)\n",
    "            preamble_attention_mask = batch['preamble_attention_mask'].unsqueeze(\n",
    "                0).to(device)\n",
    "            met_att = batch['metrics_attention'].unsqueeze(0).to(device)\n",
    "            rate_att = batch['rate_attention'].unsqueeze(0).to(device)\n",
    "            val_att = batch['value_attention'].unsqueeze(0).to(device)\n",
    "\n",
    "            if model_generator.aux_encoder is not None:\n",
    "                table_rep = model_generator.performAuxEncoding(\n",
    "                    [met, met_att], [val, val_att], [rate, rate_att])\n",
    "                #table_rep= torch.ones_like(table_rep)\n",
    "                #return table_rep\n",
    "                sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    table_inputs=table_rep,\n",
    "                                                                    table_attention_mask=None,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=1,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "            else:\n",
    "                sample_outputs = model_generator.generator.generate(input_ids=preamble_tokens,\n",
    "                                                                    attention_mask=preamble_attention_mask,\n",
    "                                                                    num_beams=bs,\n",
    "                                                                    repetition_penalty=1.5,\n",
    "                                                                    length_penalty=8.6,\n",
    "                                                                    early_stopping=True,\n",
    "                                                                    use_cache=True,\n",
    "                                                                    max_length=190,\n",
    "                                                                    no_repeat_ngram_size=2,\n",
    "                                                                    num_return_sequences=1,\n",
    "                                                                    do_sample=False\n",
    "                                                                    # bos_token_id=random.randint(1,30000),\n",
    "                                                                    )\n",
    "            ss = tokenizer.decode(sample_outputs[0],\n",
    "                                  skip_special_tokens=True,\n",
    "                                  clean_up_tokenization_spaces=True)\n",
    "            \n",
    "            \n",
    "            #break\n",
    "            generated_outputs.append(ss)\n",
    "        \n",
    "        print(f'Generation based on beam size {bs} is complete')\n",
    "        #computeParentScore(refs=refs, predicted=generated_outputs, tables=eval_tables)\n",
    "        print('\\n')\n",
    "        generated_output_dict[bs] = generated_outputs\n",
    "    return generated_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "appreciated-importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n",
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 1 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 2 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 3 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 4 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 5 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 6 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 7 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 8 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 9 is complete\n",
      "\n",
      "\n",
      "Generation based on beam size 10 is complete\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputw = generateAndEvaluate(test_dataloader,sample_too=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "geological-service",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n",
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 1 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 2 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 3 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 4 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 5 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 6 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 7 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 8 is complete\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation based on beam size 9 is complete\n",
      "\n",
      "\n",
      "Generation based on beam size 10 is complete\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputp = generateAndEvaluate(test_dataloader,sample_too=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hollywood-subcommittee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The classification performance of the classifier on this binary classification problem as evaluated based on the precision, sensitivity, accuracy, and F1score achieved by the model is 91.3%, 87.29%, 90.67%, and 88.89%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. In summary, the confidence in predictions related to the label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification task where the test instances are classified as either #CA or #CB is as follows: Accuracy (85.33%), AUC (88.32%), Sensitivity (79.13%), Precision (81.54%), and finally, an F1score of 81.42%. These scores across the different metrics suggest that this model will be somewhat effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, we can see that the false positive rate is lower than expected.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the precision, accuracy, recall, and F2score achieved by the model is 34.81%, 47.92%, 52.94%, and 45.95%, respectively. According to these scores, we can conclude that this model will likely misclassify only a small number of test cases. In summary, the confidence in predictions related to #CB is very low.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (62.5%), precision (66.95%), recall (63.49%), and an F1score of 62.07%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, the confidence in predictions related to label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.11%), Sensitivity (84.29%), AUC (90.09%), Precision (89.07%), and finally, an F1score of 84.33%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, the confidence in predictions related to the positive class label is high.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the precision, sensitivity, specificity, and accuracy is 89.07%, 84.29%, 98.36%, and 86.11%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of error. In summary, we can confidently conclude that it will have a lower misclassification error rate.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (93.31%), AUC (94.36%), precision (86.96%), and sensitivity (87.29%). Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB can be summarized as follows: (a) Accuracy equal to 66.67%, (b) Precision score of 6645%, and (c) Recall score is 6698%. Judging by the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is summarized by the following scores: (a) Sensitivity = 82.61%. (b) Precision = 63.33%; (c) Specificity = 31.25%. These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of error. Furthermore, from the F1score and precision scores, we can see that the likelihood of misclassifying any given test example is marginal.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the metrics precision, sensitivity, accuracy, and F1score is 63.33%, 82.61%, 61.54%, and 71.7%, respectively. From these scores, we can draw the conclusion that this model will likely misclassify only a small number of test cases. In summary, the confidence in predictions related to label #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (95.77%), AUC (98.62%), Recall (96.31%), and Precision score of 95.41%. These scores across the different metrics suggest that this model will be highly effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, from the precision and recall scores, we can see that the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (90.73%), AUC (95.87%), Precision (89.13%), and a sensitivity score of 90.32%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (85.11%), AUC (90.23%), Precision (63.95%), and a sensitivity score of 90.07%. Judging by the scores across the different metrics, we can conclude that this model will be somewhat effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (91.25%), Precision (73.95%), F1score (86.0%), and finally, an F1score of about 73.96%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (93.11%), AUC (94.07%), Precision (33.95%), and finally, an F1score of 82.28%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with only a few instances misclassified.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.59%), Recall (56.91%), Precision (25.07%), and finally, an F1score of 25.1%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true label for several test cases with only a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (98.45%), AUC (99.04%), Sensitivity (90.2%), and finally, an F1score of 93.95%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be highly effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the recall, precision, and F2score achieved by the model is 64.74%, 63.97%, and 6446%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true label for several test cases with a small margin of misclassification error. Furthermore, from the precision and recall scores, we can see that the confidence in predictions related to #CB is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy equal to 63.97%, (b) Specificity score of 64.46%, and (c) Precision score is 62.38%. Judging by the scores across the different metrics under consideration, we can conclude that this model will likely misclassify only a small number of test cases. Furthermore, from the precision and recall scores, it is valid to say that the confidence in predictions related to label #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (86.21%), Precision (72.84%), F1score (79.65%), and an F1score of 79.64%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, it has a moderate to high confidence in its predictions.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (86.21%), Precision (72.84%), Recall (82.03%), and finally, an F1score of 76.64%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (80.81%), Sensitivity (82.93%), Precision (79.07%), and finally, an F1score of 82.13%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, it is valid to say that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (80.81%), Specificity (78.74%), Sensitivity (82.93%), and finally, an F1score of 80.95%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the metrics sensitivity, specificity, accuracy, AUC, and sensitivity is 32.88%, 34.56%, 48.61%, and 42.81%, respectively. Based on these scores, we can conclude that this model will likely misclassify only a small number of test cases belonging to the minority class label #CA as indicated by the precision score. This implies that the confidence in predictions related to #CB is very low. In summary, the model has a high false positive rate.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (90.11%), AUC (93.17%), Precision (87.15%), and Recall (84.57%). Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with only a few misclassification errors.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (55.67%), AUC (58.69%), Sensitivity (41.23%), and finally, an F1score of 31.38%. Based on the scores across the different metrics under consideration, we can draw the conclusion that this model will likely misclassify only a small number of test cases. Furthermore, from the recall and precision scores, it is valid to conclude that the model has a moderately high false-positive rate.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 72.59%; (b) AUC score of 75.08%, (c) Sensitivity, (d) Precision score, and (e) F1score (sensitivity). Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 74.08%, (b) Precision score of 74, and (c) Recall score is 75.51%. (d) F1score (e) Specificity score (i.e. F1score ) is 74% with a moderate recall score. These scores suggest that this model will likely misclassify only a small number of test cases. In summary, we can be certain that it will be effective at correctly predicting the true labels for several test examples.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (80.4%), Specificity (78.74%), Sensitivity (82.11%), and a Precision score equal to 78.91%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with only a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the metrics precision, sensitivity, specificity, and accuracy are 38.16%, 76.45%, 79.95%, and 63.48%, respectively. From these scores, we can conclude that this model will likely misclassify only a small number of test cases. Furthermore, the confidence in predictions related to the minority class label #CB is moderately high given the difference between precision and sensitivity scores.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (94.12%), Precision (86.42%), F1score (92.11%), and finally, an F1score of 92.10%. These scores across the different metrics suggest that this model will be highly effective at correctly predicting the true labels for several test cases with only a small margin of error. Furthermore, from the precision and F1score, we can see that the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (94.12%), Specificity (91.73%), Sensitivity (98.59%), and finally, an F1score of 92.11%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be very effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, the confidence in predictions related to any given test case is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 88.13%. (b) A precision score of 84.57% and (c) Recall score is 84,11%. From these scores, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test examples with only a few misclassification errors.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (81.23%), Precision (78.91%), Specificity (92.3%), and Recall (57.7%). Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the precision, accuracy, and recall scores is 75.21%, 80.96%, 66.97%, and 71.04%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with only a few instances misclassified. Furthermore, from the F1score and precision scores, we can see that the confidence in predictions related to #CB is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (71.11%), Specificity (70.02%), Precision (67.86%), and Sensitivity (72.38%). Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of error. Furthermore, from the precision and sensitivity scores, it is valid to say that the likelihood of misclassifying any given test case is marginal.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy = 71.11%. (b) Sensitivity = 72.38%; (c) Specificity = 70.02%. From these scores, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. In summary, it has a lower false-positive rate than expected.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (78.22%), Sensitivity (82.86%), Precision (73.73%), and finally, an AUC score of 78.51%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a margin of misclassification.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the metrics precision, sensitivity, specificity, and accuracy are 73.73%, 82.86%, 78.22%, and 74.17%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is quite high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (74.67%), Specificity (84.17%), Precision (77.91%), Sensitivity (63.81%), and an F1score of 70.16%. Based on the scores across the different metrics under consideration, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, it is valid to conclude that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (74.67%), AUC (73.99%), Specificity (84.17%), and finally, an F1score of 66.21%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (78.22%), Specificity (83.34%), Precision (79.17%), and Recall (72.38%). These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (72.44%), Precision (79.45%), Recall (55.24%), and finally, an F1score of 79.42%. These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test examples. Furthermore, from the precision and recall scores, we can see that the confidence in predictions related to #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (72.44%), AUC (71.34%), Specificity (87.51%), and F1score (65.17%). Judging by the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, it has a very low false-positive rate. The confidence in predictions related to the positive class label is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (73.33%), Specificity (72.5%), AUC score of 73.39%, and finally, an F1score of 72.22%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. In summary, it has a lower false-positive rate than expected.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (73.33%), Precision (70.28%), and an F1score of 73.45%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, from the precision and accuracy scores, it is valid to say that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (70.22%), Precision (66.38%), Recall (73.33%), and finally, an F1score of 70.42%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and recall scores, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (70.22%), Specificity (67.52%), and finally, an F1score of 71.83%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a few misclassification errors.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (55.11%), Precision (54.99%), and an F1score of 54.35%. Judging by the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. In summary, the confidence in predictions related to the minority class label #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (53.33%), Precision (54.23%), Recall (52.07%), and an F1score of 50.71%. Based on the scores across the different metrics under consideration, we can draw the conclusion that this model will likely misclassify only a small number of test cases. Furthermore, from the precision and recall scores, it is valid to conclude that the confidence in predictions related to the minority class label #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (79.72%), Precision (82.15%), Recall (75.0%), and finally, an F1score of 78.41%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. Furthermore, from the precision and recall scores, it is valid to say that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (79.72%), precision (82.15%), sensitivity (75.0%), specificity (84.28%), and an AUC score of 79.65%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be somewhat effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, it is obvious that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: accuracy (79.72%), specificity (84.28%), sensitivity (75.0%), and an AUC score of 79.65%. Judging based on the scores, we can conclude that this model will be moderately effective at correctly predicting the true label for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (75.04%), AUC (74.98%), Specificity (77.78%), and finally, a sensitivity score of 72.19%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the metrics accuracy, AUC, precision, specificity, and F1score is: 75.04%, 77.52%, 76.81%, and 78.78%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, we can see that the confidence in predictions related to the minority class label #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 77.51%, (b) Precision score of 76.73% and (c) Specificity score is 78.23%. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test examples with only a small margin of misclassification error. Furthermore, from the F1score and accuracy, we can see that the confidence in predictions related to #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 77.51%. (b) Precision score of 76.73% (c) Recall score is 78.81% and (d) F1score of77.59%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, based on the precision and recall scores, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (74.07%), Precision (77.45%), Specificity (81.31%), and Recall (66.57%). These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, from the precision and recall scores, we can estimate that the confidence in predictions related to #CA is moderately high.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the metrics precision, sensitivity, accuracy, AUC, and specificity is: (a) Accuracy equal to 84.28%; (b) Specificity score of 83.74% and (c) Sensitivity score is 8483%. These scores across the different metrics suggest that this model will be somewhat effective at correctly predicting the true labels for several test cases with only a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 84.28%. (b) AUC score of 84, (c) Precision score equal 83.43%; (d) Sensitivity (e) Specificity (i.e. F1score ). These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, we can see that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (74.07%), AUC (73.93%), Precision (77.45%), Specificity (81.31%), and Recall (66.57%). Based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. In summary, it has a moderate prediction performance.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (84.41%), AUC (80.48%), Precision (85.08%), Specificity (93.63%), and Recall (67.32%). Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (84.41%), AUC (80.48%), Specificity (93.63%), Recall (67.32%), and finally, an F1score of 75.16%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, it has a moderate to high false-positive rate.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (84.41%), Precision (85.08%), Specificity (93.63%), Recall (67.32%), and finally, an F1score of 70.25%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. Furthermore, from the precision and recall scores, it is valid to say that the likelihood of misclassifying any given test case is marginal.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.21%), Precision (84.07%), Sensitivity (74.81%), and an F1score of 76.49%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, it is valid to say that the model has a low false positive rate.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.21%), AUC (83.58%), Specificity (92.36%), Precision (84.07%), and Sensitivity (74.81%). Judging based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on precision, sensitivity, specificity, and accuracy is 84.07%, 74.81%, 79.17%, 92.36%, and 86.21%, respectively. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with only a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.21%), Precision (84.07%), Specificity (92.36%), and finally, an F1score of 79.17%. Judging based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the metrics precision, accuracy, specificity, and F1score is 43.58%, 86.21%, 92.36%, and 53.26%, respectively. These scores are lower than expected indicating how poor the model is in terms of correctly predicting the true labels for several test cases. Furthermore, from the precision score, we can see that the confidence in predictions related to #CA is low.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (86.21%), Specificity (92.36%), Precision (43.58%), and finally, an F1score of 62.26%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. In summary, the confidence in predictions related to the minority class label #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (83.72%), Precision (86.17%), Specificity (94.48%), and an F1score of 73.3%. Judging based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for the majority of test cases. Specifically, it has a very low false-positive rate. This implies that the likelihood of misclassifying any given test case is only marginal.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (83.72%), Precision (86.17%), Specificity (94.48%), and an F1score of 67.28%. Judging by the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the precision, accuracy, specificity, and F2score achieved by the model is 86.17%, 83.72%, 67.28%, and 94.48%, respectively. From these scores, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. In summary, the confidence in predictions related to #CB is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (83.72%), AUC (79.13%), Precision (86.17%), Specificity (94.48%), and finally, an F1score of 73.3%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, from the precision and recall scores, the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the precision, sensitivity, accuracy, and F2score achieved by the model is 84.75%, 59.06%, 81.93%, and 62.87%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the F1score and precision scores, we can see that the false positive rate is lower than expected.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (79.25%), AUC (74.61%), Sensitivity (59.84%), and Precision (75.20%). From these scores, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, it has a very low false-positive rate as indicated by the precision and sensitivity scores achieved.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (81.93%), AUC (74.81%), Sensitivity (59.06%), Precision (84.75%), and finally, an F1score of 69.61%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, it is valid to say that the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (79.25%), AUC (77.61%), Specificity (89.38%), Sensitivity (59.84%), and finally, a Precision score of 75%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on the metrics precision, sensitivity, accuracy, and F1score is 88.99%, 81.03%, 85.24%, and 84.82%, respectively. These scores support the conclusion that this model will be moderately effective at correctly predicting the true labels for a large proportion of test cases. Furthermore, from the precision and sensitivity scores, we can see that the confidence in predictions related to #CB is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (57.44%), AUC (59.48%), Specificity (48.56%), and Sensitivity (49.6%). Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. However, from the specificity and sensitivity scores, it is obvious that the confidence in predictions related to #CA is low.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (81.66%), Sensitivity (78.05%), Precision (84.71%), Specificity (85.39%), and finally, an F1score of 81.24%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a few misclassification errors.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (83.17%), Precision (85.4%), Recall (80.76%), and an F1score of 81.64%. Judging based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true label for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (83.17%), AUC (87.65%), precision (85.4%), and recall (80.76%). Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. The confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: accuracy (85.24%), precision (88.99%), recall (81.03%), and an AUC score of 85.32%. Judging based on the scores achieved, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with only a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (87.17%), AUC (89.07%), Recall (83.74%), Precision (90.35%), and finally, an F1score of 84.98%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and recall scores, it is obvious that the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB can be summarized as follows: Accuracy (79.25%), AUC (77.61%), sensitivity (59.84%), and F1score (66.67%). Based on the scores across the different metrics under consideration, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and sensitivity scores, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification task as evaluated based on the metrics accuracy, AUC, sensitivity, and precision are 82.21%, 86.31%, 75.88%, and 87.51%, respectively. From these scores, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error.',\n",
       " 'The classification performance of the classifier on this binary classification problem as evaluated based on accuracy, precision, specificity, and recall is 87.17%, 90.35%, 89.73%, and 83.74%, respectively. Based on the scores across the different metrics under consideration, we can conclude that this model will be highly effective at correctly predicting the true labels for several test cases with a small margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is: Accuracy (82.21%), Specificity (88.76%), Sensitivity (75.88%), Precision (87.51%), and an F1score of 81.28%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (81.66%), Specificity (85.39%), AUC (86.47%), and a sensitivity score of 78.05%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. The confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where a given test case is labeled as either #CA or #CB is: Accuracy (81.66%), AUC (86.47%), Specificity (85.39%), Sensitivity (78.05%), and finally, an F1score of 81.24%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true label for several test cases with a small margin of misclassification error. Furthermore, the confidence in predictions related to the minority class label #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 81.33%. (b) A precision score of 82.77% and (c) Recall score is 82,01%. These scores across the different metrics suggest that this model will be moderately effective at correctly predicting the true labels for several test cases with only a few instances mislabeled.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: accuracy (81.33%), precision (82.77%), F2score (80.83%), and an F1score of 80.84%. Based on the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with only a small margin of misclassification error. Furthermore, the confidence in predictions related to #CA is very high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (73.78%), Precision (77.74%), and an F1score of 73.35%. Based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of error. Furthermore, from the accuracy and precision scores, it is valid to say that the likelihood of misclassifying any given test case is very low.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (73.78%), Recall (74.64%), F1score (72.87%), and finally, an F1score of 72.88%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true label for several test cases with a small margin of misclassification error. Furthermore, from the recall and precision scores, the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (72.44%), Recall (73.51%), F1score (71.94%), and finally, an F1score of 71.92%. From the scores across the different metrics under consideration, we can draw the conclusion that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of error. Furthermore, from the precision and recall scores, it is valid to conclude that the likelihood of misclassifying any given test example is low.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (72.44%), Precision (77.01%), Recall (73.51%), and finally, an F1score of 72.31%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification error. Furthermore, from the precision and recall scores, it is obvious that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 73.78%. (b) A precision score of 79.09% and (c) Recall score (d) 74.77%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a margin of error.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: Accuracy (72.01%), precision (73.06%), F1score (71.54%), and a recall score of 72.56%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases with a small margin of misclassification error. Furthermore, from the precision and recall scores, it is valid to say that the confidence in predictions related to #CA is high.',\n",
       " 'The classification performance of the classifier on this binary classification problem where the test instances are classified as either #CA or #CB is as follows: (a) Accuracy equal to 76.44%, (b) Precision score of 7681%, and (c) Recall score is 7683%. Judging by the scores across the different metrics under consideration, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test examples with a small margin of misclassification.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputp[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abandoned-blowing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/essel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from datasets  import load_metric\n",
    "\n",
    "metric_bert = load_metric('bertscore')\n",
    "metric_sbleu = load_metric('sacrebleu')\n",
    "metric_meteor = load_metric('meteor')\n",
    "#metric_bleurt = load_metric('bleurt', 'bleurt-large-512')\n",
    "metric_rouge = load_metric('rouge')\n",
    "metric_wer = load_metric('google_bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "thrown-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = [[t['narration'].lower() for t in test_sample]]\n",
    "orf = [[normalize_text(r) for r in refs[0]]]\n",
    "\n",
    "def computeBleu(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_sbleu.compute(predictions=[gen],references=[orf])\n",
    "        results[k] = score['score']\n",
    "    return results\n",
    "def computeMeteor(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_meteor.compute(predictions=[gen],references=[orf])\n",
    "        results[k] = score['meteor']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "moral-madonna",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 18.004554197856525,\n",
       "  2: 40.19507047985887,\n",
       "  3: 36.17077644323477,\n",
       "  4: 33.076046900686144,\n",
       "  5: 32.77992614434575,\n",
       "  6: 30.813337427771582,\n",
       "  7: 28.30160910041279,\n",
       "  8: 29.1079735758328,\n",
       "  9: 27.036945910838956,\n",
       "  10: 25.243362242544},\n",
       " {1: 14.842273665863551,\n",
       "  2: 30.135203525900103,\n",
       "  3: 29.791274986242087,\n",
       "  4: 29.58977970199185,\n",
       "  5: 26.439653503376174,\n",
       "  6: 25.410222070458868,\n",
       "  7: 24.477795786739154,\n",
       "  8: 24.493055642451885,\n",
       "  9: 23.21961087295014,\n",
       "  10: 22.157799841050757})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#With EF\n",
    "computeBleu(outputp),computeBleu(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "brave-spanish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.40441167175981946,\n",
       "  2: 0.4228484232100022,\n",
       "  3: 0.41094354694792584,\n",
       "  4: 0.4201395857821566,\n",
       "  5: 0.41016712463093763,\n",
       "  6: 0.409592965285363,\n",
       "  7: 0.40275118215116423,\n",
       "  8: 0.40829981478907657,\n",
       "  9: 0.40588613760886755,\n",
       "  10: 0.3947969258326691},\n",
       " {1: 0.4147459130331091,\n",
       "  2: 0.3739639791124101,\n",
       "  3: 0.3742616672998477,\n",
       "  4: 0.368348054419106,\n",
       "  5: 0.40120568358812636,\n",
       "  6: 0.39907463276369437,\n",
       "  7: 0.38225175641364634,\n",
       "  8: 0.37368728119763484,\n",
       "  9: 0.3734963571393408,\n",
       "  10: 0.3651930736379808})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(outputp),computeMeteor(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-department",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "greek-visibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 18.18647642562218,\n",
       "  2: 41.4318438834784,\n",
       "  3: 39.527575067121276,\n",
       "  4: 36.73161526980751,\n",
       "  5: 36.20430876482769,\n",
       "  6: 35.93938321063815,\n",
       "  7: 35.03524917063757,\n",
       "  8: 33.75666086597703,\n",
       "  9: 33.489605179610116,\n",
       "  10: 32.596844552620844},\n",
       " {1: 15.735459594300107,\n",
       "  2: 31.1110718667693,\n",
       "  3: 29.74483702610494,\n",
       "  4: 29.00375853757735,\n",
       "  5: 31.94280475059761,\n",
       "  6: 33.80306180420938,\n",
       "  7: 34.074777126559106,\n",
       "  8: 34.531845972866115,\n",
       "  9: 34.79236502073277,\n",
       "  10: 33.376039229060666})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without EF\n",
    "computeBleu(outputp),computeBleu(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "premier-chase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.4112025902052975,\n",
       "  2: 0.42812281580620754,\n",
       "  3: 0.42335622050165467,\n",
       "  4: 0.4221545468546283,\n",
       "  5: 0.42806984607026943,\n",
       "  6: 0.42532949168235684,\n",
       "  7: 0.42635100819758326,\n",
       "  8: 0.43149801175695013,\n",
       "  9: 0.4279057502472302,\n",
       "  10: 0.4294665929845267},\n",
       " {1: 0.42474151580077524,\n",
       "  2: 0.3944939254732413,\n",
       "  3: 0.3878642492180391,\n",
       "  4: 0.38419168626798,\n",
       "  5: 0.40673118478576925,\n",
       "  6: 0.41811280935202105,\n",
       "  7: 0.4252503409013111,\n",
       "  8: 0.41344199267966825,\n",
       "  9: 0.40516467255892824,\n",
       "  10: 0.3911980630935378})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(outputp),computeMeteor(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-timothy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-creek",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "duplicate-efficiency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1: 0.42158186788747337,\n",
       "  2: 0.4635141208937901,\n",
       "  3: 0.4635352932342456,\n",
       "  4: 0.4522811045349713,\n",
       "  5: 0.4551557890448855,\n",
       "  6: 0.4569881409483277,\n",
       "  7: 0.453155382211198,\n",
       "  8: 0.45075566073909473,\n",
       "  9: 0.4565556839764019,\n",
       "  10: 0.4572991759022991},\n",
       " {1: 0.43349293247657605,\n",
       "  2: 0.45872437966846336,\n",
       "  3: 0.45808734065434664,\n",
       "  4: 0.4518029813049793,\n",
       "  5: 0.4486871517365904,\n",
       "  6: 0.44622640136411,\n",
       "  7: 0.4469149605602011,\n",
       "  8: 0.4509605316017421,\n",
       "  9: 0.45041744465090283,\n",
       "  10: 0.4552071160221205})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(outputp),computeMeteor(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "outstanding-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "promising-pipeline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "south-harvey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4071538206134078"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_n_corpus_level(outputw[10],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-salmon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-slope",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-memphis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-passenger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "surrounded-adolescent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRouge(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_rouge.compute(predictions=[gen],references=orf,use_stemmer=True)\n",
    "        score= {key: value.high.fmeasure * 100 for key, value in score.items()}\n",
    "        results[k] = score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "radio-mission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'rouge1': 62.33269598470363,\n",
       "  'rouge2': 38.92915906968471,\n",
       "  'rougeL': 19.956423140201878,\n",
       "  'rougeLsum': 19.956423140201878},\n",
       " 2: {'rouge1': 79.44643171289508,\n",
       "  'rouge2': 53.02914785056595,\n",
       "  'rougeL': 30.07095525713951,\n",
       "  'rougeLsum': 30.07095525713951},\n",
       " 3: {'rouge1': 79.7073456248171,\n",
       "  'rouge2': 51.73564362231458,\n",
       "  'rougeL': 30.99795141937372,\n",
       "  'rougeLsum': 30.99795141937372},\n",
       " 4: {'rouge1': 76.7219941009774,\n",
       "  'rouge2': 49.75417895771878,\n",
       "  'rougeL': 31.357353536521888,\n",
       "  'rougeLsum': 31.357353536521888},\n",
       " 5: {'rouge1': 78.88184167253934,\n",
       "  'rouge2': 50.88687889110772,\n",
       "  'rougeL': 31.066478740897345,\n",
       "  'rougeLsum': 31.066478740897345},\n",
       " 6: {'rouge1': 78.23347529315676,\n",
       "  'rouge2': 50.70307485850984,\n",
       "  'rougeL': 31.001691849950415,\n",
       "  'rougeLsum': 31.001691849950415},\n",
       " 7: {'rouge1': 77.85598892222478,\n",
       "  'rouge2': 50.294287362954414,\n",
       "  'rougeL': 30.55619663051004,\n",
       "  'rougeLsum': 30.55619663051004},\n",
       " 8: {'rouge1': 78.47360412033242,\n",
       "  'rouge2': 50.12877546242098,\n",
       "  'rougeL': 30.094814467985486,\n",
       "  'rougeLsum': 30.094814467985486},\n",
       " 9: {'rouge1': 78.15589790756495,\n",
       "  'rouge2': 50.56916177992411,\n",
       "  'rougeL': 30.754196366980914,\n",
       "  'rougeLsum': 30.754196366980914},\n",
       " 10: {'rouge1': 78.50739317731983,\n",
       "  'rouge2': 50.43083372147182,\n",
       "  'rougeL': 31.062987542205146,\n",
       "  'rougeLsum': 31.062987542205146}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(outputp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "exact-curtis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {'rouge1': 63.60711412994906,\n",
       "  'rouge2': 38.38934572756525,\n",
       "  'rougeL': 21.619447671820538,\n",
       "  'rougeLsum': 21.619447671820538},\n",
       " 2: {'rouge1': 79.75496967149121,\n",
       "  'rouge2': 51.72683044026669,\n",
       "  'rougeL': 30.34052008888355,\n",
       "  'rougeLsum': 30.34052008888355},\n",
       " 3: {'rouge1': 81.68563518917256,\n",
       "  'rouge2': 53.52857933919892,\n",
       "  'rougeL': 31.14118732697631,\n",
       "  'rougeLsum': 31.14118732697631},\n",
       " 4: {'rouge1': 80.93998015873017,\n",
       "  'rouge2': 51.469676299144226,\n",
       "  'rougeL': 31.0639880952381,\n",
       "  'rougeLsum': 31.0639880952381},\n",
       " 5: {'rouge1': 80.33488372093024,\n",
       "  'rouge2': 50.921044470632026,\n",
       "  'rougeL': 30.238759689922485,\n",
       "  'rougeLsum': 30.238759689922485},\n",
       " 6: {'rouge1': 80.10467941927845,\n",
       "  'rouge2': 49.99065245840343,\n",
       "  'rougeL': 30.431802604523657,\n",
       "  'rougeLsum': 30.431802604523657},\n",
       " 7: {'rouge1': 80.32264115550554,\n",
       "  'rouge2': 50.89112625852042,\n",
       "  'rougeL': 30.51334959044582,\n",
       "  'rougeLsum': 30.51334959044582},\n",
       " 8: {'rouge1': 81.33767535070142,\n",
       "  'rouge2': 51.759989978704745,\n",
       "  'rougeL': 30.548597194388773,\n",
       "  'rougeLsum': 30.548597194388773},\n",
       " 9: {'rouge1': 79.89604603675515,\n",
       "  'rouge2': 50.05260226499164,\n",
       "  'rougeL': 30.852051234453313,\n",
       "  'rougeLsum': 30.852051234453313},\n",
       " 10: {'rouge1': 80.72020789506249,\n",
       "  'rouge2': 51.54702970297029,\n",
       "  'rougeL': 30.83776760301943,\n",
       "  'rougeLsum': 30.83776760301943}}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(outputw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "developmental-implement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(outputw,open(f'../TrainedNarrators/newBartOutputlarge_bs.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "closing-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(outputp,open(f'../TrainedNarrators/newBartOutputlarge_sample_bs.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "entertaining-karen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The classifier trained to solve the given AI task achieved the following performance evaluation scores: (a) Accuracy equal to 90.67%. (b) Sensitivity score equal 87.29%; (c) Precision score equals 91.3%;(d) F1score of 88.89%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA and #CB. Furthermore, from the F1score and precision scores, we can conclude that it will likely have a lower false-positive rate.',\n",
       " 'The performance evaluation scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 85.33%, (2) AUC score equal 88.32%, and (3) Sensitivity score of 79.13%. (4) F1score of 81.54%. Besides, it has a Precision score (sometimes referred to as the recall score) equal 87.39%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the labels #CA and #CB. Furthermore, from the F1score and precision scores, we can conclude that it will likely have a lower false positive rate.',\n",
       " 'The scores achieved by the classifier on this classification task are as follows: Accuracy (47.92%), Recall (52.94%), Precision (34.81%), and finally, an F2score of 45.95%. Judging based on the scores across the different metrics under consideration, we can conclude that this model has a lower classification performance as it is not be able to correctly predict the actual labels of multiple test examples. Furthermore, the accuracy score is only marginally higher than the dummy model constantly assigning the majority class label #CA to any given test case.',\n",
       " \"The classifier's performance was assessed based on the following evaluation metrics: accuracy, recall, precision, and F1score as shown in the table. On this multi-class classification problem, the model has an accuracy of 62.5%, a recall score of 63.49%; a precision score equal to 66.95%, and an F1score of 62%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels ( #CA, #CB and #CC ) under consideration. Furthermore, from the F1score and recall scores, we can estimate that it will likely have a lower false positive rate.\",\n",
       " 'The performance evaluation scores achieved by the model on this binary classification task are as follows: (a) Accuracy equal to 86.11%. (b) AUC score of 90.09%.(c) Sensitivity (sometimes referred to as the recall score) is 84.29%; (d) Precision score equal 89.07%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the F2score and accuracy show that the likelihood of misclassifying #CA cases as #CB is marginal; however, given the picky nature of the algorithm, some cases labeled as #CA might end up being part of #CB. Overall, these scores indicate that it has a moderate to high classification performance and can correctly identify the true label for most test cases.',\n",
       " \"The classifier's performance scores on this binary classification task are as follows: (a) Accuracy equal to 86.11%. (b) Specificity score equal 98.36%; (c) Precision score equals 89.07%;(d) F1score of 85.19%. Besides, sensitivity (sometimes referred to as the recall score) is 84.29%. Judging by the scores, this model is shown to be effective and it can correctly identify a fair amount of test instances/samples with a small margin of misclassification error. In essence, it does the job quite well.\",\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 93.31%, a sensitivity (recall) score of 87.29%, with precision and AUC scores equal to 86.96% and 94.36%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the different labels, #CA and #CB. Furthermore, from the recall (sensitivity) and precision scores, we can say it will likely have a lower false-positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 66.67%, a recall and precision scores, respectively, equal to 6698% and 66%. Besides, it has an F1score of 6631%. Judging by the scores obtained, we can conclude that this model has a moderate classification performance and will be able to correctly classify most test samples drawn randomly from any of the class labels under consideration.',\n",
       " 'The scores achieved by the classifier on this binary classification task are as follows: (1) Sensitivity equal to 82.61%. (2) Precision score of 63.33%. and (3) Specificity score equal 31.25%. Besides, the F1score is 71.7%. The scores mentioned above tell a story of a model with a relatively moderate classification performance, meaning it will likely misclassify some test samples drawn randomly from any of the classes. Overall, we can conclude that this model will not be that effective at correctly predicting the true labels of multiple test examples.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, accuracy, F1score, and sensitivity scored 63.33%, 61.54%, 71.7%, and 82.61%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can say that it will likely have a lower false-positive rate.\",\n",
       " \"The classification performance of this model is captured by the evaluation metrics: AUC, recall, accuracy, and precision. The scores achieved across these metrics are 98.62%, 95.31%, 99.77%, and 9541%, respectively. These scores are very higher than expected indicating how good the model's performance is in terms of correctly predicting the true label for the majority of test cases related to any of the class labels. In summary, we can confidently conclude that this classifier will be highly effective at assigning the correct label to the examples drawn from the different classes ( #CA and #CB ) under consideration.\",\n",
       " 'The performance of the model on this binary classification task as evaluated based on the precision, AUC, accuracy, and sensitivity scores are 89.13%, 95.87%, 90.73%, and 9032%, respectively. These scores support the conclusion that this model will be highly effective at correctly recognizing the true label for several test instances drawn randomly from the class labels #CA and #CB. Furthermore, the likelihood of misclassification is very low (actually it is equal to <acc_diff> %).',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the precision, accuracy, AUC, sensitivity, and specificity scored 63.95%, 85.11%, 90.07%,90.23%, and 90%. According to these scores, we can conclude that this model has a moderate classification performance and will be somewhat effective at correctly recognizing the true label for the majority of test cases drawn randomly from the class labels #CA and #CB. However, it is not a perfect model hence it will misclassify some test instances.',\n",
       " 'The machine learning algorithm trained on this classification task achieved an accuracy of 91.25%, a precision score of 73.95%, and an F2score of 86.0%. According to these scores, the algorithm is shown to be effective and it can correctly identify the true label for a large proportion of test cases drawn randomly from any of the class labels #CA and #CB. Besides, it has a moderate to high confidence in the #CA predictions.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, AUC, accuracy, and F1score scored 33.95%, 94.07%, 93.11%, and 82.28%, respectively. These scores are lower than expected indicating how poor the model is at correctly generating the true class label for most test cases related to any of the class labels. The above conclusion is further supported by the moderately lower F1score (which indicates the likelihood of misclassification is high).\",\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the Precision, Accuracy, Recall and F1score scored 25.07%, 86.59%, 56.91% and 251%, respectively. These scores were achieved on an imbalanced dataset. From the precision and recall scores, we can see that the model has a moderate F1score. However, looking at the accuracy score, it is obvious that this model avoids making many false-negative predictions; hence some of the #CB predictions might be wrong. Overall, these scores show that it will likely fail to correctly identify the true label for a number of test cases from both class labels.\",\n",
       " 'The performance of the model on this binary classification task as evaluated based on the F1score, accuracy, AUC, and sensitivity scored 93.95%, 98.45%, 99.04%, 90.2%, and 98%. These scores are very higher than expected indicating how good the performance is in terms of correctly picking out the test cases belonging to the class label #CA from the population under consideration. Furthermore, the precision and recall scores show that the likelihood of misclassifying #CB test samples is very low. Overall, these scores support the conclusion that this model will be highly effective at correctly recognizing the true label for several test instances drawn from the different classes with only a small margin of error.',\n",
       " 'On this classification task, the model has an accuracy of 63.97%, a recall score of 64.74%, and an F2score of 64%. Based on the scores across the different metrics under consideration, we can conclude that this model performs moderately well in terms of correctly predicting the true label for most of the test cases. Besides, it has a moderate to high confidence in the predicted output class labels.',\n",
       " \"The classifier trained to solve the given classification task achieved an accuracy of 63.97%, a specificity score of 64.46%, with a recall and precision scores equal to 6474% and 6338%, respectively. Judging based on these metrics' scores, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective enough to sort between the examples belonging to any of the different labels, #CA and #CB.\",\n",
       " 'The machine learning algorithm trained on this multi-class problem (where a given test case is labeled as either #CA or #CB or #CC or #CD ) was evaluated based on its scores across the following evaluation metrics: accuracy, precision, F2score, and sensitivity score. On this classification task, the algorithm has a prediction accuracy of 86.21% with moderate precision and F2score equal to 72.84% and 79.65%, respectively. Judging by these scores attained, it is fair to conclude that this model can accurately classify several test cases with only a few misclassifications.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: accuracy, recall, precision, and F1score. On this multi-class classification problem, the model has an accuracy of 86.21%, a recall score of 82.03%; a precision score equal to 72.84% with an F1score of 76.64%. Judging by the scores, it is fair to conclude that this model can accurately classify several test cases with only a few misclassifications.\",\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 80.81%, (2) Sensitivity score equal 82.93%, and (3) Precision score of 79.07%. (4) F2score of 82, and(5) Moderate precision score (i.e. 82). The underlying dataset has a disproportionate amount of data belonging to the different classes; therefore, judging the performance of the classification model based on only the accuracy score is not very intuitive. Therefore, making judgments about the overall performance based solely on the F2score (which incorporates both the recall and precision scores) is a valid statement. Overall, the scores are high and as such can be considered as good enough to indicate that this model will be able to accurately classify several test cases with only a few misclassifications.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 80.81%, a specificity score of 78.74%, sensitivity score equal to 82.93%, and an F1score of 80%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA and #CB. Furthermore, from the F1score and specificity scores, we can say that it will likely misclassify a small number of samples drawn randomly from any one of these classes.',\n",
       " 'The classifier was trained on this imbalanced dataset to correctly separate the examples into two different classes (i.e. #CA and #CB ). The performance assessment conducted showed that it has a very low classification performance as indicated by the scores achieved across the metrics: accuracy (42.81%), AUC (48.61%), specificity (34.56%), and sensitivity (32.88%). Overall, the model will likely fail to identify the correct labels for several test instances, especially those drawn from the class label #CB.',\n",
       " 'On this classification task, the model has an accuracy of 90.11%, recall of 84.57%, AUC of 93.17%, and a high precision score of 87.15% as its performance assessment scores on the ML task/problem. Overall, these results support the conclusion that this model will be highly effective at correctly labeling most of the test samples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The scores achieved by the model on this classification task are 55.67% (accuracy), AUC 58.69%, Sensitivity 41.23%, and F1score 31.38%. The very low F1score indicates that there is a high false positive rate implying most of the #CA examples are misclassified as #CB. This is not true given that the majority of examples under the class label #CA are correctly identified as #CA. Overall, this model has very poor classification performance as it will likely fail to correctly identify several test instances/samples from both class labels especially those difficult to pick out.',\n",
       " 'The classification performance of this model can be summarized as moderately high given that it achieved an accuracy of 72.59%, an AUC score of 75.08%, a precision score (i.e. equal to 72%), a sensitivity (sometimes referred to as the recall score) is equal or higher than expected, and finally, it has an F2score of about 72%. These scores across the different metrics suggest that this classifier will be able to accurately identify the true label for several test cases with only a few misclassifications.',\n",
       " 'The classification performance of this model can be summarized as fairly high given the scores achieved across the evaluation metrics: accuracy, recall, precision, F2score, and precision. The dataset used for modeling was balanced, supporting no sampling biases by the model. Consequently, the values of 74.08% for the accuracy metric are less impressive, indicating how good the performance is. However, based on the remaining metrics (that is recall = 74., precision =74.02%, F2score = 74%.), the false positive rate is estimated to be very low. This implies the likelihood of examples belonging to label #CB being misclassified as #CA is low, which is a good sign any model which performs well on this classification task is able to accurately identify the true label for several test instances/samples.',\n",
       " 'The classifier trained to solve the given classification task achieved a sensitivity score of 82.11%, a precision score equal to 78.91%, an F1score of 80.47%, and a specificity score (i.e. the ability to correctly detect the negative class #CA observations) of 7874%. In addition, it has an accuracy of 80% and an Specificity score that is 78%. Judging based on the scores, the model demonstrates a moderately high classification performance and will be able to accurately label several test cases drawn from the different classes under consideration ( #CA and #CB ).',\n",
       " 'The classifier was trained on this imbalanced dataset to correctly separate the examples into two different classes (i.e. #CA and #CB ). The performance assessment conducted showed that it has a moderate classification performance judging by the F1score, specificity score, sensitivity score and accuracy score. Specifically, the classifiers are characterized by: (1) Specificity equal to 79.95%, (2) Sensitivity score of 76.45%,(3) Moderate precision score (38.16%), (4) Accuracy score is 76%. (5) F1score of 63.48%. These scores across the different metrics show that this model will likely fail to identify the correct labels for a number of test cases belonging to both class labels.',\n",
       " 'On this classification task, the model has an accuracy of 94.12%, a precision score equal to 86.42%, an F1score of 92.11%, and an F2score of 94%. These scores support the conclusion that this model will be highly effective at correctly labeling most of the test samples drawn from the different labels ( #CA and #CB ) under consideration. In fact, it has a lower misclassification error rate as indicated by the F1score.',\n",
       " \"The classifier's performance scores are as follows: (a) Accuracy is 94.12%. (b) Specificity is 91.73%; (c) Sensitivity is 98.59%;(d) F1score is 92.11%. These results/scores are very impressive given the fact that the dataset was imbalanced. The precision and sensitivity scores alone would indicate that this model is very effective at correctly predicting the true label for test cases related to any of the class labels #CA and #CB. However, the F1score and specificity show that it has a moderately high false positive rate as indicated by the accuracy score achieved. Overall, these scores support the conclusion that there is a high confidence level in the model's output prediction decisions for the majority of test samples drawn from the different labels under consideration.\",\n",
       " 'The classification performance of the model on this binary classification task as evaluated based on the precision, AUC, accuracy, and recall achieved the scores 84.57%, 96.13%, 88.11%, and 84%. These scores support the conclusion that this model will be highly effective at correctly recognizing the true label for the majority of test cases drawn from the different labels ( #CA and #CB ) under consideration. Furthermore, the confidence in predictions is very high given the data was balanced between the class labels.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, accuracy, specificity, and recall scored 78.91%, 81.23%, 92.3%, and 57.7%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) and precision scores, we can say it will likely have a lower false positive rate.\",\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the Recall, Precision, F1score, and Accuracy scored 66.97%, 75.21%, 71.04%, and 80.96%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the precision and recall scores show that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in prediction decisions related to that label is also high.\",\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 71.11%, a specificity score of 70.02%; a sensitivity score (i.e. recall) equal to 72.38%, and a moderate precision score is 67.86%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the two different classes, #CA and #CB. Furthermore, from the precision and recall scores, we can assert that it will likely have a lower false positive rate.',\n",
       " 'The classification performance of the model can be summarized as moderately high given that it achieved an F2score of 71.42%, a sensitivity (sometimes referred to as the recall) score of 72.38%, an AUC score, a specificity score equal to 70.02%, and an accuracy of 71%. These scores across the different metrics suggest that this model is likely to misclassify only a small number of test cases or instances. Furthermore, from the F2score and Sensitivity scores, we can conclude that the likelihood of examples belonging to label #CA being misclassified as #CB is marginal; however, given the picky nature of some examples, it is difficult to say whether it happens frequently or not.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 78.22%, (2) Sensitivity score equal 82.86%; (3) Precision score of 73.73%, and (4) F2score of 80.80%. The underlying dataset has a disproportionate amount of data belonging to the different classes; therefore, judging the performance based on only the accuracy score is not very intuitive. Therefore, making judgments about the overall classification performance of a model based solely on the F2score (a balance between the recall and precision scores) is a valid statement. This is because the difference between precision and sensitivity shows a high level of understanding the underlying ML task and can be used to assess how good the classifier is. In summary, the F1score and accuracy indicate that the likelihood of misclassifying #CA cases is low, which is impressive but not surprising given the data was balanced.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 78.22%, a specificity score of 74.17%, sensitivity score equal to 82.86%, precision score (sometimes referred to as the recall score) is 73.73%, and finally, an F1score of 78%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 74.67%, a precision score of 77.91%, specificity score equal to 84.17%, sensitivity score (sometimes referred to as the recall score) is 63.81%, and an F1score of 70.16%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the two different classes. Furthermore, from the F1score and precision scores, we can say that it will likely misclassify some of the #CB examples but will have high confidence in its prediction decisions.',\n",
       " 'The performance evaluation scores achieved by the classifier on this binary classification task are as follows: (1) Accuracy equal to 74.67%, (2) AUC score of 73.99%; (3) Specificity score (i.e. 84.17%), (4) F2score of 66.21%, and (5) a moderate F2score equal to 66%. Judging based on the scores, this model is shown to be somewhat effective at correctly classifying most test cases with only a small margin of error. Besides, the F2score shows that the confidence in predictions related to the label #CB is high.',\n",
       " 'The machine learning algorithm trained on this classification task achieved a specificity score of 83.34%, a precision score equal to 79.17%, an accuracy of 78.22%, and a recall of 72.38%. Judging by these scores attained, it is fair to conclude that this model can accurately distinguish several test cases belonging to the class labels #CA and #CB with only a few misclassifications. Overall, the algorithm employed here is relatively confident with its prediction decisions across the majority of test examples drawn from the different labels under consideration.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, accuracy, and recall are 79.45%, 72.44%, and 55.24%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) and precision scores, we can say that it will likely have a lower false positive rate.\",\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the F1score, AUC, specificity, and accuracy are 65.17%, 71.34%, 87.51%, and 72.44%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the likelihood of misclassifying any given test case is marginal.\",\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the F1score, AUC, accuracy, and specificity scored 72.22%, 73.39%, 71.33%, and 72., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the precision and recall scores show that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in prediction decisions related to them is very high.\",\n",
       " 'On this classification task, the model has an accuracy of 73.33%, a precision score of 70.28%, and an F2score of about 72.45%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F2score and precision scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 70.22%, a recall of 73.33%, and a precision score of 66.38%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels, #CA and #CB. Furthermore, from the recall and precision scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'On this classification task, the model was trained to label test samples as class #CA or class #CB. Evaluations conducted based on the metrics F2score, Specificity, Accuracy, and Accuracy show that it has fairly high classification performance and will be able to correctly identify the true label for most of the test cases. The above statement can be attributed to the fact that the classifier achieved an accuracy of 70.22%, a specificity score of 67.52%, with the F2score equal to 71.83%. Judging by the scores, it is fair to conclude that this model can correctly classify several test instances with only a few misclassifications.',\n",
       " 'The classifier was trained to assign test cases to one of the following classes #CA, #CB, #CC, and #CD. The scores achieved across the evaluation metrics are 55.11% (accuracy), 54.99% precision score (54.35%), and an F1score of 54%. Judging based on the scores above, we can conclude that this model has a lower classification performance as it is not be able to correctly predict the actual labels of multiple test examples.',\n",
       " 'The classifier was trained on this multi-class classification task to assign test cases to either #CA or #CB or #CC or #CD. The classification performance can be summarized as moderately low given the scores achieved across the evaluation metrics: Accuracy (53.33%), Recall (52.07%), Precision (54.23%), and F1score (50.71%). Given the fact that the number of observations for each class is not balanced, these scores are not very impressive, suggesting a new set of features or more training data should be used to re-train the model. In summary, this model will likely fail to correctly identify the correct label for a small proportion of test examples drawn randomly from any of the classes.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 79.72%, a recall score of 75.0%, with the precision and F1score equal to 82.15% and 78.41%, respectively. Judging based on the scores achieved, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 79.72%, a sensitivity score of 75.0%, an AUC score equal to 79,82.15%, with precision, specificity, and specificity scores equal 82.3%, 84.28%, and 79., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely misclassify some of the #CB examples as #CA.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 79.72%, (2) Specificity score equal 84.28%, and (3) Sensitivity score (i.e. Recall) is 75.0%. (4) F2score of 76.33%, which is a balance between the sensitivity and precision scores indicates that it is fairly good at detecting examples belonging to class label #CA. (5) AUC score of 7965%, further indicating that the number of #CB predictions made is moderately high. Overall, these scores support the conclusion that this model will be moderately effective enough to sort between examples drawn randomly from any of the classes or labels.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 75.04%, a sensitivity (recall) score of 72.19%, with a specificity score equal to 77.78%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels, #CA and #CB. Furthermore, from the recall (sensitivity) and precision scores, we can say that it will likely have a lower false-positive rate.',\n",
       " 'The classification performance scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 75.04%. (2) AUC score of 77.52%; (3) Specificity score (i.e. the ability to correctly tell apart the #CA and #CB test observations). (4) Precision score equal 76.81% (5) F2score equal to 7759%, and (6) F1score of 7777.59%. These scores across the different metrics suggest that this model is moderately effective and can accurately identify the true label for several test cases with a small margin of misclassification error. Furthermore, the F2score and accuracy indicate that the likelihood of incorrect predictions is marginal.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: accuracy, precision, recall, specificity, and F1score as shown in the table. On this binary classification task, the model achieved 77.51% (accuracy), 76.73%(precision), 77%. Besides, it has a specificity score of 77%, and a recall score equal to 7781%. Judging by these scores attained, we can conclude that this model is somewhat effective as it will be able to separate the examples belonging to the class labels #CA and #CB with a small margin of misclassification error.\",\n",
       " 'The classification performance of the model can be summarized as moderately high given that it achieved an accuracy of 77.51%, a recall score of (77.81%), a precision score equal to 76.73%, and finally, with an F2score of about 77%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the two-class labels #CA and #CB.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 74.07%, a specificity score of 81.31%, with the recall and precision scores equal to 66.57% and 77.45%, respectively. Judging based on the scores achieved, we can conclude that this model has a moderate classification performance and will be somewhat effective at correctly labeling most test cases drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 84.28%, a sensitivity score of about 84,83.83%, an AUC score equal to 84., a precision score (i.e. 83.43%), and a specificity scoreof 83%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true labels for several test instances/samples with a small margin of misclassification error. Furthermore, the precision and specificity scores show that the likelihood of examples belonging to label #CA being misclassified as #CB is marginal; however, given the picky nature of the algorithm, it is unlikely to make many false-negative predictions.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 84.28%, a sensitivity score equal to 84,29%, an AUC score of about 84., and a precision score (i.e. sensitivity) of 83.43%. In addition, it has an F1score of about 85.12%. Judging based on the above scores achieved, the model is shown to have a moderately high prediction performance and as such will be able to correctly classify several test samples drawn randomly from any of the class labels: #CA and #CB.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 74.07%, with the AUC, recall, and precision scores equal to 73.93%, 66.57%, and 77.45%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) score, we can say that it will likely have a lower false-positive rate.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 84.41%, a precision score of 85.08%, an AUC score equal to 80.48%, with a recall and specificity score, respectively, equal 67.32% and 93.63%. Judging by the scores achieved, it is fair to conclude that this model can accurately distinguish several test instances with marginal misclassification error. Besides, the precision and recall scores show that the likelihood of examples belonging to label #CA being misclassified as #CB is marginal; hence the confidence in prediction decisions related to the class #CB label is very high.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 84.41%, a specificity score of 93.63%, an AUC score equal to 80.48%, with recall and F1score equal to 67.32% and 75.16%, respectively. Judging by the scores, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 84.41%, a specificity score of 93.63%, with the recall and precision scores equal to 67.32% and 85.08%, respectively. Based on the scores across the different metrics under consideration, we can conclude that the model performs moderately well in terms of correctly predicting the true label for most of the test examples. Besides, it has a moderate to high confidence in the predicted output class labels.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 86.21%, a sensitivity (recall) score of 74.81%, with the F2score and precision scores equal to 76.49% and 84.07%, respectively. Judging by the scores, this model is shown to be effective and it can correctly identify the true label for several test instances/samples with only a few misclassifications.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 86.21%, a sensitivity (recall) score of 74.81%, with a precision score equal to 84.07%, an AUC score, 83.58%, and a specificity score 92.36%. These scores support the conclusion that this model will be moderately effective enough to sort between examples belonging to any of the different labels, #CA and #CB. Furthermore, from the precision and Specificity scores, we can conclude that it will likely misclassify some samples drawn randomly from any label #CA from the class #CB as #CA.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 86.21%, a specificity score of 92.36%, sensitivity (sometimes referred to as the recall) is 74.81%, precision score equal to 84.07%, and an F1score of 79.17%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the two different classes. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify only a small number of samples drawn randomly from any of the class labels #CA and #CB.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 86.21%, a specificity score of 92.36%, with precision and F1score equal to 84.07% and 79.17%, respectively. Judging based on the scores achieved, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective at correctly predicting the true label for the examples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on precision, F1score, specificity, and accuracy scored 43.58%, 53.26%, 92.36%, and 86.21%, respectively. These scores are lower than expected indicating how poor the model is at correctly picking out the test cases belonging to the minority class label #CB. The confidence for predictions of #CB is very low given the many false positive prediction decisions (simply by looking at the recall score). With the dataset being imbalanced, the accuracy score is only marginally higher than the dummy model.\",\n",
       " 'The machine learning algorithm employed on this classification problem scored 92.36% (Specificity), 86.21%(accuracy), 43.58% precision (43.38%), and 62.26% as the F2score. The specificity score is higher than precision, which indicates that some examples under #CA are likely to be mislabeled as #CB. This implies that the algorithm is very good at correctly identifying #CA examples but at the cost of being less precise at sorting out #CB samples. On the other hand, a very high accuracy score of 86%.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, F1score, accuracy, specificity, and specificity scored 86.17%, 73.3%, 83.72%, 94.48%, and 94., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and specificity score, we can say that it will likely have a lower false positive rate.\",\n",
       " 'The machine learning algorithm trained on this classification task attained an accuracy of 83.72%, a specificity score of 94.48%, with the precision and F2score equal to 86.17% and 67.28%, respectively. Judging by the scores achieved, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective at correctly predicting the true label for the examples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 83.72%, (2) Specificity score of 94.48%; (3) F2score of 67.28%, and (4) Precision score equal 86.17%. From the precision and F2score, we can estimate that the sensitivity score will be higher than the specificity score. This implies that some examples under #CA are likely to be mislabeled as #CB (i.e., low false positive rate). However, since the difference between these two metrics is not that huge, a valid conclusion that could be made here is that this model performs quite well in terms of correctly separating the examples belonging to the class label #CB from that of #CA.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 83.72%, a precision score of 86.17%, an AUC score equal to 79.13%, with a recall score and F1score equal to 63.78% and 73.3%, respectively. Judging by the scores, this model is shown to be somewhat effective at correctly pick out the test cases belonging to the class labels #CA and #CB. Besides, the specificity score shows that a fair amount of examples under #CA are correctly identified.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, sensitivity, F2score, and accuracy scored 84.75%, 59.06%, 62.87%, and 81.93%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F2score and precision scores, we can say that it will likely have a lower false positive rate.\",\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 79.25%, with the AUC, precision and sensitivity scores equal to 74.61%, 75.75%, and 59.84%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) and precision scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 81.93%, a precision score of 84.75%, sensitivity score (i.e. 59.06%) and an F1score of 69.61%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the different labels, #CA and #CB. Furthermore, from the F1score and precision scores, we can say that it will likely misclassify some proportion of samples drawn randomly from both class labels.',\n",
       " 'The classifier trained to solve the given classification task achieved a sensitivity score of 59.84%, a precision score, an accuracy of 75.25%, an AUC score equal to 77.61%, and a specificity score (i.e. the true negative rate i). On the basis of the scores above, the model is shown to have moderately low false positive and negative rates suggesting that the likelihood of examples belonging to class label #CB being misclassified as #CA is very marginal. In summary, it is fair to conclude that this model will be moderately effective at correctly recognizing the examples associated with each class or label.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy score of 85.24%, a sensitivity score equal to 81.03%, with the precision and F1score equal to 88.99% and 84.82%, respectively. Judging based on the scores achieved, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective at correctly labeling most of the test samples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classifier was trained on this imbalanced dataset to correctly separate the examples into two different classes (i.e. #CA and #CB ). The performance assessment conducted showed that it has a moderately low classification performance judging by the scores achieved across the metrics specificity, sensitivity, AUC, accuracy, and specificity as shown in the table. Specifically, the model has: (1) Accuracy equal to 57.44%, (2) Specificity score of 48.56%; (3) Sensitivity (sometimes referred to as the recall score) is 49.52%. (4) A precision of 59.48% with an accuracy of just about 57%. Overall, these scores show that this model will likely fail to identify the correct labels for a number of test cases belonging to both classes.',\n",
       " 'The classifier trained to solve the given classification task achieved a sensitivity score of 78.05%, a precision score equal to 84.71%, an F1score of 81.24%, specificity score (sometimes referred to as the recall score) is about 85.39%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA and #CB. Furthermore, from the F1score and precision scores, we can conclude that it will likely have a lower false positive rate.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 83.17%, (2) Precision score equal 85.4%; (3) F2score of 81.64%, and (4) Recall of 80.76%. Judging based on the scores above, it is fair to conclude that this model can accurately classify several test cases with a small margin of misclassification error. Besides, the precision and recall scores show that the confidence in predictions related to the class label #CB is high.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 83.17%, with the AUC, recall, and precision scores equal to 87.65%, 80.76%, and 85.4%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) score, we can say that it will likely have a lower false positive rate.',\n",
       " 'The performance evaluation metrics employed to assess the classification capability of the algorithm on this binary classification task were: accuracy (85.24%), precision (88.99%), recall (81.03%), AUC (87.32%) and F1score (84.82%). From the accuracy score, the classifier is shown to have a moderately high F1score indicating that it is fairly good at correctly predicting the true label for test cases related to the classes under consideration. Besides, from the precision and recall scores, we can conclude that the number of #CA being misidentified as #CB is moderately lower; hence the confidence in #CB predictions is usually high.',\n",
       " 'The classification performance scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 87.17%. (2) A precision score equal 90.35%.(3) Recall score of 83.74% (4) F2score of 84.98%. Since there is a class imbalance problem, only the F2score, precision, and recall scores are important metrics to accurately assess how good the classification ability of a model is. From these scores, we can conclude that this model has high confidence in its prediction decisions and will be very effective at correctly labeling most test cases drawn from the different labels ( #CA and #CB ).',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 79.25%, a sensitivity score of 59.84%, an AUC score equal to 77.61%, and a moderate F1score of 66.67%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can say that it will likely misclassify some test samples but will have high confidence in its prediction decisions.',\n",
       " 'The performance evaluation scores achieved by the model on this binary classification task are as follows: (a) Accuracy equal to 82.21%. (b) AUC score of 86.31%.(c) Sensitivity (sometimes referred to as the recall score) is 75.88%. This score indicates that some examples from the majority class #CA are being mislabeled as #CB (i.e., low false positive rate). (d) Precision score equals 87.51%. These scores across the different metrics show that this model has a moderate to high classification performance and will be able to accurately label several test cases drawn randomly from any of the classes under consideration. Furthermore, (e) F2score of 77.95% shows that the classifier has high confidence in the #CB predictions.',\n",
       " 'The classifier trained to solve the given classification task achieved a precision score of 90.35%, a sensitivity score equal to 90,73%, an accuracy score (sometimes referred to as the recall score) equal 87.17%, and a high recall of 83.74%. These scores support the conclusion that this model will be highly effective at picking out examples related to any of the classes ( #CA and #CB ) under consideration. In fact, it has a lower misclassification error rate. Overall, the performance is very impressive given that it was trained on such an imbalanced dataset.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 82.21%, a specificity score of 88.76%, sensitivity score (sometimes referred to as the recall score) of 75.88%, precision score equal to 87.51%, and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify only a small number of test cases.',\n",
       " 'The performance evaluation metrics employed to assess the classification performance of the algorithm on this binary classification task were as follows: AUC: 86.47%; accuracy: 81.66%; specificity: 85.39%; sensitivity: 78.05%; and precision: 82.26%. The very high specificity score implies that a large number of examples under #CA are correctly identified as #CB (meaning their true label is #CA ). From these scores, we can conclude that the classifier performs well in terms of correctly separating the #CA examples from that of #CB and vice-versa.',\n",
       " 'The performance evaluation metrics employed to assess the classification performance of the algorithm on this binary classification task were as follows: accuracy (81.66%), AUC (86.47%), specificity (85.39%), sensitivity (78.05%), and F1score (82.24%). Judging based on the scores above, it is fair to conclude that this model can accurately classify several test instances with a small margin of misclassification error. Besides, the F1score indicates the confidence in predictions related to the class label #CB is moderately high.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 81.33%, a recall score of 82.01%, and a precision score equal to 82%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA, #CB, and #CC ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 81.33%, a precision score of 82.77%, and an F1score of 80.83%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels ( #CA, #CB, and #CC ) under consideration.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 73.78%, a precision score of 77.74%, and an F2score of 73%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA, #CB, and #CC ) under consideration. Furthermore, from the F2score and precision scores, we can say that it will likely have a lower false positive rate.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: accuracy, recall, F1score, and precision. On this multi-class classification problem, the model has an accuracy of 73.78%, a recall score of 74.64%, an F1score of 72.87%, and a prediction accuracy equal to 74%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the labels ( #CA, #CB and #CC ) under consideration. Furthermore, from the F1score and recall scores, we can say that it will likely have a lower false positive rate.\",\n",
       " 'On this multi-class classification problem where the test instances are classified as either #CA or #CB or #CC, the classification performance of the learning algorithm is summarized by the following evaluation scores: (a) Recall = 73.51%. (b) Accuracy = 72.44%.(c) F1score = 71.94%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the labels #CA, #CB, and #CC. Furthermore, from the F1score and recall scores, we can conclude that it will likely have a lower false-positive rate.',\n",
       " 'The classification performance of the model can be summarized as moderately high given that it achieved an F2score of 72.31%, a recall of 73.51%, an accuracy of 72, and a precision score of 77.01%. These scores across the different metrics suggest that this model is somewhat effective and can accurately identify the true label for most of its test cases.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 73.78%, a recall and precision scores, respectively, equal to 7377% and 79.09%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA, #CB, and #CC ) under consideration. Furthermore, from the precision and recall scores we can conclude that it will likely have a lower false-positive rate.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: F1score, Recall, Precision, and Accuracy. For the accuracy, the model scored 72.01%; for the precision, it achieved 73.06% with the recall score equal to 71.56%. Trained on an imbalanced dataset, these scores are quite impressive. It has a lower false-positive rate (as shown by comparing precision and recall scores) hence the confidence in prediction decisions related to the minority class label #CB, is very high.\",\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: accuracy, recall, precision, and F1score as shown in the table. On this multi-class classification problem, the model has an accuracy of 76.44%, a recall score equal to 7683%, and a precision score of about 76%. Furthermore, it has moderate F1score and an F1score equal to76.03%, respectively. Judging by these scores attained, we can conclude that this model is somewhat effective and can accurately identify the true labels for several test cases with a small margin of misclassification error.\"]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputw[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "overhead-intellectual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The performance evaluation scores achieved by the classifier on this binary classification task are as follows (1) Accuracy equal to 90.67%. (2) Sensitivity score equal 87.29%; (3) Precision score equals 91.3%; and (4) F1score of 88.89%. The underlying dataset has a disproportionate amount of data belonging to the different classes; therefore, judging the performance of the model based on only the accuracy score is not very intuitive. Therefore, making judgments about the overall performance based solely on the F1score (balance between the recall and precision scores) is a valid statement. Since these scores are not that pperfect the might be able to assign the actual labels for a number of test cases.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy score of 85.33%, a sensitivity score equal to 79.13%, with the precision and F1score equal to 87.2% and 81.54%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify only a small number of samples drawn randomly from any of the two classes.',\n",
       " 'The scores achieved by the classifier on this classification task are as follows: Accuracy (47.92%), Recall (52.94%), Precision (34.81%), and finally, an F2score of 45.95%. Judging base on the scores above, the model is shown to have a lower classification performance than anticipated in terms of correctly picking out the test cases belonging to the classes #CA, #CB, and #CC. The confidence for predictions of #CB is very low given the many false positive prediction decisions (simply by looking at the recall and precision scores). With the dataset being imbalanced, we can conclude that the accuracy score is only marginally higher than the dummy model always assigning the majority class label #CA to any given test case.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 62.5%, a recall and precision scores of 63.49% and 66.95%, respectively. Based on the scores across the different metrics under consideration, we can conclude that the model performs moderately well in terms of correctly predicting the true label for most of the test examples. Besides, it has a moderate to high F1score indicating that its confidence in prediction decisions is moderately high.',\n",
       " 'The performance evaluation scores achieved by the model on this binary classification task are as follows: (a) Accuracy equal to 86.11%. (b) AUC score of 90.09%.(c) Precision score equal 89.07%; (d) Sensitivity (sometimes referred to as the recall score) is 84.29%. These scores indicate that the classifier has low false positive and false negative rates implying the likelihood of examples belonging to label #CA being misclassified as #CB is small; however, given the picky nature of the algorithm with respect to #CB examples, it is important to note that only a few examples from #CB are likely to be mislabeled as #CA and vice-versa. Overall, these scores support the conclusion that this model will be highly effective at correctly recognizing the true label for the majority of test cases drawn randomly from the different classes under consideration.',\n",
       " \"This model has a very high specificity score of 98.36%, an accuracy of 86.11%, a precision score equal to 89.07%, sensitivity score (i.e. recall) of 84.29%, and an F1score of 85.19%. Overall, it is fair to conclude that this model can correctly classify a large number of test cases drawn from the class labels #CA and #CB with a small margin of misclassification error. Besides, the F1score indicates the model's confidence in predictions related to the label #CB is very good.\",\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 93.31%, an AUC score of 94.36%, a precision score equal to 86.96%, sensitivity score (sometimes referred to as the recall score) is 87.29%, and finally, an F2score of 93%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels, #CA and #CB. Furthermore, from the precision and recall scores, we can conclude that it will likely have a lower false-positive rate.',\n",
       " 'The classifier trained to solve the given classification problem achieved an accuracy of 66.67%, a recall and precision scores, respectively, equal to 6698% and 66%. Besides, it has an F1score of 6631%. Judging by the scores mentioned above, we can conclude that this model has demonstrated a moderate classification performance and will be able to correctly classify a decent number of test samples drawn randomly from any of the labels: #CA and #CB.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 63.33%, (2) Specificity score of 31.25%, and (3) Sensitivity score equal 82.61%. (4) F1score of 71.7%. These scores are lower than expected indicating how poor the performance is. The false positive rate is high as a number of test cases belonging to class label #CA are likely to be misclassified as #CB (which is also the minority class with <|minority_dist|> of examples in the dataset). Overall, this model will fail to correctly identify the true label for several test instances.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows: 61.54% (accuracy), 82.61%(sensitivity), 63.33% score (precision), and an F1score of 71.7%. The model has a moderately low false positive and negative rates suggesting that the likelihood of examples belonging to class label #CA being misclassified as #CB is small, which is impressive but not surprising given the distribution in the dataset. Overall, this model achieved a moderate performance since it can accurately classify several test cases/instances with only a few misclassification errors.',\n",
       " 'The classification performance of the model is captured by the following evaluation metrics: AUC, accuracy, recall, and precision, respectively, equal to 98.62%, 95.77%, and 9541%. Judging by these scores attained, it is fair to conclude that this model can accurately classify several test cases with little misclassification error. Furthermore, the precision and recall scores show that the likelihood of misclassified samples is very marginal.',\n",
       " 'The classifier trained to solve the given AI task achieved the following performance evaluation scores: (a) Accuracy equal to 90.73%. (b) AUC score of 95.87%; (c) Precision score equal 89.13%; d) Sensitivity (sometimes referred to as the recall score) is equal or higher than 90% (e) Specificity (sensitivity) score is 90%. These scores across the different metrics show that this model is very effective and can correctly identify the true label for a large proportion of test cases with a small margin of misclassification error.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 85.11%, a sensitivity (recall) score of 90.07%, with a precision score equal to 63.95%, and an AUC score, respectively, of about 90%. These scores support the conclusion that this model will be highly effective at correctly predicting the true label for the majority of test cases drawn randomly from any of the class labels #CA and #CB. Furthermore, from the precision and recall scores, it is obvious that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in predictions related to the #CB label is very high.',\n",
       " \"The machine learning model's performance scores on this binary classification task as evaluated based on the precision, accuracy, F2score, and accuracy are 73.95%, 91.25%, 86.0%, and 73%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels, #CA and #CB. Furthermore, from the F2score and precision scores, we can conclude that it will likely misclassify some test samples drawn randomly from any of the two classes.\",\n",
       " 'This model scored an accuracy of 93.11%, an AUC of 94.07%, precision of 33.95%, F1score of 82.28% and an F1score (a balance between the recall and precision scores) of 82%. According to these scores attained, we can conclude that this model has a lower performance as it will not be able to correctly predict the actual labels of multiple test examples. Furthermore, the accuracy score is only marginally higher than the dummy model constantly assigning the majority class label #CA to any given test case.',\n",
       " 'The classifier was trained on this imbalanced dataset to correctly separate the examples into two different classes (i.e. #CA and #CB ). The scores achieved across the evaluation metrics are 86.59% (accuracy), precision (25.07%), recall (56.91%) and F1score (25%). From the F1score, we can see that only a few examples from #CA will likely be mislabeled as #CB (that is, it has a true-negative rate). Overall, the model is relatively unreliable with its prediction decisions. In conclusion, this model will likely fail to identify the correct labels for several test instances.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 98.45%, (2) Sensitivity score equal 90.2%, and (3) AUC score of 99.04%. (4) F1score of 93.95%. These results/scores are very impressive given that the dataset was imbalanced. The precision and sensitivity scores demonstrate that several samples from #CA are likely to be misclassified as #CB (i.e., low false positive rate). Therefore, based on the above conclusion, the likelihood of #CB predictions is lower for examples drawn randomly from any of the class labels #CA and #CB. Overall, this model is highly effective and performed quite well in terms of correctly predicting the true label for several test cases with a marginal misclassification error margin.',\n",
       " 'The scores achieved by the model on this binary classification task are 64.46% ( F2score ), 63.97%(accuracy), and a recall score of 64%. These results support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F2score and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 63.97%, a specificity score of 64.46%, with a recall and precision scores equal to 6474% and 6338%, respectively. Judging based on the above scores attained, we can conclude that this model has a moderate classification performance and will be somewhat effective at correctly predicting the true label for the majority of the test samples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classification performance of the algorithm explored based on the evaluation metrics accuracy, precision, F2score, and sensitivity scored 86.21%, 72.84%, 79.65%, and 72%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA, #CB, #CC,and #CD ) under consideration. Furthermore, the precision and F2score show that the likelihood of misclassifying #CA cases as #CB is marginal.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: accuracy, recall, precision, F1score, and specificity. On this multi-class classification problem, the model has an accuracy of 86.21%, a recall score of 82.03%; a precision score equal to 72.84%, and an F1score of 76.64%. Judging by the scores, it is fair to conclude that this model can correctly classify several test samples with only a few misclassifications.\",\n",
       " 'The performance evaluation scores achieved on this binary classification task are as follows: (a) Accuracy equal to 80.81%. (b) A precision score of 79.07%; (c) Sensitivity score equals 82.93%;(d) F2score equal to 82; (e) Moderate F2score of 82, and (f) Prediction accuracy of about 80% indicate that the model is good at correctly recognizing the #CA and #CB samples with a small margin of misclassification error. Overall, these scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels, #CB and #CC.',\n",
       " 'The performance evaluation scores achieved by the classifier on this binary classification task are as follows: (a) Accuracy equal to 80.81%. (b) Specificity score of 78.74%; (c) Sensitivity score (i.e. Recall) equal 82.93%; and (d) F1score of 8080%. These scores across the different metrics suggest that this model is moderately effective and can accurately identify the true label for several test instances with a small margin of misclassification error. Furthermore, the F1score and accuracy indicate that the likelihood of incorrect predictions is marginal; hence the confidence in predictions related to the label #CB is moderately high.',\n",
       " 'The classifier was trained on this imbalanced dataset to correctly separate the examples into two different classes (i.e. #CA and #CB ). The scores achieved across the metrics Specificity, Accuracy, AUC, and Sensitivity are 34.56%, 42.81%, 48.61%, and 32.88%, respectively. With such moderately low scores for specificity and sensitivity, this model is shown to have a very poor classification performance in terms of correctly picking out the #CB examples. In summary, it is not effective enough to sort out examples belonging to the minority class label #CB.',\n",
       " 'This model has a very high accuracy score of 90.11%, with an AUC score equal to 93.17% and a recall (sometimes referred to as sensitivity or true positive rate) score is 84.57%. These scores support the conclusion that this model will be highly effective in terms of telling-apart the examples drawn from the different classes ( #CA and #CB ) under consideration. Furthermore, the precision and recall scores show that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in prediction decisions related to the class label #CA is also high.',\n",
       " 'The classifier or algorithm scores 55.67%, 41.23%, 58.69%, and 31.38% across the evaluation metrics accuracy, sensitivity, AUC, and F1score, respectively, on this machine learning classification task. Judging by the scores achieved, this model is shown to be less effective at correctly predicting the true labels for test cases drawn randomly from any of the class labels. The confidence for predictions of #CB is very low given the many false positive prediction decisions (simply by looking at the recall and precision scores). With such a low confidence in the labeling decision, the model will fail to correctly identify the majority of examples belonging to both classes, #CA and #CB.',\n",
       " 'The performance evaluation scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 72.59%. (2) AUC score of 75.08%; (3) Sensitivity (sometimes referred to as the recall score) is 72; (4) Precision score equal 72%, (5) F2score equal to72.29% (6) and (7) an F2score of 72..12%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the labels #CA and #CB. Furthermore, from the F2score and accuracy scores, we can conclude that it will likely have a lower misclassification error rate.',\n",
       " 'The evaluation metrics employed to assess or assess the performance of the algorithm on this binary classification task were as follows: the F2score, accuracy, recall, and precision. The classifier has an F2score of 74.2%, an accuracy score of 74% with a precision score equal to 74%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can conclude that it will likely have a lower false-positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved a sensitivity score of 82.11%, a precision score equal to 78.91%, an F1score of 80.47%, and a specificity score (i.e. the ability to correctly detect the negative class #CA observations) of 78%. In addition, it has an accuracy of 80% and an Specificity score that is equal or higher than expected. Judging by the scores obtained, the model is shown to have a moderately high prediction performance in terms of correctly picking out the test cases belonging to the class label #CB from the population with a marginal likelihood of misclassification error.',\n",
       " \"The classifier was trained based on the labeling objective where a given test case is labeled as either belonging to class #CA or #CB. The classification performance can be summarized as moderately high given that it achieved an accuracy of 76.89%, a precision score of 38.16%, an F1score of 63.48%, and a specificity score equal to 79.95%. From the precision and specificity scores, we can estimate that the sensitivity score will likely be identical to the recall score, therefore judging by the difference between the two metrics' scores it is safe to say this model can correctly identify a fair amount of test examples drawn randomly from any of the classes.\",\n",
       " \"The classifier's performance on this binary classification task was evaluated based on the following evaluation metrics: accuracy, precision, F1score, and sensitivity score. On the basis of the scores across the different metrics under consideration, we can conclude that the model performs very well in terms of correctly predicting the true label for the majority of test cases related to class labels #CA and #CB. Specifically, the accuracy score is 94.12%, precision score equal to 86.42%, and F1score of 92.11%. Judging by the near-perfect scores, it is fair to say that this model can be trusted to make several classification errors with a lower misclassification error rate.\",\n",
       " 'The scores achieved by the classifier on this binary classification task are as follows (1) Accuracy equal to 94.12%. (2) Specificity score of 91.73%.(3) Sensitivity score equal 98.59%; (4) F1score of 92.11%. These results/scores are very impressive given that the dataset was imbalanced. The very high specificity score implies most of the #CA examples are correctly identified as #CA. Furthermore, the F1score indicates that a large number of #CB predictions are correct. Overall, these scores support the conclusion that this model will be highly effective at correctly labeling most test cases drawn from the different classes under consideration with only a small margin of error.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 88.13%, recall of 84.11%, AUC of 96.12%, and a precision score equal to 84%. These scores support the conclusion that this model will be highly effective at correctly labeling most of the test samples drawn from the different labels ( #CA and #CB ) under consideration. Furthermore, from these scores, it is valid to conclude that the likelihood of misclassification is very low (actually, It is only marginal).',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the precision, accuracy, specificity, and recall scored 78.91%, 81.23%, 92.3%, and 57.7%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) and precision scores, we can say it will likely have a lower false positive rate.\",\n",
       " 'The evaluation metrics employed to assess the performance of the model on this binary classification task were: recall, accuracy, precision, F1score, and accuracy. From the table shown, we can see that it has an accuracy of 80.96% with moderate precision and recall scores equal to 75.21% and 66.97%, respectively. Besides, the F1score is 71.04%. Judging based on the scores above, it is fair to conclude that this model can accurately classify several test cases with only a few misclassifications. In summary, its classification performance can be summarized as moderately high.',\n",
       " 'The classification performance can be summarized as moderately high given that it achieved a predictive accuracy of 71.11%, a precision score of 67.86%, sensitivity score equal to 72.38%, specificity score (sometimes referred to as the recall score) is 70.02%. These scores across the different metrics suggest that this model will be somewhat effective at correctly labeling most of the test cases drawn randomly from the class labels #CA and #CB. In addition, a small number of examples belonging to #CA will likely be misclassified as #CB (i.e. low false positive rate).',\n",
       " 'The classifier trained to solve the given classification task achieved a sensitivity score of 72.38%, a specificity score equal to 70.02%, an F2score of 71.42%, and an accuracy of 71%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA and #CB. Furthermore, from the F2score and Sensitivity scores, we can conclude that it will likely misclassify only a small number of test cases.',\n",
       " 'The scores achieved by the classifier on this binary classification task are 78.22%, 82.86%, 73.73%, 80.71%, and 78., respectively, across the metrics accuracy, sensitivity, precision, F2score, AUC, and precision. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can conclude that it will likely misclassify a small number of samples drawn randomly from any of the two classes.',\n",
       " 'The classifier trained to tackle the classification task (where a given test case is labeled as either #CA or #CB ) achieved an accuracy of 78.22%, a precision score of 73.73%, sensitivity score equal to 82.86%, specificity score (sometimes referred to as the recall score) is 74.17%, and finally, an F1score of 78%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to each label under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify a small number of test cases drawn randomly from any of the two classes.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the precision, accuracy, sensitivity, specificity, and F1score scored 77.91%, 74.67%, 63.81%, 84.17%, and 70.16%, respectively. These scores are moderately higher than expected indicating how good the classifier is in terms of correctly picking the true label for most test cases related to the different classes under consideration. The precision and specificity scores show that the likelihood of misclassifying #CA and #CB test samples is lower; hence the confidence in #CB predictions is also high.',\n",
       " 'This model has an AUC score of 73.99%, an accuracy of 74.67%, specificity of 84.17%, F2score of 66.21%, and an F2score (computed based on the precision and sensitivity score) is about 73%. This model is shown to be somewhat good at correctly predicting the true label for test cases drawn randomly from any of the class labels #CA and #CB. The high specificity score shows that a fair amount of examples under #CA are correctly identified. Furthermore, the F2score shows that the model tries its best to avoid making many false-positive predictions, so it assigns the #CB class to only a subset of new cases. Overall, these scores support the conclusion that this model will likely misclassify a small number of test samples but will be able to accurately identify the correct classification labels for the majority of them.',\n",
       " 'As shown in the metrics table, the classifier trained to solve the given AI task achieved a predictive accuracy of 78.22%, a precision score of 79.17%; a specificity score equal to 83.34%, and a recall score (i.e. sensitivity) equal 72.38%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely misclassify some proportion of samples drawn randomly from any of the classes.',\n",
       " \"For this classification task, the model's performance was evaluated as accuracy (72.44%), precision (79.45%), and recall (55.24%). The scores achieved across these evaluation metrics indicate that this model is somewhat effective and can accurately identify the true label for most of the test cases/samples. However, predictions from the classifier should be taken with caution. Unlikelihood of misclassification is high considering the difference between the precision, recall, and accuracy scores.\",\n",
       " \"The classifier's performance on this binary classification task as evaluated based on the F1score, AUC, accuracy, and specificity scored 65.17%, 71.34%, 87.51%, 72.44%, and 71., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the likelihood of misclassification is marginal; hence the confidence in prediction decisions related to any of the two classes is high.\",\n",
       " 'The evaluation metrics employed to assess the performance of the model on this binary classification task were AUC, accuracy, specificity, F1score, and predictive Accuracy. The scores achieved across these metrics are 73.39%, 72.33%, 71.22%, and 72., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and Specificity scores, we can conclude that it will likely misclassify a small number of samples drawn randomly from any of those classes.',\n",
       " 'The classification performance of this model can be summarized as follows: it has an accuracy of 73.33%, a moderate F2score of about 72.45%, and a precision score of 70.28%. These scores support the conclusion that this classifier will likely be moderately effective enough to sort between the examples belonging to any of the different labels ( #CA and #CB ) under consideration. Furthermore, from the F2score and precision scores, we can conclude that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in prediction decisions related to the #CB label is very high.',\n",
       " 'The machine learning model trained on this classification task achieved an accuracy of 70.22%, a recall and precision scores of 73.33% and 66.38%, respectively. Based on the evaluation metrics used to assess the performance of the model, we can conclude that this model has moderate classification performance and will be somewhat effective at correctly recognizing the true label for the majority of test cases belonging to the different classes ( #CA and #CB ).',\n",
       " 'The machine learning algorithm employed on this classification problem has an accuracy of 70.22%, specificity of 67.52%, F2score of 71.83%, and a moderate F2score equal to 70%. Based on the scores across the different metrics under consideration, we can conclude that this model will be somewhat effective at correctly predicting the true label for the majority of test cases drawn randomly from any of the class labels ( #CA and #CB ).',\n",
       " 'The classifier was trained to assign test cases to one of the following classes #CA, #CB, #CC, and #CD. The scores achieved across the evaluation metrics are 55.11% (accuracy), 54.99% precision score (54.35%), and an F1score of 54%. Judging by the scores, this model is shown to be not that effective at correctly predicting the true labels for multiple test examples. In summary, it fails to recognize a fair amount of test instances.',\n",
       " 'The scores achieved by the classifier on this classification task are 53.33%, 52.07%, 54.23%, and 50.71%, respectively, across the following evaluation metrics: accuracy, recall, precision, and F1score. Judging base on the scores above, we can conclude that this model has a lower classification performance as it is not be able to correctly predict the actual labels of multiple test examples. Furthermore, the accuracy score is only marginally higher than the dummy model constantly assigning the majority class label #CA to any given test case.',\n",
       " 'The machine learning algorithm trained on this classification task was evaluated and it achieved a prediction accuracy of 79.72% with the associated precision and recall scores equal to 82.15% and 75.0%, respectively. Based on the scores across the different metrics under consideration, we can conclude that the algorithm performs fairly well in terms of correctly predicting the true label for most of the test cases. Besides, it has a moderate to high F1score indicating that its confidence in predictions related to label #CB is moderately high.',\n",
       " 'The machine learning model trained on this classification task scored: accuracy (79.72%), specificity (84.28%), sensitivity (75.0%), precision (82.15%), and an AUC score of 79.65%. These scores support the conclusion that this model will be moderately effective enough to sort between examples belonging to any of the different labels, #CA and #CB. Furthermore, from the precision and recall scores, we can say that it will likely misclassify some proportion of samples drawn randomly from both class labels.',\n",
       " \"The classifier's performance scores are as follows: accuracy (79.72%), sensitivity (75.0%), specificity (84.28%), F2score (76.33%), and AUC score of 79.65%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels, #CA and #CB. Furthermore, from the F2score and sensitivity scores, we can conclude that it will likely have a lower false-positive rate.\",\n",
       " 'The performance of the model on this binary classification task as evaluated based on accuracy, sensitivity, AUC, specificity, and predictive accuracy scored 75.04%, 72.19%, 74.98%, 77.78%, and 72., respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, the precision and recall scores show that the likelihood of misclassifying #CA cases as #CB is marginal; hence the confidence in prediction decisions related to their #CB label is high.',\n",
       " 'On this imbalanced classification task, the model was trained to assign test cases to one of the following classes #CA and #CB. Evaluations or assessment conducted based on the metrics accuracy, AUC, precision, specificity, F2score, and Specificity show that it has fairly high classification performance and will be able to correctly identify the true label for most test instances. Specifically, it scored: (a) Accuracy equal to 75.04%. (b) A precision score of 75% (c) F2score equal to 77.59%. Besides, (d) The specificity score (sometimes referred to as the sensitivity score) is 7778%. These scores across the different metrics show a fair understanding of this machine learning problem. In essence, we can assert that this model has high confidence in its prediction decisions and can correctly classify a moderate proportion of test samples drawn from the two classes.',\n",
       " \"The classifier's performance scores are 77.51%, 76.73%,77.81%, and 77.,27%, respectively, based on the following evaluation metrics: accuracy, precision, recall, specificity, and F1score. On this machine learning classification problem, these scores support the conclusion that the model will be moderately effective enough to sort between the examples belonging to any of the labels ( #CA and #CB ) under consideration. Furthermore, the likelihood of misclassification is marginal.\",\n",
       " 'The classification performance evaluation scores achieved by the model on this binary classification task are as follows: (a) Accuracy equal to 77.51%. (b) F2score of about 76.59%; (c) Recall (sensitivity) score of 77; (d) Precision score (sometimes referred to as the sensitivity score) is 76%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels, #CA and #CB. Furthermore, from the F2score and precision scores, we can conclude that it will likely have a lower false-positive rate.',\n",
       " \"On this machine learning classification task, the model's performance was evaluated based on the following evaluation metrics: accuracy, specificity, recall, and precision. For the accuracy and specificity scores, it scored 74.07% and 81.31%, respectively. The precision score is 77.45%, with the recall score equal to 66.57%. Judging by these scores attained, we can conclude that this model has a moderate classification performance and will be somewhat effective at correctly recognizing the examples drawn from the different labels ( #CA and #CB ) under consideration. In fact, its misclassification error rate is about <acc_diff> %.\",\n",
       " 'The performance of the model on this binary classification task as evaluated based on the precision, accuracy, AUC, specificity, and sensitivity scores are 83.43%, 84.28%, 85.29%, and 8483.74%, respectively. These scores support the conclusion that this model will be highly effective at correctly recognizing the true label for the majority of test cases drawn randomly from the class labels #CA and #CB. Furthermore, the likelihood of misclassification is very marginal.',\n",
       " 'The performance of the model on this binary classification task as evaluated based on the precision, accuracy, AUC, sensitivity, F1score, and predictive accuracy scored 83.43%, 84.28%, 85.29%, 86.12%, and 8483%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the accuracy score, it is valid to conclude that it will likely misclassify only a small number of samples drawn randomly from each label.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 74.07%, a specificity score of 81.31%, with the AUC, recall and precision scores equal to 73.93%, 66.57%, and 77.45%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the different labels, #CA and #CB. Furthermore, from the recall (sensitivity), we can assert that it will likely have a lower false-positive rate.',\n",
       " 'The machine learning model trained on this classification task achieved an accuracy of 84.41%, a specificity score of 93.63%, with the AUC, recall and precision scores equal to 80.48%, 67.32%, and 85.08%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the two different labels, #CA and #CB. Furthermore, from the recall (sensitivity), we can say that it will likely have a lower false positive rate as indicated by the precision and recall scores.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 84.41%, a specificity score of 93.63%, an AUC score equal to 80.48%, with recall and F1score equal to 67.32% and 75.16%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and recall scores, we can say that it will likely have a lower false-positive rate.',\n",
       " 'The classification performance scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 84.41%. (2) Specificity score of 93.63%.(3) F2score of 70.25% (4) Recall (sensitivity) score equal 67.32%; (5) Precision score equals 85.08%. From the accuracy and specificity scores, we can see that the F2score is a moderate metric that indicates how good the classifier is at correctly assigning the appropriate label to test cases related to any of the labels #CA and #CB. Furthermore, since the difference between the precision and recall scores is not that high, some examples belonging to #CB are mistakenly classified as #CA. Overall, these scores indicate that this classifying performance can be reasonably trusted to be true given that it has a relatively moderate to high confidence in its prediction decisions.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 86.21%, a sensitivity (recall) score of 74.81%, with the F2score and precision scores equal to 76.49% and 84.07%, respectively. Judging based on the scores achieved, we can conclude that this model has a moderate classification performance, and hence will be somewhat effective at correctly recognizing the examples drawn from the different labels ( #CA and #CB ) under consideration.',\n",
       " 'The classifier trained to tell-apart the examples belonging to the different classes ( #CA and #CB ) was evaluated based on the metrics accuracy, AUC, precision, specificity, and sensitivity. The scores achieved across these metrics are 86.21%, 84.07%, 92.36%, 74.81%, and 83.58%, respectively. These scores support the conclusion that this model will be highly effective at correctly predicting the true label for the majority of the test examples drawn randomly from the class labels. Furthermore, the precision and specificity scores show that the likelihood of misclassifying #CA examples as #CB is marginal; hence the confidence in predictions related to their class label is very high.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 86.21%, a specificity score of 92.36%, sensitivity (sometimes referred to as the recall) is 74.81%, precision score equal to 84.07%, and an F1score of 79.17%. Judging based on the scores above, it is fair to conclude that this model can accurately identify the true label for several test instances with only a few misclassifications. Besides, the F1score and specificity scores show that the likelihood of examples belonging to label #CA being misclassified as #CB is marginal; however, given the picky nature of the algorithm, some cases labeled #CB might end up being wrongly labeled as #CA.',\n",
       " \"The classifier trained to solve the given classification task achieved an accuracy of 86.21%, a precision score of 84.07%; a specificity score equal to 92.36%, and an F1score of 79.17%. Judging based on the scores achieved, it is fair to conclude that this model can accurately distinguish several test cases with marginal misclassification error. Besides, the F1score indicates the model's classification confidence of output predictions related to label #CB is moderately high.\",\n",
       " \"The machine learning algorithm employed on this classification task scored 86.21% for accuracy, 92.36% specificity, 43.58% precision, and 53.26% F1score. The F1score is a measure that encompasses a model's ability to detect both class #CA and #CB, but it is low here given that the data was severely imbalanced. This means that only the specificity score and precision score are important metrics to correctly assess how good the model is on the classification problem. From the F1score, we can estimate that this model will have a moderate performance as it will likely fail to identify the correct class label for a number of test cases.\",\n",
       " 'The machine learning algorithm employed on this classification task scored 86.21% (accuracy), 92.36%(specificity), 43.58% precision (43.38%), and 62.26% as the F2score. The specificity score is higher than precision, which indicates how good the algorithm is at correctly partitioning out the #CA and #CB samples. However, from the precision and F2score, we can see that some examples from #CB are likely to be mislabeled as #CA given that the difference between them is not that high. Overall, this algorithm has relatively poor classification performance judging by the accuracy score achieved. In summary, it will struggle to identify examples under both classes.',\n",
       " \"On this binary classification task, the model's performance as evaluated based on accuracy, precision, specificity, and F1score scored 83.72%, 86.17%, 94.48%, and 73.3%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can say that it will likely misclassify a small number of samples drawn randomly from any of the two classes.\",\n",
       " 'The scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 83.72%. (2) Specificity score of 94.48%; (3) Precision score equal 86.17%; and (4) F2score of 67.28%. The underlying dataset has a disproportionate amount of data belonging to the different classes; therefore, judging the effectiveness of the classification based on only the accuracy score is not very intuitive. Therefore, from the precision and F2score, we can make the conclusion that this model will have a moderate classification performance when it comes to examples drawn randomly from any of these classes. However, looking at the specificity score, there is little confidence in the prediction output decisions related to #CB. Furthermore, even the dummy model constantly assigning label #CA for any given test example will likely outperform this performance in terms of correctly predicting the true label for the majority of test cases relatedto the class label',\n",
       " 'The classification performance scores achieved by the model on this binary classification task are as follows: (1) Accuracy equal to 83.72%, (2) Specificity score of 94.48%; (3) F2score of 67.28%, and (4) Precision score equal 86.17%. Judging based on the scores above, it is fair to conclude that this model can accurately identify the true label for several test instances/samples with marginal misclassification error margin. Besides, the precision and F2score show that the false positive rate is low, which goes further to show how good the classifier is.',\n",
       " 'The performance evaluation metrics employed to assess the classification capability of the algorithm on this binary classification task were as follows: (a) AUC score of 79.13%. (b) Accuracy equal to 83.72%; (c) Precision score equal 86.17%;(d) Specificity (94.48%), (e) Recall (63.78%) and (f) F1score of 73.3%. The very high specificity coupled with the precision and recall scores demonstrate that the classifier is very good at identifying items belonging to #CA. Overall, the scores support the conclusion that this model will be highly effective at correctly labeling several test cases drawn from the different classes, #CA and #CB.',\n",
       " 'The scores achieved by the model on this binary classification task are 81.93% (accuracy), 59.06%(sensitivity), 84.75% precision score (sometimes referred to as the sensitivity score), and 62.87% F2score (a balance between the recall and precision scores). From the F2score, we can see that this model has moderately low false positive and negative rates suggesting that the likelihood of examples belonging to label #CA being misclassified as #CB is moderately small, which is impressive but not surprising given the data was balanced. Overall, the scores are motivating the conclusion that it can accurately produce the true label for a number of test cases drawn randomly from any of the class labels.',\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 79.25%, (2) AUC score of 74.61%, and (3) Sensitivity (recall or sensitivity) score 59.84%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false-positive rate.',\n",
       " 'The classifier trained to classify test samples based on the following class labels #CA and #CB achieved the scores 81.93% (accuracy), 59.06%(sensitivity), 74.81% as the AUC score, 84.75% precision score with the F1score equal to 69.61%. Judging by these scores attained, it is fair to conclude that this model has a moderate classification performance and will be somewhat effective at correctly recognizing the examples belonging to each class label under consideration. However, considering the difference between sensitivity and precision scores, there could be some instances where test cases labeled as #CB are mistakenly classified as #CA.',\n",
       " 'The classifier trained to solve the given classification task achieved the following performance evaluation scores: (a) Accuracy equal to 79.25%. (b) Specificity score of 89.38%.(c) AUC score (indicating how good the model is at telling apart the #CA and #CB observations). (d) Precision (75.26%). (e) Sensitivity (sometimes referred to as the recall score) is 59.84%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the two classes. Furthermore, confidence in predictions related to label #CB is moderately high given that it has a relatively low false-positive rate.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy score of 85.24%, a sensitivity score equal to 81.03%, with the precision and F1score equal to 88.99% and 84.82%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can say that it will likely misclassify only a small number of samples drawn randomly from each label.',\n",
       " \"The classifier's performance on this binary classification task as evaluated based on specificity, accuracy, AUC, sensitivity, and precision scored 48.56%, 57.44%, 59.48%, 49.52%, and 59., respectively. These scores were achieved on an imbalanced dataset. From the specificity and sensitivity scores, we can see that the model tends to be good at correctly identifying #CA samples but at the cost of only being correct 59% of the time when labeling part of #CB. The model has moderately low precision and accuracy scores hence will struggle to correctly identify examples belonging to class #CB than #CA.\",\n",
       " 'The scores achieved by the model on this binary classification task are as follows (1) Accuracy equal to 81.66%. (2) Sensitivity score equal 78.05%.(3) Specificity score of 85.39%; (4) Precision score equals 84.71%; and (5) F1score of 81..24%. The underlying dataset has a disproportionate amount of data belonging to the different classes; therefore, judging the effectiveness of the classification based on only the accuracy score is not very intuitive. Therefore, making judgments about the overall performance of a model based solely on the F1score (a balance between the recall and precision scores) is a valid statement. This is because the difference between precision and recall shows that a high quantity of actual #CB predictions is likely to be misclassified as #CA given that the precision is greater than recall. Overall, the scores are impressive but not surprising given the data was balanced between class labels.',\n",
       " \"On this imbalanced classification task, the model's performance as evaluated based on the F2score, Accuracy, Precision, and Recall are 81.64%, 83.17%, 85.4%, and 80.76%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.\",\n",
       " 'The machine learning algorithm trained on this classification task achieved an accuracy of 83.17%, with the AUC, recall, and precision scores equal to 87.65%, 80.76%, and 85.4%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the recall (sensitivity) score, we can say that it will likely have a lower false positive rate.',\n",
       " \"The classifier's performance scores on this binary classification task are as follows: (a) Accuracy equal to 85.24%. (b) A precision score equal 88.99%; (c) Recall (sensitivity) score equals 81.03%;(d) F1score equal to 84.82%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels, #CA and #CB. Furthermore, from the F1score and precision scores, we can conclude that it will likely have a lower false-positive rate.\",\n",
       " 'The classifier trained to solve the given classification task achieved the following performance evaluation scores: (a) AUC: 89.07%. (b) Accuracy: 87.17%; (c) Precision: 90.35%.(d) F2score : 84.98%. Besides, the recall (sensitivity) and precision scores are 83.74% and (e) Recall: 83%, respectively. Judging by the scores above, we can conclude that this model has a high classification performance and as such will be quite effective at correctly recognizing the observations drawn from the different labels ( #CA and #CB ) under consideration. In summary, it has high confidence in its prediction decisions.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 79.25%, a sensitivity score of 59.84%, an F1score of 66.67%, and a precision score equal to 75.75%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify a small number of samples drawn randomly from any of the two classes.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 82.21%, sensitivity score of 75.88%, AUC score equal to 86.31%, F2score equal to 77.95%, and a precision score 87.51%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA and #CB ) under consideration. Furthermore, from the F2score and precision scores, we can conclude that it will likely misclassify only a small number of samples drawn randomly from any of the two classes.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy score of 87.17%, a specificity score equal to 90.73%, with the recall and precision scores, respectively, equal 83.74% and 90%. Judging by these scores attained, it is fair to conclude that this model can accurately distinguish several test cases with little misclassification error. Besides, the precision and recall scores show that the model has a high false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 82.21%, a specificity score of 88.76%, sensitivity score (sometimes referred to as the recall score) of 75.88%, precision score equal to 87.51%, and finally, an F1score of 81.28%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to any of the labels under consideration. Furthermore, from the F1score and precision scores, we can conclude that it will likely misclassify only a small number of test cases.',\n",
       " 'The performance evaluation metrics employed to assess the classification performance of the algorithm on this binary classification task were accuracy, AUC, specificity, sensitivity, and specificity. The scores achieved across these metrics are 81.66%, 86.47%, 85.39%, 78.05%, and 78%. According to these scores, it can be said that this classifier has a moderate performance and will be able to correctly identify the true label for most test cases drawn randomly from the class labels #CA and #CB.',\n",
       " 'The performance evaluation metrics employed to assess the classification capability of the classifier on this binary classification task were as follows: accuracy (81.66%), AUC (86.47%), specificity (85.39%), sensitivity (78.05%), and F1score (82.24%). Judging based on the scores, the model demonstrates a moderately high classification performance and will be able to accurately label several test cases drawn from the different labels under consideration ( #CA and #CB ).',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 81.33%, a recall score of 82.01%, and a precision score equal to about 82%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the three-class labels ( #CA, #CB, and #CC ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classifier trained to solve the given classification task achieved an accuracy of 81.33%, a precision score of 82.77%, and an F1score of 80.83%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA, #CB, and #CC. Furthermore, from the F1score and precision scores, we can say that it will likely have a lower false positive rate.',\n",
       " 'The classification performance scores achieved by the model on this multi-class classification task are as follows (1) Accuracy equal to 73.78%, (2) Precision score equal 77.74%, and (3) F2score of 73%. This classifier demonstrates a relatively high classification ability given that it was trained on a balanced dataset with an identical number of cases under each label #CA, #CB, and #CC. In essence, we can assert that the classification capability of the classifiers is fairly high and will be able to correctly classify most test samples with only a small margin of error.',\n",
       " \"The algorithm's prediction performance on this multi-class classification problem where the test instances are classified as either #CA or #CB or #CC is: Accuracy is 73.78%, Recall is 74.64%, and finally, an F1score of 72.87%. These scores across the different metrics show that this algorithm has a moderate to high classification performance and will be able to accurately label most of the examples sampled from each class label under consideration.\",\n",
       " 'This model has a fairly moderate classification performance as indicated by the scores achieved across the evaluation metrics: accuracy, recall, F1score, and precision. With an F1score of 71.94%, the model is shown to have a somewhat low false positive rate. Besides, the accuracy score is 72.44%. The model does fairly well to avoid false negatives than it avoids false positives.',\n",
       " 'On this multi-class classification problem where the test instances are classified as either #CA or #CB or #CC, the classification performance of the learning algorithm is summarized by the following evaluation scores: (a) Accuracy equal to 72.44%. (b) F2score of 72; (c) Recall equals 73.51%; (d) Precision score equals 77.01%. These scores across the different metrics suggest that this model will be moderately effective enough to sort between the examples belonging to the three labels. Furthermore, from the F2score and precision scores, we can conclude that it will likely misclassify only a few samples of all the possible labels under consideration.',\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy of 73.78, a recall score of about 74.77%, and a precision score equal to 79.09%. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to the different labels ( #CA, #CB, and #CC ) under consideration. Furthermore, from the precision and recall scores, we can say that it will likely have a lower false-positive rate.',\n",
       " \"The classifier's performance was evaluated based on the following evaluation metrics: F1score, Recall, Precision, and Accuracy. For the accuracy, it scored 72.01%, for the precision it achieved 73.06% with the recall score equal to 72%. Trained on a balanced dataset, these scores are quite impressive. It has a moderate F1score (71.54%) which means that its prediction decisions can be reasonably trusted. The precision and recall scores demonstrate that the model tries its best to avoid making many false-negative predictions, so it assigns the #CB class to only a subset of new cases. Overall, we can conclude that this model has relatively high classification performance and will be able to correctly classify most test samples.\",\n",
       " 'The classifier trained to solve the given AI task achieved an accuracy score of 76.44%, a recall (sometimes referred to as sensitivity or true positive rate) score, a precision score and an F1score equal to 7683%, and 7603%, respectively. These scores support the conclusion that this model will be moderately effective enough to sort between the examples belonging to any of the labels #CA, #CB, and #CC. Furthermore, from the F1score and precision scores, we can say that it will likely have a lower false-positive rate.']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputp[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-copper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "widespread-partnership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 90.67%, (b) Specificity score of 88.89%, and (c) Precision score = 90%. From these scores, we can conclude that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy equal to 79.33%, (b) Specificity score equal 87.13%, and (c) Sensitivity score of 88.54%. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 45.92% (accuracy), 52.94% accuracy (recall), and 45,92%. These scores across the different metrics suggest that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 62.95% (recall), 62%.95%, 62,49%, 63.07% and 62 for95%), respectively. Based on the scores across the different metrics, we can conclude that this model will be moderately effective at correctly predicting the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy equal to 84.33%, (b) Specificity score equal 86.07%, and (c) Sensitivity score of 89.11%. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 89.07% (b) Specificity score of 86.11%. (c) Sensitivity score = 85.36%. From the precision and sensitivity scores, we can see that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 93.31% (b) Specificity score of 94.36% and (c) Precision score is 93%. From these scores, we can conclude that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 66.67% (accuracy), 66,98% for recall, 66%.66% accuracy (recall) and 66%, respectively. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 82.61% (accuracy), 71.33% for specificity, and 71 with a precision score equal to 82%. The model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 82.33% (accuracy), 63.61% for recall, 61.54% accuracy, and 71.31% precision. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 95.41% (accuracy), 98.31% for recall, and 95%, respectively. These scores across the different metrics suggest that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 90.73% (accuracy), 95.32% accuracy (sensitivity) and 89.33% for sensitivity. These scores across the different metrics suggest that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 90.07%, (b) Specificity score of 90% and (c) Sensitivity score = 85.23%. From the precision and sensitivity scores, we can see that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 91.25%, (b) Recall score of 86.0%, and (c) Specificity score. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy equal to 93.28% (b) Recall equal 94.07% and (c) AUC score of 33.11%. From these scores, we can conclude that the model has a very high classification performance hence will be able to correctly classify several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score of 86.91% (b) Recall score equal to 25.59% and (c) Precision score = 25%. From the precision and recall scores, we can see that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy = 98.95% (b) Specificity = 93.04% and (c) Precision = 90.45%. From the scores across the metrics, we can conclude that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 64.97% (i.e. accuracy), 64,74%, 63.74% and 64 for97%, respectively. The model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 64.74% (accuracy), 63.46% for recall, 64%.38% accuracy, and 64%, respectively. These scores indicate that the model has a moderate classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 79.21%, (b) Recall score of 86.84%. (c) Specificity score: 79% (d) Precision score = 72.65%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 76.21%, (b) Recall score of 86.84%, and (c) Specificity score = 82.03%. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 82.93% (accuracy), 79.07% for sensitivity, 82%.81% accuracy, 80.81%, and 82 with a precision score equal to 82 of93%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 80.81%, 80,74%, and 80% respectively. Based on the scores across the metrics, we can conclude that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 32.81% (accuracy), 48.88% for sensitivity, 32%.61% accuracy, 34.56% specificity, and 32%, respectively. These scores across the different metrics suggest that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 93.15% (b) Recall score of 90.17% and (c) AUC score = 93%.15%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score of 41.38% (b) Specificity score, (c) Sensitivity score equal to 55.67% and (d) F1score of 58.39%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 72.36% (accuracy), 72,29% for sensitivity, 72%.36%, and 75.12%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 74.02% (for the accuracy), 73.08% for the recall, 74%.2%, and 75.51% respectively. These scores are very high indicating that the model has a moderate classification performance hence will be able to correctly classify several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 78.11%, (b) Specificity score of 80.91%, and (c) Sensitivity score = 78% with the F1score equal to 82.47%. From the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 76.89% (accuracy), 76,89%, and 79.95%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 94.12% (accuracy), 92.42% for recall, and 86.11% accuracy. These scores across the different metrics suggest that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 92.59% (accuracy), 91.73% for sensitivity, 98.12% accuracy, and 94.11% specificity. These scores across the different metrics suggest that the model has a very high classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 84.13% (accuracy), 88.11% for recall, 84%.13%, and 84 with the precision and recall equal to 84.,13%. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 81.23% (b) Recall score of 92.91%. (c) Specificity score: 91.3%. From the precision and recall scores, we can see that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 75.96%, (b) Recall score of 75,21%, and (c) Specificity score. From these scores, we can see that the model has a moderate classification performance hence will be able to correctly classify several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 71.38%, (b) Specificity score of 72.86%, and (c) Sensitivity score. These scores are very high indicating that the model has a moderate classification performance hence will be able to correctly classify several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 71.42% (accuracy), 71,38% for sensitivity, 71%, and 72.38%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true label for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 78.86%, (b) Specificity score of 73.73%, and (c) Sensitivity score = 78,73%. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 78.22%, (b) Specificity score of 78,73%, and (c) Sensitivity score. These scores across the different metrics suggest that the model has a moderately high classification performance hence will be able to correctly identify the true label for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (1) Accuracy score equal to 77.91%, (2) Specificity score of 74.81%, and (3) Sensitivity score. From the scores across the metrics, we can see that the model has a moderate classification performance hence will be able to correctly classify several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 84.67% (accuracy), 73.99% for recall, 66.21% accuracy, and 66%.21%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 79.17%, (b) Specificity score of 78.38%, and (c) Recall score = 78% with the F1score equal to 83.22%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: (a) Accuracy score equal to 72.44%, (b) Recall score of 79.45%. (c) Precision score: 55.24%. From the precision and recall scores, we can see that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is summarized as follows: (a) Accuracy score equal to 71.34% (b) Specificity score of 72.51% and (c) Sensitivity score = 71,44%. From these scores, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 72.39% (recall), 72,39%, 72 and 73.33%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true label for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 73.33% (accuracy), 73,45% for precision, 73%, 73%.28% accuracy, and 73 with 73 of 70.28%, respectively. The model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 66.33% (accuracy), 66,38% for recall, 66%, and 73.38%, respectively. The model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of misclassification error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 67.52% (accuracy), 71.83% for recall, 71 with 70.22% accuracy. The model has a moderate classification performance and will be able to correctly classify several test cases with a small margin of misclassification.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 54.11% (accuracy), 55.35% for recall, 54%.99% accuracy, and 54%, respectively. Based on the scores across the metrics, we can conclude that the model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases with a small margin of error.\",\n",
       " \"The model's performance on this binary classification problem where the test instances are classified as either #CA or #CB is: 50.07% (accuracy), 52.33% for recall, 53.71% accuracy, and 54.23% with the precision and recall scores equal to 52%. The model has a moderate classification performance hence will be able to correctly identify the true labels for several test cases/samples.\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputd[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-haiti",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('annotation': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd04aea246828f75a58a93204fce55d322b87a38415c2742fb8a88040418150f4d4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
