{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "comprehensive-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring ways to assess the quality of narrations\n",
    "# Simple scores proposed here ensures the metrics, values, class labels mentions are as accurate as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf81a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../TrainedNarrators/')\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beautiful-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))\n",
    "    # eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(\n",
    "        pc, identical_metrics=identicals))\n",
    "\n",
    "refs = [[t['narration'].lower() for t in test_sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adequate-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacrebleu.tokenizers import tokenizer_13a\n",
    "def normalize_text(s):\n",
    "    tokenize_fn = lambda x: tokenizer_13a.Tokenizer13a()(x)\n",
    "    return tokenize_fn(s.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "honey-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_rated/t5-small/narrations_outputoe.json','rb'))\n",
    "t5_small_r= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_raw/t5-small/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "molecular-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_base= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_rated/t5-base/narrations_outputoe.json','rb'))\n",
    "t5_base_r= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_raw/t5-base/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "material-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_large= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_rated/t5-large/narrations_outputoe.json','rb'))\n",
    "t5_large_r= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs_full_raw/t5-large/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc880bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t5_small= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs/t5-small/narrations_outputoe.json','rb'))\n",
    "t5_base= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs/t5-base/narrations_outputoe.json','rb'))\n",
    "t5_large= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs/t5-large/narrations_outputoe.json','rb'))\n",
    "\n",
    "bart_base= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs/bart-base/narrations_outputoe.json','rb'))\n",
    "bart_large= json.load(open('../TrainedNarrators/P-NarrationsModels/baselineoutputs/bart-large/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65624ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_small_ef= json.load(open('../TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/t5-small/narrations_outputoe.json','rb'))\n",
    "t5_base_ef= json.load(open('../TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/t5-base/narrations_outputoe.json','rb'))\n",
    "t5_large_ef= json.load(open('../TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/t5-large/narrations_outputoe.json','rb'))\n",
    "\n",
    "bart_base_ef= json.load(open('../TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/bart-base/narrations_outputoe.json','rb'))\n",
    "bart_large_ef= json.load(open('../TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/bart-large/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "single-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t5_hybrid= json.load(open('TrainedNarrators/P-NarrationsModels/hybridoutputs/narrations_outputoe.json','rb'))\n",
    "t5_early= json.load(open('TrainedNarrators/P-NarrationsModels/earlyfusionoutputs/narrations_outputoe.json','rb'))\n",
    "t5_late= json.load(open('TrainedNarrators/P-NarrationsModels/latefusionoutputs/narrations_outputoe.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "described-smart",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_base= json.load(open('TrainedNarrators/newBartOutputep.json','rb'))\n",
    "bart_early= json.load(open('TrainedNarrators/newBartOutputearlyfusion.json','rb'))\n",
    "bart_early1= json.load(open('TrainedNarrators/newBartOutputearlyfusion1.json','rb'))\n",
    "bart_early2= json.load(open('TrainedNarrators/newBartOutputearlyfusion2.json','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sunset-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the F1-score to determine the mentions of the metrics the narration is supposed to consider.\n",
    "# We can compute this using the overlap between the reference text or the input table\n",
    "# The accuracy of the narration will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "wanted-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluations import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olive-spider",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics = [[m.lower().strip() for m in test_sample[idx]['metrics']] for idx in range(len(test_sample))]\n",
    "\n",
    "# replace metrics such as sensitivity with recall\n",
    "cleans_table_refs =[]\n",
    "for r in copy.deepcopy( table_ref_metrics):\n",
    "    if 'sensitivity' in set(r):\n",
    "        idx = r.index('sensitivity')\n",
    "        r[idx] ='recall'\n",
    "    cleans_table_refs.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "raised-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_ref_metrics[0]\n",
    "ref_mentions = [extractMetricMentions(r,ref_mentions=t) for r,t in zip(refs[0],table_ref_metrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "usual-merit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['sensitivity', 'auc', 'accuracy', 'precision', 'f1score'],\n",
       " ['precision', 'auc', 'accuracy', 'sensitivity', 'f1score'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref_metrics[1],ref_mentions[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "extensive-margin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.98095, 'recall': 0.98329, 'precision': 0.97862}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricMentionScore(table_ref_metrics,refs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acting-manual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.91082, 'recall': 0.90974, 'precision': 0.9119}\n",
      "2 :  {'f1_score': 0.88269, 'recall': 0.90261, 'precision': 0.86364}\n",
      "3 :  {'f1_score': 0.88837, 'recall': 0.89786, 'precision': 0.87907}\n",
      "4 :  {'f1_score': 0.90178, 'recall': 0.90499, 'precision': 0.89858}\n",
      "5 :  {'f1_score': 0.89941, 'recall': 0.90261, 'precision': 0.89623}\n",
      "6 :  {'f1_score': 0.89123, 'recall': 0.90499, 'precision': 0.87788}\n",
      "7 :  {'f1_score': 0.88837, 'recall': 0.89786, 'precision': 0.87907}\n",
      "8 :  {'f1_score': 0.88706, 'recall': 0.89549, 'precision': 0.87879}\n",
      "9 :  {'f1_score': 0.88602, 'recall': 0.89549, 'precision': 0.87674}\n",
      "10 :  {'f1_score': 0.88389, 'recall': 0.88599, 'precision': 0.8818}\n",
      "with ratings\n",
      "1 :  {'f1_score': 0.88647, 'recall': 0.87173, 'precision': 0.90172}\n",
      "2 :  {'f1_score': 0.88544, 'recall': 0.88124, 'precision': 0.88969}\n",
      "3 :  {'f1_score': 0.86987, 'recall': 0.88124, 'precision': 0.8588}\n",
      "4 :  {'f1_score': 0.88439, 'recall': 0.88124, 'precision': 0.88756}\n",
      "5 :  {'f1_score': 0.86467, 'recall': 0.85748, 'precision': 0.87198}\n",
      "6 :  {'f1_score': 0.86756, 'recall': 0.84798, 'precision': 0.88806}\n",
      "7 :  {'f1_score': 0.87019, 'recall': 0.85986, 'precision': 0.88078}\n",
      "8 :  {'f1_score': 0.87317, 'recall': 0.85036, 'precision': 0.89724}\n",
      "9 :  {'f1_score': 0.86207, 'recall': 0.83135, 'precision': 0.89514}\n",
      "10 :  {'f1_score': 0.86797, 'recall': 0.84323, 'precision': 0.89421}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_small_r.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('with ratings')\n",
    "for k,v in t5_small.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "radical-status",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.89387, 'recall': 0.90024, 'precision': 0.88759}\n",
      "2 :  {'f1_score': 0.91208, 'recall': 0.92399, 'precision': 0.90046}\n",
      "3 :  {'f1_score': 0.90995, 'recall': 0.91211, 'precision': 0.9078}\n",
      "4 :  {'f1_score': 0.92723, 'recall': 0.93824, 'precision': 0.91647}\n",
      "5 :  {'f1_score': 0.92019, 'recall': 0.93112, 'precision': 0.90951}\n",
      "6 :  {'f1_score': 0.92815, 'recall': 0.93587, 'precision': 0.92056}\n",
      "7 :  {'f1_score': 0.9258, 'recall': 0.93349, 'precision': 0.91822}\n",
      "8 :  {'f1_score': 0.92798, 'recall': 0.93349, 'precision': 0.92254}\n",
      "9 :  {'f1_score': 0.92093, 'recall': 0.94062, 'precision': 0.90205}\n",
      "10 :  {'f1_score': 0.92399, 'recall': 0.92399, 'precision': 0.92399}\n",
      "with ratings\n",
      "1 :  {'f1_score': 0.87963, 'recall': 0.90261, 'precision': 0.85779}\n",
      "2 :  {'f1_score': 0.89908, 'recall': 0.93112, 'precision': 0.86918}\n",
      "3 :  {'f1_score': 0.89298, 'recall': 0.92162, 'precision': 0.86607}\n",
      "4 :  {'f1_score': 0.89631, 'recall': 0.92399, 'precision': 0.87025}\n",
      "5 :  {'f1_score': 0.88631, 'recall': 0.90736, 'precision': 0.86621}\n",
      "6 :  {'f1_score': 0.91036, 'recall': 0.92874, 'precision': 0.89269}\n",
      "7 :  {'f1_score': 0.91015, 'recall': 0.92637, 'precision': 0.8945}\n",
      "8 :  {'f1_score': 0.91482, 'recall': 0.93112, 'precision': 0.89908}\n",
      "9 :  {'f1_score': 0.91482, 'recall': 0.93112, 'precision': 0.89908}\n",
      "10 :  {'f1_score': 0.91315, 'recall': 0.92399, 'precision': 0.90255}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_base_r.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('with ratings')\n",
    "for k,v in t5_base.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "anticipated-bidding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.89829, 'recall': 0.93349, 'precision': 0.86564}\n",
      "2 :  {'f1_score': 0.91204, 'recall': 0.93587, 'precision': 0.88939}\n",
      "3 :  {'f1_score': 0.92, 'recall': 0.92874, 'precision': 0.91142}\n",
      "4 :  {'f1_score': 0.91776, 'recall': 0.91449, 'precision': 0.92105}\n",
      "5 :  {'f1_score': 0.92326, 'recall': 0.91449, 'precision': 0.9322}\n",
      "6 :  {'f1_score': 0.93269, 'recall': 0.92162, 'precision': 0.94404}\n",
      "7 :  {'f1_score': 0.92271, 'recall': 0.92162, 'precision': 0.92381}\n",
      "8 :  {'f1_score': 0.92105, 'recall': 0.91449, 'precision': 0.92771}\n",
      "9 :  {'f1_score': 0.91847, 'recall': 0.90974, 'precision': 0.92736}\n",
      "10 :  {'f1_score': 0.93029, 'recall': 0.91924, 'precision': 0.94161}\n",
      "with ratings\n",
      "1 :  {'f1_score': 0.90433, 'recall': 0.94299, 'precision': 0.86871}\n",
      "2 :  {'f1_score': 0.90323, 'recall': 0.93112, 'precision': 0.87696}\n",
      "3 :  {'f1_score': 0.9193, 'recall': 0.93349, 'precision': 0.90553}\n",
      "4 :  {'f1_score': 0.90973, 'recall': 0.92162, 'precision': 0.89815}\n",
      "5 :  {'f1_score': 0.91962, 'recall': 0.92399, 'precision': 0.91529}\n",
      "6 :  {'f1_score': 0.92747, 'recall': 0.92637, 'precision': 0.92857}\n",
      "7 :  {'f1_score': 0.92695, 'recall': 0.91924, 'precision': 0.93478}\n",
      "8 :  {'f1_score': 0.92754, 'recall': 0.91211, 'precision': 0.94349}\n",
      "9 :  {'f1_score': 0.93157, 'recall': 0.92162, 'precision': 0.94175}\n",
      "10 :  {'f1_score': 0.93333, 'recall': 0.91449, 'precision': 0.95297}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_large_r.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('with ratings')\n",
    "for k,v in t5_large.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-sleeve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sunset-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.86717, 'recall': 0.85644, 'precision': 0.87817}\n",
      "2 :  {'f1_score': 0.89756, 'recall': 0.91089, 'precision': 0.88462}\n",
      "3 :  {'f1_score': 0.89394, 'recall': 0.87624, 'precision': 0.91237}\n",
      "4 :  {'f1_score': 0.87841, 'recall': 0.87624, 'precision': 0.8806}\n",
      "5 :  {'f1_score': 0.8995, 'recall': 0.88614, 'precision': 0.91327}\n",
      "6 :  {'f1_score': 0.88041, 'recall': 0.85644, 'precision': 0.90576}\n",
      "7 :  {'f1_score': 0.86957, 'recall': 0.84158, 'precision': 0.89947}\n",
      "8 :  {'f1_score': 0.86822, 'recall': 0.83168, 'precision': 0.90811}\n",
      "9 :  {'f1_score': 0.86305, 'recall': 0.82673, 'precision': 0.9027}\n",
      "10 :  {'f1_score': 0.86822, 'recall': 0.83168, 'precision': 0.90811}\n",
      "EarlyFusion\n",
      "1 :  {'f1_score': 0.81481, 'recall': 0.76238, 'precision': 0.875}\n",
      "2 :  {'f1_score': 0.85496, 'recall': 0.83168, 'precision': 0.87958}\n",
      "3 :  {'f1_score': 0.8601, 'recall': 0.82178, 'precision': 0.90217}\n",
      "4 :  {'f1_score': 0.86822, 'recall': 0.83168, 'precision': 0.90811}\n",
      "5 :  {'f1_score': 0.87855, 'recall': 0.84158, 'precision': 0.91892}\n",
      "6 :  {'f1_score': 0.87047, 'recall': 0.83168, 'precision': 0.91304}\n",
      "7 :  {'f1_score': 0.87629, 'recall': 0.84158, 'precision': 0.91398}\n",
      "8 :  {'f1_score': 0.88372, 'recall': 0.84653, 'precision': 0.92432}\n",
      "9 :  {'f1_score': 0.86822, 'recall': 0.83168, 'precision': 0.90811}\n",
      "10 :  {'f1_score': 0.87565, 'recall': 0.83663, 'precision': 0.91848}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_small.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('EarlyFusion')\n",
    "for k,v in t5_small_ef.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "celtic-norway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.87089, 'recall': 0.85149, 'precision': 0.89119}\n",
      "2 :  {'f1_score': 0.85926, 'recall': 0.86139, 'precision': 0.85714}\n",
      "3 :  {'f1_score': 0.88119, 'recall': 0.88119, 'precision': 0.88119}\n",
      "4 :  {'f1_score': 0.87407, 'recall': 0.87624, 'precision': 0.87192}\n",
      "5 :  {'f1_score': 0.86139, 'recall': 0.86139, 'precision': 0.86139}\n",
      "6 :  {'f1_score': 0.85075, 'recall': 0.84653, 'precision': 0.855}\n",
      "7 :  {'f1_score': 0.86207, 'recall': 0.86634, 'precision': 0.85784}\n",
      "8 :  {'f1_score': 0.86284, 'recall': 0.85644, 'precision': 0.86935}\n",
      "9 :  {'f1_score': 0.86567, 'recall': 0.86139, 'precision': 0.87}\n",
      "10 :  {'f1_score': 0.86352, 'recall': 0.86139, 'precision': 0.86567}\n",
      "EarlyFusion\n",
      "1 :  {'f1_score': 0.86914, 'recall': 0.87129, 'precision': 0.867}\n",
      "2 :  {'f1_score': 0.90307, 'recall': 0.94554, 'precision': 0.86425}\n",
      "3 :  {'f1_score': 0.92754, 'recall': 0.9505, 'precision': 0.90566}\n",
      "4 :  {'f1_score': 0.92978, 'recall': 0.9505, 'precision': 0.90995}\n",
      "5 :  {'f1_score': 0.93689, 'recall': 0.95545, 'precision': 0.91905}\n",
      "6 :  {'f1_score': 0.93888, 'recall': 0.9505, 'precision': 0.92754}\n",
      "7 :  {'f1_score': 0.93171, 'recall': 0.94554, 'precision': 0.91827}\n",
      "8 :  {'f1_score': 0.93947, 'recall': 0.9604, 'precision': 0.91943}\n",
      "9 :  {'f1_score': 0.95332, 'recall': 0.9604, 'precision': 0.94634}\n",
      "10 :  {'f1_score': 0.94059, 'recall': 0.94059, 'precision': 0.94059}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_base.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('EarlyFusion')\n",
    "for k,v in t5_base_ef.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "verbal-chancellor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.87204, 'recall': 0.91089, 'precision': 0.83636}\n",
      "2 :  {'f1_score': 0.91584, 'recall': 0.91584, 'precision': 0.91584}\n",
      "3 :  {'f1_score': 0.91228, 'recall': 0.90099, 'precision': 0.92386}\n",
      "4 :  {'f1_score': 0.90819, 'recall': 0.90594, 'precision': 0.91045}\n",
      "5 :  {'f1_score': 0.92, 'recall': 0.91089, 'precision': 0.92929}\n",
      "6 :  {'f1_score': 0.90777, 'recall': 0.92574, 'precision': 0.89048}\n",
      "7 :  {'f1_score': 0.91811, 'recall': 0.91584, 'precision': 0.9204}\n",
      "8 :  {'f1_score': 0.92157, 'recall': 0.93069, 'precision': 0.91262}\n",
      "9 :  {'f1_score': 0.92346, 'recall': 0.92574, 'precision': 0.92118}\n",
      "10 :  {'f1_score': 0.92308, 'recall': 0.92079, 'precision': 0.92537}\n",
      "EarlyFusion\n",
      "1 :  {'f1_score': 0.89157, 'recall': 0.91584, 'precision': 0.86854}\n",
      "2 :  {'f1_score': 0.90777, 'recall': 0.92574, 'precision': 0.89048}\n",
      "3 :  {'f1_score': 0.90291, 'recall': 0.92079, 'precision': 0.88571}\n",
      "4 :  {'f1_score': 0.92118, 'recall': 0.92574, 'precision': 0.91667}\n",
      "5 :  {'f1_score': 0.9204, 'recall': 0.91584, 'precision': 0.925}\n",
      "6 :  {'f1_score': 0.91176, 'recall': 0.92079, 'precision': 0.90291}\n",
      "7 :  {'f1_score': 0.9284, 'recall': 0.93069, 'precision': 0.92611}\n",
      "8 :  {'f1_score': 0.933, 'recall': 0.93069, 'precision': 0.93532}\n",
      "9 :  {'f1_score': 0.93069, 'recall': 0.93069, 'precision': 0.93069}\n",
      "10 :  {'f1_score': 0.93627, 'recall': 0.94554, 'precision': 0.92718}\n"
     ]
    }
   ],
   "source": [
    "for k,v in t5_large.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('EarlyFusion')\n",
    "for k,v in t5_large_ef.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "variable-colombia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.89311, 'recall': 0.93069, 'precision': 0.85845}\n",
      "2 :  {'f1_score': 0.9291, 'recall': 0.94059, 'precision': 0.91787}\n",
      "3 :  {'f1_score': 0.92647, 'recall': 0.93564, 'precision': 0.91748}\n",
      "4 :  {'f1_score': 0.95074, 'recall': 0.95545, 'precision': 0.94608}\n",
      "5 :  {'f1_score': 0.9403, 'recall': 0.93564, 'precision': 0.945}\n",
      "6 :  {'f1_score': 0.94527, 'recall': 0.94059, 'precision': 0.95}\n",
      "7 :  {'f1_score': 0.94527, 'recall': 0.94059, 'precision': 0.95}\n",
      "8 :  {'f1_score': 0.94554, 'recall': 0.94554, 'precision': 0.94554}\n",
      "9 :  {'f1_score': 0.94608, 'recall': 0.95545, 'precision': 0.93689}\n",
      "10 :  {'f1_score': 0.94608, 'recall': 0.95545, 'precision': 0.93689}\n",
      "EarlyFusion\n",
      "1 :  {'f1_score': 0.90821, 'recall': 0.93069, 'precision': 0.88679}\n",
      "2 :  {'f1_score': 0.91176, 'recall': 0.92079, 'precision': 0.90291}\n",
      "3 :  {'f1_score': 0.92308, 'recall': 0.92079, 'precision': 0.92537}\n",
      "4 :  {'f1_score': 0.92346, 'recall': 0.92574, 'precision': 0.92118}\n",
      "5 :  {'f1_score': 0.92611, 'recall': 0.93069, 'precision': 0.92157}\n",
      "6 :  {'f1_score': 0.93137, 'recall': 0.94059, 'precision': 0.92233}\n",
      "7 :  {'f1_score': 0.92383, 'recall': 0.93069, 'precision': 0.91707}\n",
      "8 :  {'f1_score': 0.92611, 'recall': 0.93069, 'precision': 0.92157}\n",
      "9 :  {'f1_score': 0.9291, 'recall': 0.94059, 'precision': 0.91787}\n",
      "10 :  {'f1_score': 0.92383, 'recall': 0.93069, 'precision': 0.91707}\n"
     ]
    }
   ],
   "source": [
    "for k,v in bart_base.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('EarlyFusion')\n",
    "for k,v in bart_base_ef.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "regular-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 :  {'f1_score': 0.85514, 'recall': 0.90594, 'precision': 0.80973}\n",
      "2 :  {'f1_score': 0.82443, 'recall': 0.80198, 'precision': 0.84817}\n",
      "3 :  {'f1_score': 0.84021, 'recall': 0.80693, 'precision': 0.87634}\n",
      "4 :  {'f1_score': 0.84536, 'recall': 0.81188, 'precision': 0.88172}\n",
      "5 :  {'f1_score': 0.84755, 'recall': 0.81188, 'precision': 0.88649}\n",
      "6 :  {'f1_score': 0.85938, 'recall': 0.81683, 'precision': 0.90659}\n",
      "7 :  {'f1_score': 0.87047, 'recall': 0.83168, 'precision': 0.91304}\n",
      "8 :  {'f1_score': 0.86957, 'recall': 0.84158, 'precision': 0.89947}\n",
      "9 :  {'f1_score': 0.87245, 'recall': 0.84653, 'precision': 0.9}\n",
      "10 :  {'f1_score': 0.90226, 'recall': 0.89109, 'precision': 0.91371}\n",
      "EarlyFusion\n",
      "1 :  {'f1_score': 0.82005, 'recall': 0.89109, 'precision': 0.75949}\n",
      "2 :  {'f1_score': 0.88293, 'recall': 0.89604, 'precision': 0.87019}\n",
      "3 :  {'f1_score': 0.88452, 'recall': 0.89109, 'precision': 0.87805}\n",
      "4 :  {'f1_score': 0.90819, 'recall': 0.90594, 'precision': 0.91045}\n",
      "5 :  {'f1_score': 0.90819, 'recall': 0.90594, 'precision': 0.91045}\n",
      "6 :  {'f1_score': 0.88778, 'recall': 0.88119, 'precision': 0.89447}\n",
      "7 :  {'f1_score': 0.89055, 'recall': 0.88614, 'precision': 0.895}\n",
      "8 :  {'f1_score': 0.88119, 'recall': 0.88119, 'precision': 0.88119}\n",
      "9 :  {'f1_score': 0.88395, 'recall': 0.88614, 'precision': 0.88177}\n",
      "10 :  {'f1_score': 0.88557, 'recall': 0.88119, 'precision': 0.89}\n"
     ]
    }
   ],
   "source": [
    "for k,v in bart_large.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))\n",
    "print('EarlyFusion')\n",
    "for k,v in bart_large_ef.items():\n",
    "    print(k,': ',metricMentionScore(ref_mentions,v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-welcome",
   "metadata": {},
   "source": [
    "# Fluency Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "intense-minneapolis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint ../../../bleurt/BLEURT-20/.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: ../../../bleurt/BLEURT-20/sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "from bleurt import score\n",
    "scorer = score.LengthBatchingBleurtScorer('../../../bleurt/BLEURT-20/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "invalid-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSentences(file,lower=False):\n",
    "    with open(file,'r', encoding=\"utf-8\") as o_file:\n",
    "        sentennces = []\n",
    "        for s in o_file.readlines():\n",
    "            ss = s.strip() #.lower() if  lower else s.strip()\n",
    "            sentennces.append(ss)\n",
    "    return sentennces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "intellectual-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "atrefs = [[t['narration'] for t in test_sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-livestock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adolescent-strike",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBLEURT(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [t for t in data[k]]\n",
    "        score=np.mean(scorer.score(references=atrefs[0], candidates=gen))\n",
    "        results[k] = score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "pleasant-house",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 220.75\n",
      "INFO:tensorflow:Average batch sequence length: 215.5\n",
      "INFO:tensorflow:Average batch sequence length: 203.5\n",
      "INFO:tensorflow:Average batch sequence length: 202.0\n",
      "INFO:tensorflow:Average batch sequence length: 198.5\n",
      "INFO:tensorflow:Average batch sequence length: 199.75\n",
      "INFO:tensorflow:Average batch sequence length: 201.0\n",
      "INFO:tensorflow:Average batch sequence length: 200.0\n",
      "INFO:tensorflow:Average batch sequence length: 202.5\n",
      "INFO:tensorflow:Average batch sequence length: 205.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5445767796039581,\n",
       " '2': 0.5629921448230744,\n",
       " '3': 0.5606406557559968,\n",
       " '4': 0.5611169564723969,\n",
       " '5': 0.5627802777290344,\n",
       " '6': 0.5601527905464172,\n",
       " '7': 0.5593305480480194,\n",
       " '8': 0.561673846244812,\n",
       " '9': 0.5653157460689545,\n",
       " '10': 0.5695234930515289}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "pointed-native",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 205.5\n",
      "INFO:tensorflow:Average batch sequence length: 206.75\n",
      "INFO:tensorflow:Average batch sequence length: 211.5\n",
      "INFO:tensorflow:Average batch sequence length: 206.25\n",
      "INFO:tensorflow:Average batch sequence length: 203.25\n",
      "INFO:tensorflow:Average batch sequence length: 198.0\n",
      "INFO:tensorflow:Average batch sequence length: 202.5\n",
      "INFO:tensorflow:Average batch sequence length: 198.25\n",
      "INFO:tensorflow:Average batch sequence length: 200.5\n",
      "INFO:tensorflow:Average batch sequence length: 201.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.541463041305542,\n",
       " '2': 0.5734943425655366,\n",
       " '3': 0.5702847051620483,\n",
       " '4': 0.5743310308456421,\n",
       " '5': 0.576119726896286,\n",
       " '6': 0.5749999856948853,\n",
       " '7': 0.5782185077667237,\n",
       " '8': 0.5779695892333985,\n",
       " '9': 0.582180209159851,\n",
       " '10': 0.5777841699123383}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_small_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "million-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 219.25\n",
      "INFO:tensorflow:Average batch sequence length: 226.0\n",
      "INFO:tensorflow:Average batch sequence length: 231.25\n",
      "INFO:tensorflow:Average batch sequence length: 235.5\n",
      "INFO:tensorflow:Average batch sequence length: 240.0\n",
      "INFO:tensorflow:Average batch sequence length: 229.25\n",
      "INFO:tensorflow:Average batch sequence length: 232.25\n",
      "INFO:tensorflow:Average batch sequence length: 238.25\n",
      "INFO:tensorflow:Average batch sequence length: 235.25\n",
      "INFO:tensorflow:Average batch sequence length: 237.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.546378356218338,\n",
       " '2': 0.5514997017383575,\n",
       " '3': 0.5559064424037934,\n",
       " '4': 0.553865282535553,\n",
       " '5': 0.5482598745822906,\n",
       " '6': 0.545771313905716,\n",
       " '7': 0.5474131631851197,\n",
       " '8': 0.549100284576416,\n",
       " '9': 0.5475997519493103,\n",
       " '10': 0.5504473543167114}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1baed0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 228.0\n",
      "INFO:tensorflow:Average batch sequence length: 241.75\n",
      "INFO:tensorflow:Average batch sequence length: 239.5\n",
      "INFO:tensorflow:Average batch sequence length: 244.0\n",
      "INFO:tensorflow:Average batch sequence length: 239.75\n",
      "INFO:tensorflow:Average batch sequence length: 243.75\n",
      "INFO:tensorflow:Average batch sequence length: 242.25\n",
      "INFO:tensorflow:Average batch sequence length: 242.0\n",
      "INFO:tensorflow:Average batch sequence length: 240.75\n",
      "INFO:tensorflow:Average batch sequence length: 240.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5339756417274475,\n",
       " '2': 0.55786994099617,\n",
       " '3': 0.5646085011959076,\n",
       " '4': 0.5526654696464539,\n",
       " '5': 0.5643269348144532,\n",
       " '6': 0.5609292387962341,\n",
       " '7': 0.5622669398784638,\n",
       " '8': 0.5642463457584381,\n",
       " '9': 0.5618602621555329,\n",
       " '10': 0.562724142074585}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "thermal-bachelor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 231.5\n",
      "INFO:tensorflow:Average batch sequence length: 241.0\n",
      "INFO:tensorflow:Average batch sequence length: 230.75\n",
      "INFO:tensorflow:Average batch sequence length: 245.5\n",
      "INFO:tensorflow:Average batch sequence length: 232.25\n",
      "INFO:tensorflow:Average batch sequence length: 239.75\n",
      "INFO:tensorflow:Average batch sequence length: 236.25\n",
      "INFO:tensorflow:Average batch sequence length: 233.75\n",
      "INFO:tensorflow:Average batch sequence length: 236.25\n",
      "INFO:tensorflow:Average batch sequence length: 236.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5258877038955688,\n",
       " '2': 0.5385336983203888,\n",
       " '3': 0.5452136254310608,\n",
       " '4': 0.5457654047012329,\n",
       " '5': 0.5464544272422791,\n",
       " '6': 0.5446231496334076,\n",
       " '7': 0.5453492212295532,\n",
       " '8': 0.5495243310928345,\n",
       " '9': 0.5535838210582733,\n",
       " '10': 0.5508652198314666}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91c0a0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 237.5\n",
      "INFO:tensorflow:Average batch sequence length: 231.25\n",
      "INFO:tensorflow:Average batch sequence length: 242.0\n",
      "INFO:tensorflow:Average batch sequence length: 239.0\n",
      "INFO:tensorflow:Average batch sequence length: 237.75\n",
      "INFO:tensorflow:Average batch sequence length: 238.75\n",
      "INFO:tensorflow:Average batch sequence length: 234.5\n",
      "INFO:tensorflow:Average batch sequence length: 235.75\n",
      "INFO:tensorflow:Average batch sequence length: 240.75\n",
      "INFO:tensorflow:Average batch sequence length: 235.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5222187256813049,\n",
       " '2': 0.5435921251773834,\n",
       " '3': 0.5299697399139405,\n",
       " '4': 0.5488362085819244,\n",
       " '5': 0.5465029394626617,\n",
       " '6': 0.5542054498195648,\n",
       " '7': 0.5581784117221832,\n",
       " '8': 0.5613947749137879,\n",
       " '9': 0.5612260258197784,\n",
       " '10': 0.5608684837818145}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(t5_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1144e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 284.25\n",
      "INFO:tensorflow:Average batch sequence length: 235.25\n",
      "INFO:tensorflow:Average batch sequence length: 242.0\n",
      "INFO:tensorflow:Average batch sequence length: 238.0\n",
      "INFO:tensorflow:Average batch sequence length: 240.5\n",
      "INFO:tensorflow:Average batch sequence length: 244.5\n",
      "INFO:tensorflow:Average batch sequence length: 241.5\n",
      "INFO:tensorflow:Average batch sequence length: 240.75\n",
      "INFO:tensorflow:Average batch sequence length: 242.75\n",
      "INFO:tensorflow:Average batch sequence length: 240.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5195593202114105,\n",
       " '2': 0.5622959136962891,\n",
       " '3': 0.5580628538131713,\n",
       " '4': 0.5648966467380524,\n",
       " '5': 0.5627898490428924,\n",
       " '6': 0.5623090243339539,\n",
       " '7': 0.5682191812992096,\n",
       " '8': 0.5644427525997162,\n",
       " '9': 0.5682531297206879,\n",
       " '10': 0.5649414205551148}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(bart_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f6df2520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 261.5\n",
      "INFO:tensorflow:Average batch sequence length: 236.0\n",
      "INFO:tensorflow:Average batch sequence length: 230.5\n",
      "INFO:tensorflow:Average batch sequence length: 234.0\n",
      "INFO:tensorflow:Average batch sequence length: 234.0\n",
      "INFO:tensorflow:Average batch sequence length: 233.5\n",
      "INFO:tensorflow:Average batch sequence length: 235.75\n",
      "INFO:tensorflow:Average batch sequence length: 236.75\n",
      "INFO:tensorflow:Average batch sequence length: 233.25\n",
      "INFO:tensorflow:Average batch sequence length: 236.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.5343435144424439,\n",
       " '2': 0.5499734723567963,\n",
       " '3': 0.5564940476417541,\n",
       " '4': 0.5548633146286011,\n",
       " '5': 0.5591939365863801,\n",
       " '6': 0.5573651611804962,\n",
       " '7': 0.561020656824112,\n",
       " '8': 0.5584145081043244,\n",
       " '9': 0.5619839262962342,\n",
       " '10': 0.5576010155677795}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(bart_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2d5e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 296.25\n",
      "INFO:tensorflow:Average batch sequence length: 225.0\n",
      "INFO:tensorflow:Average batch sequence length: 224.25\n",
      "INFO:tensorflow:Average batch sequence length: 233.25\n",
      "INFO:tensorflow:Average batch sequence length: 229.5\n",
      "INFO:tensorflow:Average batch sequence length: 238.5\n",
      "INFO:tensorflow:Average batch sequence length: 234.75\n",
      "INFO:tensorflow:Average batch sequence length: 235.75\n",
      "INFO:tensorflow:Average batch sequence length: 240.75\n",
      "INFO:tensorflow:Average batch sequence length: 243.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.4916332226991653,\n",
       " '2': 0.46589902639389036,\n",
       " '3': 0.45713416278362273,\n",
       " '4': 0.4753798049688339,\n",
       " '5': 0.48053023815155027,\n",
       " '6': 0.4844422906637192,\n",
       " '7': 0.4919989365339279,\n",
       " '8': 0.49374524295330047,\n",
       " '9': 0.49732184648513794,\n",
       " '10': 0.4997748613357544}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(bart_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1db3c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Average batch sequence length: 305.0\n",
      "INFO:tensorflow:Average batch sequence length: 241.25\n",
      "INFO:tensorflow:Average batch sequence length: 249.5\n",
      "INFO:tensorflow:Average batch sequence length: 239.5\n",
      "INFO:tensorflow:Average batch sequence length: 238.0\n",
      "INFO:tensorflow:Average batch sequence length: 239.0\n",
      "INFO:tensorflow:Average batch sequence length: 240.75\n",
      "INFO:tensorflow:Average batch sequence length: 241.75\n",
      "INFO:tensorflow:Average batch sequence length: 241.25\n",
      "INFO:tensorflow:Average batch sequence length: 239.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1': 0.47189278244972227,\n",
       " '2': 0.531566492319107,\n",
       " '3': 0.5320205760002136,\n",
       " '4': 0.5297759389877319,\n",
       " '5': 0.5319879293441773,\n",
       " '6': 0.5300736558437348,\n",
       " '7': 0.5284016096591949,\n",
       " '8': 0.5287890183925629,\n",
       " '9': 0.5284844470024109,\n",
       " '10': 0.5251536858081818}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBLEURT(bart_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "italian-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets  import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "appropriate-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/essel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "metric_bert = load_metric('bertscore')\n",
    "metric_sbleu = load_metric('sacrebleu')\n",
    "metric_meteor = load_metric('meteor')\n",
    "#metric_bleurt = load_metric('bleurt', 'bleurt-large-512')\n",
    "metric_rouge = load_metric('rouge')\n",
    "metric_wer = load_metric('google_bleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "spatial-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "orf = [[normalize_text(r) for r in refs[0]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "starting-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBleu(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_sbleu.compute(predictions=[gen],references=[orf])\n",
    "        results[k] = score['score']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "entertaining-martin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 38.27345294153349,\n",
       "  '2': 45.534023036346326,\n",
       "  '3': 44.26559236879197,\n",
       "  '4': 44.206592043430135,\n",
       "  '5': 43.66661753408078,\n",
       "  '6': 42.547709396266264,\n",
       "  '7': 41.81934821490277,\n",
       "  '8': 41.866051058841144,\n",
       "  '9': 41.608994724883445,\n",
       "  '10': 40.66380072998268},\n",
       " {'1': 37.448451168322144,\n",
       "  '2': 41.91481246564572,\n",
       "  '3': 41.305162065646684,\n",
       "  '4': 41.551905946860536,\n",
       "  '5': 41.5251801544433,\n",
       "  '6': 41.422376527152906,\n",
       "  '7': 41.357048658652886,\n",
       "  '8': 40.75400994914726,\n",
       "  '9': 39.07300750363442,\n",
       "  '10': 38.966814682082045})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_small),computeBleu(t5_small_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fresh-attack",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 42.45077198935736,\n",
       "  '2': 46.62546545522973,\n",
       "  '3': 46.65390763132382,\n",
       "  '4': 46.93863472987938,\n",
       "  '5': 46.10454790056323,\n",
       "  '6': 44.820374194293244,\n",
       "  '7': 44.59037159683151,\n",
       "  '8': 44.58766325792447,\n",
       "  '9': 44.54564775941757,\n",
       "  '10': 44.77200073712461},\n",
       " {'1': 45.45684073972856,\n",
       "  '2': 48.42942514299975,\n",
       "  '3': 47.90074058246407,\n",
       "  '4': 46.807961415544284,\n",
       "  '5': 46.738116253619964,\n",
       "  '6': 46.903486857783754,\n",
       "  '7': 46.244636641731944,\n",
       "  '8': 46.51730881662203,\n",
       "  '9': 46.09693212277259,\n",
       "  '10': 46.85466827275238})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_base),computeBleu(t5_base_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "lesser-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 46.62238998195075,\n",
       "  '2': 49.448456338574545,\n",
       "  '3': 47.67152724091295,\n",
       "  '4': 49.52237502281744,\n",
       "  '5': 48.25900275625839,\n",
       "  '6': 47.911167696372594,\n",
       "  '7': 47.81916304112054,\n",
       "  '8': 48.11077463385348,\n",
       "  '9': 47.945896076028944,\n",
       "  '10': 47.46334697266313},\n",
       " {'1': 45.617909249556995,\n",
       "  '2': 48.58355922319422,\n",
       "  '3': 47.83446515881546,\n",
       "  '4': 47.029011399498266,\n",
       "  '5': 48.121726000324685,\n",
       "  '6': 47.45348834499562,\n",
       "  '7': 46.84629737925212,\n",
       "  '8': 47.070302531058154,\n",
       "  '9': 46.676984996350654,\n",
       "  '10': 46.47226140915426})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_large),computeBleu(t5_large_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-percentage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-circus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "unnecessary-swimming",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 24.158348771379284,\n",
       " '2': 39.040559350802674,\n",
       " '3': 37.53155164977854,\n",
       " '4': 37.91663220224658,\n",
       " '5': 35.80349558007994,\n",
       " '6': 36.86505310076973,\n",
       " '7': 36.4178033441204,\n",
       " '8': 36.770158313762046,\n",
       " '9': 35.28767851656243,\n",
       " '10': 35.53096890889824}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(bart_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fossil-fancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 26.564670646523137,\n",
       " '2': 38.23977409150721,\n",
       " '3': 38.66599869402021,\n",
       " '4': 39.16953037415189,\n",
       " '5': 38.68522066071208,\n",
       " '6': 38.073129062046235,\n",
       " '7': 37.7517381097432,\n",
       " '8': 37.47160072983503,\n",
       " '9': 37.125194109075466,\n",
       " '10': 36.68434828598114}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(bart_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "billion-bailey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 21.900665878276378,\n",
       " '2': 41.1334085722937,\n",
       " '3': 39.862152668101764,\n",
       " '4': 40.37100773091837,\n",
       " '5': 39.10382129234722,\n",
       " '6': 38.45930352670108,\n",
       " '7': 38.33269706228829,\n",
       " '8': 38.51184666344686,\n",
       " '9': 37.515734347630755,\n",
       " '10': 37.566088729079816}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(bart_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4dd8db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 14.40625571184708,\n",
       " '2': 23.38901938741906,\n",
       " '3': 23.319668648295654,\n",
       " '4': 23.961501465030807,\n",
       " '5': 27.492777326568394,\n",
       " '6': 27.574820028002282,\n",
       " '7': 27.046222495982445,\n",
       " '8': 26.73471210817571,\n",
       " '9': 26.549893639115275,\n",
       " '10': 27.32416521439176}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(bart_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "artificial-parallel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 30.200654269771494,\n",
       "  '2': 30.21411268719779,\n",
       "  '3': 27.094601613754218,\n",
       "  '4': 25.820486711676352,\n",
       "  '5': 24.727385120237635,\n",
       "  '6': 25.74879840626904,\n",
       "  '7': 22.46401773949923,\n",
       "  '8': 22.303238694833748,\n",
       "  '9': 22.07867940004412,\n",
       "  '10': 23.554795058591242},\n",
       " {'1': 25.149858943669187,\n",
       "  '2': 29.17998772329856,\n",
       "  '3': 33.287515511825674,\n",
       "  '4': 29.49742693680722,\n",
       "  '5': 29.610714795504403,\n",
       "  '6': 27.94885850814741,\n",
       "  '7': 28.267555067237986,\n",
       "  '8': 25.93515993364522,\n",
       "  '9': 26.976690773858767,\n",
       "  '10': 26.8069503160034})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_small),computeBleu(t5_small_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fabulous-moisture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 35.67367510406442,\n",
       "  '2': 38.57377485550358,\n",
       "  '3': 38.58997510772958,\n",
       "  '4': 37.00668419930309,\n",
       "  '5': 36.357219098653836,\n",
       "  '6': 35.46797388636084,\n",
       "  '7': 35.12276353302408,\n",
       "  '8': 35.18608303549143,\n",
       "  '9': 34.60233357915515,\n",
       "  '10': 33.35313539050177},\n",
       " {'1': 34.051997834997465,\n",
       "  '2': 37.55960817475411,\n",
       "  '3': 37.677763627111716,\n",
       "  '4': 37.77344091102631,\n",
       "  '5': 37.24337549592579,\n",
       "  '6': 37.485819117483295,\n",
       "  '7': 37.17677840505928,\n",
       "  '8': 37.68101865417388,\n",
       "  '9': 36.97616352269113,\n",
       "  '10': 37.93515651243025})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_base),computeBleu(t5_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "worthy-sydney",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 39.40214392452215,\n",
       "  '2': 39.59040812432521,\n",
       "  '3': 38.373172510388365,\n",
       "  '4': 38.06656052934417,\n",
       "  '5': 39.48776056965934,\n",
       "  '6': 38.727358331966,\n",
       "  '7': 36.557642890806505,\n",
       "  '8': 38.316906695577146,\n",
       "  '9': 37.63146530419262,\n",
       "  '10': 37.538130513298796},\n",
       " {'1': 39.54431740831587,\n",
       "  '2': 42.412717415522415,\n",
       "  '3': 40.438744882686535,\n",
       "  '4': 41.87798983139435,\n",
       "  '5': 41.639171339135785,\n",
       "  '6': 41.478904372931275,\n",
       "  '7': 40.9149531902747,\n",
       "  '8': 40.668237116960995,\n",
       "  '9': 40.9001312702792,\n",
       "  '10': 41.55530346535353})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeBleu(t5_large),computeBleu(t5_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "civilian-royal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMeteor(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_meteor.compute(predictions=[gen],references=[orf])\n",
    "        results[k] = score['meteor']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "exact-tobacco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.4218469619412912,\n",
       "  '2': 0.4288965025941477,\n",
       "  '3': 0.4424513501452273,\n",
       "  '4': 0.44330823375766076,\n",
       "  '5': 0.4366643741939009,\n",
       "  '6': 0.43323270060369184,\n",
       "  '7': 0.43589035071469745,\n",
       "  '8': 0.4321072706301222,\n",
       "  '9': 0.43133994480891585,\n",
       "  '10': 0.42838875150969963},\n",
       " {'1': 0.4232531030208556,\n",
       "  '2': 0.4362075274248919,\n",
       "  '3': 0.4434147242218517,\n",
       "  '4': 0.4512117451388694,\n",
       "  '5': 0.4402178972116547,\n",
       "  '6': 0.44663568245351987,\n",
       "  '7': 0.4353047072263491,\n",
       "  '8': 0.429518667646878,\n",
       "  '9': 0.4330514841549833,\n",
       "  '10': 0.43150083080150453})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_small),computeMeteor(t5_small_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "thermal-spectacular",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.4452601989981421,\n",
       "  '2': 0.46645736858761155,\n",
       "  '3': 0.4715953748572525,\n",
       "  '4': 0.47667436870806507,\n",
       "  '5': 0.477859005488201,\n",
       "  '6': 0.4753311077980487,\n",
       "  '7': 0.47305294676711585,\n",
       "  '8': 0.4767908434772875,\n",
       "  '9': 0.47155532576772163,\n",
       "  '10': 0.4706481931635712},\n",
       " {'1': 0.4490921146929969,\n",
       "  '2': 0.46925272346358476,\n",
       "  '3': 0.4726857307658456,\n",
       "  '4': 0.4765914325607007,\n",
       "  '5': 0.4726245908313482,\n",
       "  '6': 0.47697433340795325,\n",
       "  '7': 0.47753883655349677,\n",
       "  '8': 0.4742287093650576,\n",
       "  '9': 0.4779662500851051,\n",
       "  '10': 0.4771801774136818})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_base),computeMeteor(t5_base_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rental-professional",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.4571177806824887,\n",
       "  '2': 0.4715511017853208,\n",
       "  '3': 0.47293396206269284,\n",
       "  '4': 0.47845250881179446,\n",
       "  '5': 0.47183714957796463,\n",
       "  '6': 0.4737929917365779,\n",
       "  '7': 0.46584797421734164,\n",
       "  '8': 0.4670003999596732,\n",
       "  '9': 0.47036770527036476,\n",
       "  '10': 0.46965224118150123},\n",
       " {'1': 0.46474595737835234,\n",
       "  '2': 0.48098338103434857,\n",
       "  '3': 0.47765354652270675,\n",
       "  '4': 0.477089241711812,\n",
       "  '5': 0.47318614815896287,\n",
       "  '6': 0.4762304250316424,\n",
       "  '7': 0.48005543899031783,\n",
       "  '8': 0.47774076148452477,\n",
       "  '9': 0.4765491282846846,\n",
       "  '10': 0.4723069758722677})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_large),computeMeteor(t5_large_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "lonely-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.45263353188403754,\n",
       "  '2': 0.48212598900183457,\n",
       "  '3': 0.4645210577643031,\n",
       "  '4': 0.48341102993693563,\n",
       "  '5': 0.4682091893648382,\n",
       "  '6': 0.484913235108725,\n",
       "  '7': 0.47823662450104554,\n",
       "  '8': 0.4877780010474901,\n",
       "  '9': 0.47519921319961617,\n",
       "  '10': 0.47470599049972345},\n",
       " {'1': 0.4480941983204554,\n",
       "  '2': 0.46340063966791695,\n",
       "  '3': 0.46709016631746453,\n",
       "  '4': 0.47418621036673575,\n",
       "  '5': 0.46435363082181336,\n",
       "  '6': 0.46794089493767865,\n",
       "  '7': 0.47518102717811517,\n",
       "  '8': 0.4674545197334252,\n",
       "  '9': 0.47539533872720646,\n",
       "  '10': 0.4757756475951134})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(bart_base),computeMeteor(bart_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "critical-tanzania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.4455845242262339,\n",
       "  '2': 0.45650881124825177,\n",
       "  '3': 0.43449759599131244,\n",
       "  '4': 0.46191074343538296,\n",
       "  '5': 0.44892468126351076,\n",
       "  '6': 0.45698807486024884,\n",
       "  '7': 0.4483735444423719,\n",
       "  '8': 0.45032261208913926,\n",
       "  '9': 0.4537599935862856,\n",
       "  '10': 0.46257516607481236},\n",
       " {'1': 0.42111578738360095,\n",
       "  '2': 0.40133698464961387,\n",
       "  '3': 0.39615999384656125,\n",
       "  '4': 0.4006973216880516,\n",
       "  '5': 0.41806436026694016,\n",
       "  '6': 0.41879715707568915,\n",
       "  '7': 0.4150160852701299,\n",
       "  '8': 0.4162673660059679,\n",
       "  '9': 0.41911324347330925,\n",
       "  '10': 0.4389151483865066})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(bart_large),computeMeteor(bart_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "adverse-headline",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.3965420382748779,\n",
       "  '2': 0.36874543358397766,\n",
       "  '3': 0.35279495355651763,\n",
       "  '4': 0.34502746331459083,\n",
       "  '5': 0.34176267550951145,\n",
       "  '6': 0.3478068980721911,\n",
       "  '7': 0.33370974717321567,\n",
       "  '8': 0.33256018554945116,\n",
       "  '9': 0.3379407619669508,\n",
       "  '10': 0.3491044045885226},\n",
       " {'1': 0.35378265804676057,\n",
       "  '2': 0.35072383431840715,\n",
       "  '3': 0.3722742891403734,\n",
       "  '4': 0.35756610780067855,\n",
       "  '5': 0.35997204386295134,\n",
       "  '6': 0.350209949629615,\n",
       "  '7': 0.36115797178506714,\n",
       "  '8': 0.3466618061667494,\n",
       "  '9': 0.3483741061567088,\n",
       "  '10': 0.3507526038737978})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_small),computeMeteor(t5_small_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "united-hollow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.414214443063727,\n",
       "  '2': 0.45809677993986536,\n",
       "  '3': 0.4556431583402754,\n",
       "  '4': 0.4486951740038113,\n",
       "  '5': 0.45154365399493723,\n",
       "  '6': 0.4443878780420834,\n",
       "  '7': 0.4491334767816698,\n",
       "  '8': 0.45273774283104734,\n",
       "  '9': 0.4473861634337102,\n",
       "  '10': 0.44518612846691913},\n",
       " {'1': 0.45270905328235356,\n",
       "  '2': 0.4788027294765709,\n",
       "  '3': 0.47059675639775284,\n",
       "  '4': 0.47014599683863983,\n",
       "  '5': 0.48900912529673296,\n",
       "  '6': 0.4855738478507699,\n",
       "  '7': 0.4862078945044895,\n",
       "  '8': 0.49571399733707705,\n",
       "  '9': 0.48998566321692066,\n",
       "  '10': 0.48838312952236995})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_base),computeMeteor(t5_base_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cosmetic-terrorism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'1': 0.4667894399083713,\n",
       "  '2': 0.4683817613815737,\n",
       "  '3': 0.46563135129707395,\n",
       "  '4': 0.47640413408918664,\n",
       "  '5': 0.476865022532185,\n",
       "  '6': 0.47683656645086897,\n",
       "  '7': 0.47272705852215324,\n",
       "  '8': 0.4707019971331393,\n",
       "  '9': 0.46936351717701963,\n",
       "  '10': 0.46992569220693825},\n",
       " {'1': 0.45914938610849676,\n",
       "  '2': 0.472163945536663,\n",
       "  '3': 0.4722213832925814,\n",
       "  '4': 0.47433237541698003,\n",
       "  '5': 0.46926010228940107,\n",
       "  '6': 0.480029413154591,\n",
       "  '7': 0.47824289376686774,\n",
       "  '8': 0.4726113877079858,\n",
       "  '9': 0.4708505125199394,\n",
       "  '10': 0.47780450426707505})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeMeteor(t5_large),computeMeteor(t5_large_ef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prerequisite-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeRouge(data):\n",
    "    results={}\n",
    "    for k in data.keys():\n",
    "        gen= [normalize_text(t) for t in data[k]]\n",
    "        score=metric_rouge.compute(predictions=[gen],references=orf,use_stemmer=True)\n",
    "        score= {key: value.high.fmeasure * 100 for key, value in score.items()}\n",
    "        results[k] = score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "incomplete-battery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 60.50842722786107,\n",
       "  'rouge2': 31.275030269162706,\n",
       "  'rougeL': 23.186516435422295,\n",
       "  'rougeLsum': 23.186516435422295},\n",
       " '2': {'rouge1': 66.6759388038943,\n",
       "  'rouge2': 31.60823594880356,\n",
       "  'rougeL': 31.84979137691237,\n",
       "  'rougeLsum': 31.84979137691237},\n",
       " '3': {'rouge1': 66.80503666805038,\n",
       "  'rouge2': 30.920415224913494,\n",
       "  'rougeL': 31.825100318251003,\n",
       "  'rougeLsum': 31.825100318251003},\n",
       " '4': {'rouge1': 66.06598281120046,\n",
       "  'rouge2': 30.00554631170272,\n",
       "  'rougeL': 32.07651788189631,\n",
       "  'rougeLsum': 32.07651788189631},\n",
       " '5': {'rouge1': 67.70126091173618,\n",
       "  'rouge2': 33.319473319473325,\n",
       "  'rougeL': 31.536649577386726,\n",
       "  'rougeLsum': 31.536649577386726},\n",
       " '6': {'rouge1': 68.12456747404845,\n",
       "  'rouge2': 33.19950159213623,\n",
       "  'rougeL': 31.640138408304498,\n",
       "  'rougeLsum': 31.640138408304498},\n",
       " '7': {'rouge1': 68.4065934065934,\n",
       "  'rouge2': 33.71805441055235,\n",
       "  'rougeL': 32.11538461538462,\n",
       "  'rougeLsum': 32.11538461538462},\n",
       " '8': {'rouge1': 69.17498973867835,\n",
       "  'rouge2': 34.10428356370603,\n",
       "  'rougeL': 31.796415378300726,\n",
       "  'rougeLsum': 31.796415378300726},\n",
       " '9': {'rouge1': 69.08742121129077,\n",
       "  'rouge2': 34.45723684210527,\n",
       "  'rougeL': 31.67991230474102,\n",
       "  'rougeLsum': 31.67991230474102},\n",
       " '10': {'rouge1': 69.71850232303908,\n",
       "  'rouge2': 35.53854565336248,\n",
       "  'rougeL': 31.729980869089918,\n",
       "  'rougeLsum': 31.729980869089918}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(bart_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "comic-agency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 72.61837495762234,\n",
       "  'rouge2': 44.35401831129196,\n",
       "  'rougeL': 25.40400045202848,\n",
       "  'rougeLsum': 25.40400045202848},\n",
       " '2': {'rouge1': 82.1764556318332,\n",
       "  'rouge2': 54.01831129196337,\n",
       "  'rougeL': 30.816170861937454,\n",
       "  'rougeLsum': 30.816170861937454},\n",
       " '3': {'rouge1': 79.9101572248565,\n",
       "  'rouge2': 51.79730404393411,\n",
       "  'rougeL': 30.621412528075865,\n",
       "  'rougeLsum': 30.621412528075865},\n",
       " '4': {'rouge1': 79.80504873781554,\n",
       "  'rouge2': 52.025,\n",
       "  'rougeL': 29.992501874531367,\n",
       "  'rougeLsum': 29.992501874531367},\n",
       " '5': {'rouge1': 80.14368883934101,\n",
       "  'rouge2': 51.41865939784414,\n",
       "  'rougeL': 29.258020562368394,\n",
       "  'rougeLsum': 29.258020562368394},\n",
       " '6': {'rouge1': 80.38194444444444,\n",
       "  'rouge2': 50.93029025055817,\n",
       "  'rougeL': 29.63789682539682,\n",
       "  'rougeLsum': 29.63789682539682},\n",
       " '7': {'rouge1': 79.76278724981468,\n",
       "  'rouge2': 50.76618882847257,\n",
       "  'rougeL': 29.89869038794169,\n",
       "  'rougeLsum': 29.89869038794169},\n",
       " '8': {'rouge1': 79.3069306930693,\n",
       "  'rouge2': 49.59148304035653,\n",
       "  'rougeL': 29.628712871287128,\n",
       "  'rougeLsum': 29.628712871287128},\n",
       " '9': {'rouge1': 79.11111111111111,\n",
       "  'rouge2': 49.37021486786861,\n",
       "  'rougeL': 29.728395061728396,\n",
       "  'rougeLsum': 29.728395061728396},\n",
       " '10': {'rouge1': 79.72653822249845,\n",
       "  'rouge2': 49.359691657341784,\n",
       "  'rougeL': 29.53387197016781,\n",
       "  'rougeLsum': 29.53387197016781}}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(bart_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sporting-delicious",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 75.88633288227334,\n",
       "  'rouge2': 44.34217650243638,\n",
       "  'rougeL': 27.902571041948576,\n",
       "  'rougeLsum': 27.902571041948576},\n",
       " '2': {'rouge1': 79.09694555112881,\n",
       "  'rouge2': 48.45908607863974,\n",
       "  'rougeL': 30.942895086321382,\n",
       "  'rougeLsum': 30.942895086321382},\n",
       " '3': {'rouge1': 77.515948444213,\n",
       "  'rouge2': 46.20393280375048,\n",
       "  'rougeL': 32.053118083582866,\n",
       "  'rougeLsum': 32.053118083582866},\n",
       " '4': {'rouge1': 79.44451722782654,\n",
       "  'rouge2': 48.250556938802255,\n",
       "  'rougeL': 30.76116860998297,\n",
       "  'rougeLsum': 30.76116860998297},\n",
       " '5': {'rouge1': 78.95906891591473,\n",
       "  'rouge2': 47.61281883584043,\n",
       "  'rougeL': 30.861775859814312,\n",
       "  'rougeLsum': 30.861775859814312},\n",
       " '6': {'rouge1': 77.83980267428275,\n",
       "  'rouge2': 45.78626152447734,\n",
       "  'rougeL': 29.676749318447353,\n",
       "  'rougeLsum': 29.676749318447353},\n",
       " '7': {'rouge1': 76.84045061566675,\n",
       "  'rouge2': 43.81551362683439,\n",
       "  'rougeL': 29.656798532879225,\n",
       "  'rougeLsum': 29.656798532879225},\n",
       " '8': {'rouge1': 75.7992073976222,\n",
       "  'rouge2': 42.28329809725159,\n",
       "  'rougeL': 30.40951122853368,\n",
       "  'rougeLsum': 30.40951122853368},\n",
       " '9': {'rouge1': 75.91836734693878,\n",
       "  'rouge2': 41.98603977347556,\n",
       "  'rougeL': 30.862409479921,\n",
       "  'rougeLsum': 30.862409479921},\n",
       " '10': {'rouge1': 76.84664662047327,\n",
       "  'rouge2': 43.075715967045895,\n",
       "  'rougeL': 30.06929010328147,\n",
       "  'rougeLsum': 30.06929010328147}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(t5_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "tutorial-orlando",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 72.1740804929713,\n",
       "  'rouge2': 35.29185128106338,\n",
       "  'rougeL': 27.190448680916614,\n",
       "  'rougeLsum': 27.190448680916614},\n",
       " '2': {'rouge1': 76.98650674662669,\n",
       "  'rouge2': 42.29471316085489,\n",
       "  'rougeL': 29.160419790104946,\n",
       "  'rougeLsum': 29.160419790104946},\n",
       " '3': {'rouge1': 76.99289660615626,\n",
       "  'rouge2': 41.68969601263324,\n",
       "  'rougeL': 29.40015785319653,\n",
       "  'rougeLsum': 29.40015785319653},\n",
       " '4': {'rouge1': 77.28051799657209,\n",
       "  'rouge2': 41.26500285768718,\n",
       "  'rougeL': 29.594362978480287,\n",
       "  'rougeLsum': 29.594362978480287},\n",
       " '5': {'rouge1': 76.58423493044822,\n",
       "  'rouge2': 39.69849246231155,\n",
       "  'rougeL': 29.018547140649154,\n",
       "  'rougeLsum': 29.018547140649154},\n",
       " '6': {'rouge1': 75.85296889726675,\n",
       "  'rouge2': 38.28021874410712,\n",
       "  'rougeL': 29.142318567389253,\n",
       "  'rougeLsum': 29.142318567389253},\n",
       " '7': {'rouge1': 76.51544033549371,\n",
       "  'rouge2': 40.2745995423341,\n",
       "  'rougeL': 29.203202439954246,\n",
       "  'rougeLsum': 29.203202439954246},\n",
       " '10': {'rouge1': 75.83317463340316,\n",
       "  'rouge2': 38.71213564488475,\n",
       "  'rougeL': 28.83260331365454,\n",
       "  'rougeLsum': 28.83260331365454}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(t5_early)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "thick-gibson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 75.6145142411237,\n",
       "  'rouge2': 39.539422326307566,\n",
       "  'rougeL': 28.365197034724932,\n",
       "  'rougeLsum': 28.365197034724932},\n",
       " '2': {'rouge1': 77.48136524127109,\n",
       "  'rouge2': 42.70015698587127,\n",
       "  'rougeL': 29.384072185170655,\n",
       "  'rougeLsum': 29.384072185170655},\n",
       " '3': {'rouge1': 77.08573625528643,\n",
       "  'rouge2': 40.73076923076923,\n",
       "  'rougeL': 29.52710495963091,\n",
       "  'rougeLsum': 29.52710495963091},\n",
       " '4': {'rouge1': 76.13458528951486,\n",
       "  'rouge2': 38.786692759295505,\n",
       "  'rougeL': 29.694835680751176,\n",
       "  'rougeLsum': 29.694835680751176},\n",
       " '5': {'rouge1': 75.37138389366693,\n",
       "  'rouge2': 39.53852170512319,\n",
       "  'rougeL': 30.492572322126666,\n",
       "  'rougeLsum': 30.492572322126666},\n",
       " '6': {'rouge1': 74.40015863573271,\n",
       "  'rouge2': 38.00833168022218,\n",
       "  'rougeL': 30.696014277215944,\n",
       "  'rougeLsum': 30.696014277215944},\n",
       " '7': {'rouge1': 74.1916286451101,\n",
       "  'rouge2': 36.991466560825565,\n",
       "  'rougeL': 29.7163261257687,\n",
       "  'rougeLsum': 29.7163261257687},\n",
       " '10': {'rouge1': 74.68928782797396,\n",
       "  'rouge2': 35.800276297612,\n",
       "  'rougeL': 29.90727954231604,\n",
       "  'rougeLsum': 29.90727954231604}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(t5_late)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "knowing-louisiana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'rouge1': 73.44699196551048,\n",
       "  'rouge2': 40.03136639874534,\n",
       "  'rougeL': 27.9835390946502,\n",
       "  'rougeLsum': 27.9835390946502},\n",
       " '2': {'rouge1': 77.34375,\n",
       "  'rouge2': 42.32121922626026,\n",
       "  'rougeL': 29.218750000000004,\n",
       "  'rougeLsum': 29.218750000000004},\n",
       " '3': {'rouge1': 76.35327635327637,\n",
       "  'rouge2': 39.33117993539807,\n",
       "  'rougeL': 29.59164292497626,\n",
       "  'rougeLsum': 29.59164292497626},\n",
       " '4': {'rouge1': 69.8594674556213,\n",
       "  'rouge2': 34.59119496855346,\n",
       "  'rougeL': 30.066568047337277,\n",
       "  'rougeLsum': 30.066568047337277},\n",
       " '5': {'rouge1': 69.33919092073953,\n",
       "  'rouge2': 33.73008606482329,\n",
       "  'rougeL': 29.361156873512726,\n",
       "  'rougeLsum': 29.361156873512726},\n",
       " '6': {'rouge1': 69.24062214089662,\n",
       "  'rouge2': 33.13197876624565,\n",
       "  'rougeL': 29.31381518755718,\n",
       "  'rougeLsum': 29.31381518755718},\n",
       " '7': {'rouge1': 67.37115313311087,\n",
       "  'rouge2': 31.268545994065278,\n",
       "  'rougeL': 29.699666295884313,\n",
       "  'rougeLsum': 29.699666295884313},\n",
       " '10': {'rouge1': 66.94713736967257,\n",
       "  'rouge2': 30.88746569075938,\n",
       "  'rougeL': 29.99817084324126,\n",
       "  'rougeLsum': 29.99817084324126}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeRouge(t5_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-cycle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-richmond",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-puppy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration,BartConfig,Bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-indonesia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-draft",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-corporation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-abortion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-uniform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-trigger",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-mechanics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-equality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-million",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-savannah",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-jason",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-homework",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "entire-secretary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.8727, 'recall': 0.8451, 'precision': 0.9023}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs=4\n",
    "metricMentionScore(table_ref_metrics,t5_baseline[f'{bs}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "floppy-pennsylvania",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1_score': 0.8803, 'recall': 0.8803, 'precision': 0.8803}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metricMentionScore(table_ref_metrics,t5_hybrid[f'{bs}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "sitting-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_counts =  np.sum([len(e) for e in ref_mentions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "dramatic-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=4\n",
    "baseline_mentions = [extractMetricMentions(r) for r in t5_baseline[f'{bs}']]\n",
    "g_baseCount = np.sum([len(e) for e in baseline_mentions])\n",
    "hyb_mentions = [extractMetricMentions(r) for r in t5_hybrid[f'{bs}']]\n",
    "g_hybCount = np.sum([len(e) for e in hyb_mentions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "convertible-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_overlap=np.sum([getOverlap(r,b) for r,b in zip(ref_mentions,baseline_mentions)])\n",
    "hyb_overlap =np.sum([getOverlap(r,b) for r,b in zip(ref_mentions,hyb_mentions)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "synthetic-lease",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9304347826086956"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_h=hyb_overlap/ref_counts\n",
    "prec_h=hyb_overlap/g_baseCount\n",
    "h_f1= 2*(rec_h*prec_h)/(rec_h+prec_h)\n",
    "h_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "appointed-dispute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.8699186991869918)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_h,rec_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "consolidated-sacramento",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695652173913043"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_b=base_overlap/ref_counts\n",
    "prec_b = base_overlap/g_baseCount\n",
    "b_f1= 2*(rec_b*prec_b)/(rec_b+prec_b)\n",
    "b_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "strong-coalition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9345794392523364, 0.8130081300813008)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_b,rec_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "polar-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the right metrics were mentioned\n",
    "import numpy as np\n",
    "import re\n",
    "from utils import normalize_text\n",
    "\n",
    "identicals = {'sensitivity': 'recall', 'true positive rate': 'recall'}\n",
    "def metricEval(reference_metrics, generated):\n",
    "    metrics_list = ['recall', 'sensitivity', 'f1-score',\n",
    "                    'precision', 'f2-score', 'f1',\n",
    "                    'f2', 'accuracy',\n",
    "                    'auc',\n",
    "                    'specificity', ]\n",
    "    nn = normalize_text(generated).split()\n",
    "    found_metrics = [s.lower().replace('-score', '').replace('score', '')\n",
    "                     for s in set(metrics_list).intersection(nn)]\n",
    "    n_found = 0\n",
    "    for m in set(found_metrics):\n",
    "        # Check if the indentical metric is specified\n",
    "        # eg. if the model used generated recall instead of sensitivity\n",
    "        ii = identicals.get(m, -1)\n",
    "        if m in reference_metrics or ii in reference_metrics:\n",
    "            # print(m)\n",
    "            n_found += 1\n",
    "        else:\n",
    "\n",
    "            #print(m,' oops')\n",
    "            n_found -= 1\n",
    "    if n_found != 0:\n",
    "        return n_found/len(found_metrics)\n",
    "    else:\n",
    "        # We assume all the metrics we considered when describing the  model's performance in the narration\n",
    "        return 1\n",
    "\n",
    "\n",
    "def classLabelsEval(reference_classes, generated, penalty=-1):\n",
    "    # In some cases the model will generate the phrase \"binary classification\" or \"multi-class\"\n",
    "    # \"2 classes\", \"3 classes\",\"two labels\", \"two classes\"\n",
    "    false_mentions = 1\n",
    "    if any(ele in generated for ele in ['binary', 'two classes', 'two class labels', 'two-labels', '2 classes',\n",
    "                                        'two-classes', 'two class labels', '2 labels', '2 classes']) and len(reference_classes) > 2:\n",
    "        # should have generated multi-class\n",
    "        false_mentions = penalty\n",
    "\n",
    "    if any(ele in generated for ele in ['multi-class', 'multi class', 'multi classes', 'multi labels',\n",
    "                                        'three-classes', 'three classes',\n",
    "                                        'three class labels',\n",
    "                                        '3-labels',\n",
    "                                        '3-classes',\n",
    "                                        'four classes',\n",
    "                                        'four class labels',\n",
    "                                        '4 labels',\n",
    "                                        '4 classes']) and len(reference_classes) < 3:\n",
    "        # should have generated binary\n",
    "        false_mentions = penalty\n",
    "\n",
    "    classes_tokens = [f'c{i}' for i in range(1, 6)]\n",
    "    nn = normalize_text(generated).split()\n",
    "    found_classes = [s.lower() for s in set(classes_tokens).intersection(nn)]\n",
    "    n_found = 0\n",
    "    label_mentions = {}\n",
    "    label_was_mentioned = False\n",
    "    predicted_text = normalize_text(generated)\n",
    "    for cl in classes_tokens:\n",
    "        c = predicted_text.count(cl)\n",
    "        if cl in reference_classes:\n",
    "            # The class label exists in the reference class list so the model gets a score of 1\n",
    "            label_mentions[cl] = 1\n",
    "        else:\n",
    "            if c > 0:\n",
    "                # Invalid class label was mentioned\n",
    "                # A penalty is assigned t\n",
    "                label_mentions[cl] = penalty\n",
    "            else:\n",
    "                # No mention of the class\n",
    "                label_mentions[cl] = 0\n",
    "        if c > 0:\n",
    "            label_was_mentioned = True\n",
    "\n",
    "    if not label_was_mentioned:\n",
    "        # if no label was mentioned, we assume  they were mentioned implicitly\n",
    "        return 1\n",
    "    else:\n",
    "        # Check to make sure only the right labels were accounted for\n",
    "        score = np.mean(\n",
    "            [v for v in list(label_mentions.values()) if v != 0]+[false_mentions])\n",
    "        return np.round(score, 2)\n",
    "\n",
    "\n",
    "def computeValuesMentions(reference_values, generated):\n",
    "    \n",
    "    # print(reference_values)\n",
    "    prohibitedWords = [f'({i})' for i in range(10)]\n",
    "    big_regex = re.compile('|'.join(map(re.escape, prohibitedWords)))\n",
    "    the_message = big_regex.sub(\"\", str(generated))\n",
    "\n",
    "    # Extract all numbers\n",
    "    numbers = []\n",
    "    toks = the_message.split()\n",
    "    for t in toks:\n",
    "        try:\n",
    "            tt = normalize_text(t).split()\n",
    "            v = float(tt[0].replace('%', '').replace(',', ''))\n",
    "            numbers.append(v)\n",
    "        except:\n",
    "            result = re.findall('\\(.*?\\)', t)\n",
    "            if len(result) > 0:\n",
    "                result = normalize_text(result[0].replace(\n",
    "                    '(', '').replace(')', '')).split()\n",
    "                try:\n",
    "                    v = float(result[0].replace('%', '').replace(',', ''))\n",
    "                    numbers.append(v)\n",
    "                except:\n",
    "                    pass\n",
    "            pass\n",
    "\n",
    "    if len(numbers) == 0:\n",
    "        return 1\n",
    "    # Compare the numbers to the list\n",
    "    val_mentions = {}\n",
    "    for num in numbers:\n",
    "        if num in reference_values:\n",
    "            val_mentions[num] = 1\n",
    "        else:\n",
    "            val_mentions[num] = -1\n",
    "    # print(val_mentions)\n",
    "    score = np.mean([v for v in val_mentions.values() if v != 0])\n",
    "\n",
    "    return np.round(score, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "outside-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "needed-finland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\Essel.Ampomah\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\dd5c89920ca26c45408d5d4030d53cb6cc68a8af088db93410ad1ed4db8018f0\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "meteors= load_metric('bleurt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decimal-desperate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> accuracy | VALUE_HIGH | 90.67% <|> sensitivity | VALUE_HIGH | 87.29% && sensitivity | also_known_as | recall <|> f1-score | VALUE_HIGH | 88.89% <|> precision | VALUE_HIGH | 91.3%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['sensitivity', 'accuracy', 'precision', 'f1-score'],\n",
       " 'values': ['87.29%', '90.67%', '91.3%', '88.89%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1-score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1-score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "damaged-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_bleu = load_metric('sacrebleu')\n",
    "metric_ter = load_metric('ter')\n",
    "metric_xnli = load_metric('xnli')\n",
    "metric_bert =load_metric('bertscore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equal-prisoner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fatal-proceeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# CC'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_13a.Tokenizer13a()('#CC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "boring-korea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exciting-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "refs =[[t['narration'] for t in test_sample]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dangerous-commissioner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The classifier was able to produce fairly high scores across the metrics sensitivity, accuracy, precision and F1score. Specifically, for the sensitivity it scored 87.29%, accuracy (96.67%) and precision (91.3%) with the F1score equal to 88.89%. These scores suggest that the model will incorrectly assign the wrong labels for only a small number of test cases. Overall, the model's prediction decisions are quite precise and accurate.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "accompanied-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=3\n",
    "gen_base=[t for t in t5_baseline[f'{bs}']]\n",
    "gen_h=[t for t in t5_hybrid[f'{bs}']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "divided-bahamas",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32108, 1]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_.encode('F1-score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "threaded-nepal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> recall | VALUE_HIGH | 93.9% <|> accuracy | VALUE_HIGH | 95.84% <|> precision | VALUE_HIGH | 87.1% <|> auc | VALUE_HIGH | 98.01%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['accuracy', 'recall', 'auc', 'precision'],\n",
       " 'values': ['95.84%', '93.9%', '98.01%', '87.1%'],\n",
       " 'rates': ['VALUE_HIGH', 'VALUE_HIGH', 'VALUE_HIGH', 'VALUE_HIGH'],\n",
       " 'narration': 'The classifier boasts very high values for the recall, precision, accuracy, and AUC metrics (i.e 93.9, 87.1, 95.84, and 98.01, respectively). Judging by the near-perfect AUC, accuracy, and recall scores, we can be confident that the model will be very effective at predicting the true class labels for the test cases with little chance of misclassification.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-transsexual",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "prepared-variation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> accuracy | VALUE_HIGH | 76.89% <|> sensitivity | VALUE_MODERATE | 76.45% && sensitivity | also_known_as | recall <|> specificity | VALUE_HIGH | 79.95% <|> precision | VALUE_LOW | 38.16% <|> f1-score | VALUE_MODERATE | 63.48%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|IMBALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|IMBALANCED|>'],\n",
       " 'metrics': ['f1-score',\n",
       "  'specificity',\n",
       "  'precision',\n",
       "  'sensitivity',\n",
       "  'accuracy'],\n",
       " 'values': ['63.48%', '79.95%', '38.16%', '76.45%', '76.89%'],\n",
       " 'rates': ['MODERATE', 'HIGH', 'LOW', 'MODERATE', 'HIGH'],\n",
       " 'narration': 'For this classification task, the number of observations is imbalanced between the class labels #CA and #CB. The scores achieved with respect to the metrics accuracy, precision, F1-score, sensitivity, and specificity are 76.89%, 38.16%, 63.48%, 76.45%, and 79.95%. According to the specificity, sensitivity, and precision scores, the model is shown to be quite good at correctly identifying the cases belonging to class #CA compared to its ability with respect to #CB examples.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tidx=29\n",
    "test_sample[tidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "material-proportion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for this classification task , the model was trained to label the test samples as class # ca or class # cb . the classifier shows signs of low understanding of the classification problem under consideration . this assertion is based on the scores across the metrics accuracy , sensitivity / recall , f1 - score , specificity , and precision . as shown in the table , it obtained a moderate scores of 76.89 % ( accuracy ) , 63.48 % ( f2 - score ) , and 38.16 % ( precision ) . from the precision and recall scores , we can see that the false positive is higher than the true positive predictions . overall , this model is shown to have lower confidence regarding the prediction output decisions related to the minority class label # cb for the majority of test cases .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_base[tidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "informal-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sensitivity , specificity and accuracy scores of 76.45 % , 79.95 % , and 78.16 , respectively , indicate how poor the model's performance is on this ml task . it has a very low f1 - score of 63.48 % suggesting that the likelihood of examples belonging to class label # ca being misclassified as # cb is very marginal .\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_h[tidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dominant-honduras",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['for this classification task , the number of observations is imbalanced between the class labels # ca and # cb . the scores achieved with respect to the metrics accuracy , precision , f1 - score , sensitivity , and specificity are 76.89 % , 38.16 % , 63.48 % , 76.45 % , and 79.95 % . according to the specificity , sensitivity , and precision scores , the model is shown to be quite good at correctly identifying the cases belonging to class # ca compared to its ability with respect to # cb examples .']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[refs[0][tidx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "interracial-truck",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': [0.8942320346832275],\n",
       " 'recall': [0.9001883268356323],\n",
       " 'f1': [0.8972002863883972],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.10(hug_trans=4.12.5)'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_bert.compute(predictions=[gen_h],references=[refs],lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "postal-helmet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e161079a4841fd9c78a0f1f71ab070",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da3042d8b8f4c66bd6d9281f27b9cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9016a2589a0841c68c331e0bc2b724b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5023c6354e418ba8924498b6998e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a38fb4f70ca48b28c8c04c2c63d1656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'precision': [0.9030911326408386],\n",
       " 'recall': [0.8971150517463684],\n",
       " 'f1': [0.9000931978225708],\n",
       " 'hashcode': 'roberta-large_L17_no-idf_version=0.3.10(hug_trans=4.12.5)'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_bert.compute(predictions=[gen_base],references=[refs],lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "clinical-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fresh-institution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'bertscore',\n",
       " 'bleu',\n",
       " 'bleurt',\n",
       " 'cer',\n",
       " 'chrf',\n",
       " 'code_eval',\n",
       " 'comet',\n",
       " 'competition_math',\n",
       " 'coval',\n",
       " 'cuad',\n",
       " 'f1',\n",
       " 'gleu',\n",
       " 'glue',\n",
       " 'google_bleu',\n",
       " 'indic_glue',\n",
       " 'matthews_correlation',\n",
       " 'meteor',\n",
       " 'pearsonr',\n",
       " 'precision',\n",
       " 'recall',\n",
       " 'rouge',\n",
       " 'sacrebleu',\n",
       " 'sari',\n",
       " 'seqeval',\n",
       " 'spearmanr',\n",
       " 'squad',\n",
       " 'squad_v2',\n",
       " 'super_glue',\n",
       " 'ter',\n",
       " 'wer',\n",
       " 'wiki_split',\n",
       " 'xnli']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.list_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
