{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "magnetic-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/essel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/essel/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle as pk\n",
    "import tensorflow as tf\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import pandas as pd\n",
    "ACCEPTABLE_AVAILABLE_MEMORY = 8000\n",
    "import subprocess as sp\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']='1'\n",
    "def mask_unused_gpus(leave_unmasked=1,random=True):\n",
    "    COMMAND = \"nvidia-smi --query-gpu=memory.free --format=csv\"\n",
    "    try:\n",
    "        _output_to_list = lambda x: x.decode('ascii').split('\\n')[:-1]\n",
    "        memory_free_info = _output_to_list(sp.check_output(COMMAND.split()))[1:]\n",
    "        memory_free_values = [int(x.split()[0]) for i, x in enumerate(memory_free_info)]\n",
    "        available_gpus = [i for i, x in enumerate(memory_free_values) if x > ACCEPTABLE_AVAILABLE_MEMORY]\n",
    "        import numpy as np\n",
    "        #print(available_gpus)\n",
    "        if random:\n",
    "           available_gpus = np.asarray(available_gpus)\n",
    "           np.random.shuffle(available_gpus)\n",
    "        if len(available_gpus) < leave_unmasked: raise ValueError('Found only %d usable GPUs in the system' %\\\n",
    "                                                                  len(available_gpus))\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, available_gpus[:leave_unmasked]))\n",
    "        #print(','.join(map(str, available_gpus[:leave_unmasked])))\n",
    "    except Exception as e:\n",
    "        print('\"nvidia-smi\" is probably not installed. GPUs are not masked', e)\n",
    "\n",
    "#mask_unused_gpus(2)\n",
    "mask_unused_gpus(1)\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "#sys.path.append('/content/drive/MyDrive/PerformanceNarrativesRuns/')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "torch.manual_seed(42)\n",
    "\n",
    "from transformers import GPT2LMHeadModel,  GPT2Tokenizer, GPT2Config, GPT2LMHeadModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('brown')\n",
    "import functools\n",
    "\n",
    "from nltk import wordpunct_tokenize,word_tokenize\n",
    "# Read the dataset\n",
    "full_data= json.load(open('../dataset/annotation_data_with_augmentations.json'))['data']\n",
    "#jdata = json.load(open('james_submission.json'))\n",
    "first_batch= json.load(open('../dataset/annotation_download.json'))\n",
    "full_data = train_data = full_data#+first_batch\n",
    "test_data = json.load(open('../dataset/test set.json'))\n",
    "classes_tokens =[f'C{i}' for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "raised-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "above-mortgage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> accuracy | VALUE_MODERATE | 75.49% <|> recall | VALUE_MODERATE | 77.56% <|> precision | VALUE_MODERATE | 74.65%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['recall', 'accuracy', 'precision'],\n",
       " 'values': ['77.56%', '75.49%', '74.65%'],\n",
       " 'rates': ['MODERATE', 'MODERATE', 'MODERATE'],\n",
       " 'narration': 'The classifier trained to solve the given classification problem was evaluated based on its scores across the following metrics: accuracy, recall and precision. For the accuracy, the model achieved 75.49%, and 77.56% for the recall with a moderate precision score of (74.65%). Considering these values, we can make the conclusion that this model can correctly differentiating between the examples belonging to the different class labels with a close to moderate chance of misclassification.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc= full_data[123]\n",
    "examples = composePreambleAndInputs(pc,identical_metrics=identicals,augnment_metrics=True,augment_output_order=True)\n",
    "examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221869bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processInputTableAndNarrations = composePreambleAndInputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "appointed-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = []\n",
    "eval_tables = []\n",
    "for pc in test_data:\n",
    "    test_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #eval_tables.append(parseTableStructureForEval(pc,identicals))\n",
    "rtest_sample = []\n",
    "reval_tables = []\n",
    "for pc in test_data:\n",
    "    rtest_sample.append(processInputTableAndNarrations(pc,identical_metrics=identicals))\n",
    "    #reval_tables.append(parseTableStructureForEval(pc,identicals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "light-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "opened-tragedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_met=False\n",
    "processed=[]\n",
    "preambles = []\n",
    "# apply the preprocessing to the data\n",
    "for idx,pc in enumerate(full_data):\n",
    "    #print(idx)\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=False,)\n",
    "    preambles.append(examples['preamble'])\n",
    "    processed.append(examples)\n",
    "\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if len(examples['classes'])> 2:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)        \n",
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if examples['preamble'] not in set(preambles):\n",
    "        preambles.append(examples['preamble'])\n",
    "        processed.append(examples)\n",
    "\n",
    "\n",
    "sample_rounds = 10\n",
    "sample_size = 500\n",
    "for i in range(sample_rounds):\n",
    "    for idx,pc in enumerate(random.sample(full_data,sample_size)):\n",
    "        examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)        \n",
    "        \n",
    "random.shuffle(processed)\n",
    "random.shuffle(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "shaped-calculator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4977"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "intended-diversity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mature-investigator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4977"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preambles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "protecting-separate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> sensitivity | VALUE_HIGH | 88.89% && sensitivity | also_known_as | recall <|> accuracy | VALUE_HIGH | 85.53% <|> f2score | VALUE_HIGH | 88.15% <|> precision | VALUE_HIGH | 85.33% <|> specificity | VALUE_HIGH | 82.59%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA and #CB  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['specificity', 'accuracy', 'sensitivity', 'precision', 'f2score'],\n",
       " 'values': ['82.59%', '85.53%', '88.89%', '85.33%', '88.15%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"Evaluating the classifier's prowess on the classification task produced the scores 85.33%, 88.89%, 82.59%, 85.53%, and 88.15%, respectively, across the metrics precision, sensitivity, specificity, accuracy, and F2score. The difference between the precision, and sensitivity scores indicates that the classifier is very confident about its #CB predictions. Similarly, the specificity score also suggests the confidence with respect to #CA predictions is also high. From the above statements, we can conclude that the classifier has a good classification ability, only misclassifying a small percentage of all possible test cases.\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "instructional-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,pc in enumerate(full_data):\n",
    "    examples = processInputTableAndNarrations(pc,\n",
    "                                              identical_metrics=identicals,\n",
    "                                              augnment_metrics=True,\n",
    "                                              augment_output_order=True)\n",
    "    if len(examples['classes'])> 2:\n",
    "        if examples['preamble'] not in set(preambles):\n",
    "            preambles.append(examples['preamble'])\n",
    "            processed.append(examples)\n",
    "\n",
    "random.shuffle(processed)\n",
    "random.shuffle(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "satisfactory-powell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "premier-helping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mobile-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intermediate-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset\n",
    "pk.dump(processed,open('../dataset/train_dataset_full.dat','wb'))\n",
    "processed = pk.load(open('../dataset/train_dataset_full.dat','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6874a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4977"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "future-surrey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preamble': '<MetricsInfo> precision | VALUE_HIGH | 90.98% <|> accuracy | VALUE_HIGH | 93.42% <|> recall | VALUE_HIGH | 88.13% <|> f1score | VALUE_HIGH | 89.42%  <|section-sep|> <TaskDec> ml_task | dataset_attributes | <|BALANCED|> && ml_task | class_labels | #CA, #CB and #CC  <|section-sep|> <|table2text|> ',\n",
       " 'classes': ['#CA', '#CB', '#CC'],\n",
       " 'dataset_attribute': ['<|BALANCED|>'],\n",
       " 'metrics': ['precision', 'recall', 'f1score', 'accuracy'],\n",
       " 'values': ['90.98%', '88.13%', '89.42%', '93.42%'],\n",
       " 'rates': ['HIGH', 'HIGH', 'HIGH', 'HIGH'],\n",
       " 'narration': \"The algorithm's prediction performance on the given multi-class classification problem where the test instances are classified as either #CA or #CB or #CC is: (a) Accuracy = 93.42%. (b) Precision = 90.98%. (c) F1score = 89.42%. (d) Recall = 88.13%. On this multi-class problem, the algorithm is shown to perform very well across all the evaluation metrics under consideration. The scores across the different metrics indicate that it is very effective and precise at correctly labeling most of the test observations.\"}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[593]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
